# GAIA Ecosystem — Product Requirements Document (PRD)

**Version:** 1.1.0-draft
**Date:** February 08, 2026
**Status:** Draft for ENG, PROD, UX handoff
**Document Owner:** Federico (Product Owner)
**Generated by:** Claude Opus 4.6
**Source Documents:** GAIA_BIBLE.md v0.4.3, GECO_AUDIT.md v1.0.0, registry.json

---

## Table of Contents

### Part I: Vision & Foundation
- [Section 1: Vision & Problem Statement](#section-1-vision--problem-statement)
- [Section 2: Architecture Overview](#section-2-architecture-overview)
- [Section 3: Trust Contract & Constitutional Governance](#section-3-trust-contract--constitutional-governance)

### Part II: Platform & Products
- [Section 4: Platform Capabilities](#section-4-platform-capabilities)
- [Section 5: Product Catalog](#section-5-product-catalog)

### Part III: Execution
- [Section 6: Implementation Roadmap](#section-6-implementation-roadmap)
- [Section 7: Success Criteria](#section-7-success-criteria)
- [Section 8: Risks & Mitigations](#section-8-risks--mitigations)

### Part IV: Reference
- [Section 9: Technical Specifications](#section-9-technical-specifications)
- [Section 10: Open Questions for Team Review](#section-10-open-questions-for-team-review)
- [Section 11: Appendices](#section-11-appendices)

---

# Part I: Vision & Foundation

## Section 1: Vision & Problem Statement

### 1.1 The Problem

AI-powered development tools have created a productivity paradox. Claude, GPT-4, and Gemini can generate thousands of lines of production-ready code in seconds. Yet this capability has introduced three critical pathologies that compound over time:

#### 1.1.1 Fragmentation — The Duplicate Implementation Problem

Developers create isolated projects with no shared infrastructure. Every new project reinvents the wheel:

**Real-world example from GAIA pre-v0.1.0 ecosystem (Feb 3, 2026):**
- 7 production Python AI projects running in isolation
- 5 different LLM client implementations (HART OS, VIA, DATA FORGE, ECHO, THE PALACE)
- 4 chunking/embedding implementations
- 3 different approaches to `.env` loading
- No shared configuration, no unified logging, no cross-project cost tracking
- 3 out of 4 production systems had NO version control

**Cost:** When VIA needed to upgrade from OpenAI to multi-provider support (OpenAI + Gemini + Anthropic), the change required rewriting 400+ lines of LLM client code. HART OS had already solved this problem months earlier, but the knowledge was siloed in `hart_os/services/llm_gateway.py` and never extracted.

**Pattern:** Each project pays the full integration cost for every LLM provider, every API change, every security patch. Fragmentation scales costs linearly instead of amortizing them.

#### 1.1.2 Opacity — The Black Box Crisis

AI generates code fast, but users don't understand what was built. They can't modify it confidently. They can't explain it to stakeholders. They become permanently dependent on the AI.

**Real-world example from HART OS v6.x:**
- Licensed therapists using HART OS to generate therapy session plans
- 100% of therapy plans include canonical citations (DSM-5-TR, evidence-based modalities)
- But therapists cannot inspect *why* a specific technique was selected
- System works but provides no pedagogical value — users don't grow in capability
- Trust is based on blind faith rather than transparency

**Real-world example from VIA v6.4:**
- Investment analysts need AI-synthesized research with semantic claim tracking
- VIA generates multi-source synthesis across 10-Ks, earnings calls, analyst reports
- But there's no explainability layer — users see "Claude said X" with a citation, not "Claude said X *because* of pattern Y detected across 3 sources using mental model Z"
- Analysts can't distinguish high-confidence claims from speculative ones without manually re-reading source documents

**Pattern:** Users adopt AI tools quickly but learn slowly. Velocity increases but capability stagnates. The glass ceiling appears after 6-12 months when users need to modify or extend functionality but lack mental models of the underlying system.

#### 1.1.3 Ungoverned Growth — The Entropy Problem

No shared infrastructure means no institutional memory, no cross-project learning, no quality enforcement, no observability.

**Real-world example from pre-GAIA ecosystem:**
- HART OS production system had OpenAI API key `sk-proj-kNkhu_LsFv...` committed to git history (discovered during v0.2.0 stabilization on Feb 4, 2026)
- No automated secrets scanning, no pre-commit hooks, no governance layer
- Key remained exposed for 3+ months before discovery
- No cost tracking system — total LLM spend across 7 projects was unknown
- No telemetry — couldn't answer "which project called OpenAI 847 times yesterday?"

**Real-world example from THE PALACE (case study project):**
- Built as standalone visualization of agent architectures
- Beautiful HTML/CSS/JS implementation, fully complete
- Zero git history, zero documentation, zero tests
- When user wanted to replicate pattern in VIA 6 months later, had to reverse-engineer the implementation from scratch
- Pattern recognition learned by THE PALACE project was lost — no cross-project memory

**Pattern:** Every project exists in isolation. Patterns learned by one project (good or bad) don't propagate. Governance is reactive instead of proactive. Quality degrades over time because there's no enforcement layer.

---

### 1.2 The GAIA Solution

GAIA is a **constitutional AI governance framework** that sits between human creativity and AI-powered development. It transforms vague, fast, creative intent into structured, inspectable, and governable products without forcing users to think like engineers from day one.

#### 1.2.1 What GAIA Is

**GAIA is a meta-layer** — not a product, but an ecosystem governance framework that:
1. **Standardizes project creation** (VULCAN ensures every new project is GAIA-compliant from birth)
2. **Provides shared intelligence** (MYCEL supplies unified LLM clients, chunking, retrieval to all projects)
3. **Enforces memory discipline** (MNEMIS manages hierarchical memory with explicit promotion workflow)
4. **Orchestrates workflows** (LOOM provides glass-box agent editing with execution contracts)
5. **Monitors transparently** (ARGUS provides mental model library, pattern detection, explainability)
6. **Governs constitutionally** (WARDEN enforces git hygiene, test coverage, secrets safety)
7. **UXUI Creative Director** (AURORA creates design specs, manages design system, researches inspiration design)
8. **RAVEN** (RAVEN is a research orchestator)
9  **ABIS** (ABIS node based Ai architecture editor and generator for products)

#### 1.2.2 What GAIA Is Not

- **Not a code generator** — GAIA doesn't replace Claude/GPT-4, it governs their output
- **Not a no-code platform** — GAIA assumes Python knowledge, teaches architectural discipline
- **Not a deployment platform** — GAIA is local-first, Windows 11 native, X:\Projects\ filesystem-based
- **Not a SaaS** — GAIA has no cloud backend, no user accounts, no telemetry sent externally

---

### 1.3 North Star Principle

**GAIA is a bridge, not a replacement.**

It teaches you to build with AI safely while actually doing the work. You learn by doing. You grow in capability. You become the architect.

**Anti-pattern we reject:** "Just ask the AI to fix it"
**Pattern we embrace:** "ARGUS shows me which mental model applies here. LOOM lets me rewire agent 3's memory schema. MNEMIS shows me we solved this exact pattern in HART OS 4 months ago. I understand the change I'm making."

---

### 1.4 Target Users

#### 1.4.1 Primary Persona: The Solo Creator

**Profile:**
- Beginer Technical creator with Python knowledge (1 years experience)
- Overwhelmed by managing multiple AI projects
- Strong domain expertise (therapy, finance, data processing) but weak on software architecture
- Using Claude/Cursor/v0 to generate code quickly but struggling to maintain it
- Background in UX and product but not eng, does not know best practices

**Pain Points:**
- "I have 7 projects scattered everywhere — some in X:\Projects\, some in C:\Users\Fede\, no consistency"
- "I duplicated my LLM client code 5 times. Now OpenAI deprecated an API and I need to update 5 places"
- "I don't know which project is burning through my API quota"
- "I have no version control on 3/4 production systems — terrified of losing work"
- "Claude generates great code but I don't understand the architecture. I'm afraid to change it."

**Goals:**
- Stabilize existing chaos (bring 7 projects under version control, unified structure)
- Create new GAIA-compliant projects in under 10 minutes with confidence the structure is sound
- Share LLM client code so one upgrade flows to all projects
- Track costs across all projects from single dashboard
- Grow technical capability through glass-box transparency

**Success Metrics:**
- All production projects under git version control within 1 day (using VULCAN retroactive registration)
- New project creation time: <10 minutes (7-step VULCAN questionnaire)
- LLM client code duplication: 0 (all projects use MYCEL)
- API key security incidents: 0 (WARDEN pre-commit hooks block commits)
- Time to modify agent workflow: reduced from "ask Claude to rewrite it" (30+ min, no learning) to "use LOOM visual editor" (5 min, full transparency)

#### 1.4.1.2 Primary Persona: The Creative Solopreneur

**Profile:**
- zero Technical knowledge
- Overwhelmed by managing multiple projects, lt alone AI
- Strong domain expertise (therapy, finance, data processing) but weak on software architecture and divergent creative thinking
- Using Claude/Gemini to generate apps quickly but struggling to maintain it

**Pain Points:**
- "I dont understand how this works and dont have time I must produce outputs"
- "because I am in a rush, now I cant track my processes or projects"

**Goals:**
- Stabilize existing chaos
- Create new GAIA-compliant projects in under 10 minutes with confidence the structure is sound
- Share LLM client code so one upgrade flows to all projects
- Track costs across all projects from single dashboard
- Grow technical capability through glass-box transparency
- Help Persona achieve discipline and clear workflow

**Success Metrics:**
- All production projects under git version control within 1 day (using VULCAN retroactive registration)
- New project creation time: <10 minutes (7-step VULCAN questionnaire)
- LLM client code duplication: 0 (all projects use MYCEL)
- API key security incidents: 0 (WARDEN pre-commit hooks block commits)
- Time to modify agent workflow: reduced from "ask Claude to rewrite it" (30+ min, no learning) to "use LOOM visual editor" (5 min, full transparency)

#### 1.4.2 Secondary Persona: The Growing Team

**Profile:**
- 2-5 person team building AI products
- Need shared infrastructure to avoid stepping on each other
- Mix of technical levels (1-2 senior, 2-3 junior)
- Already using git but lacking ecosystem-level governance

**Pain Points:**
- "Every team member has their own LLM client. We have 3 different Anthropic integrations."
- "Our projects don't talk to each other. VIA learned a great RAG pattern but DATA FORGE can't access it."
- "No observability — I can't see what my teammate's agent is doing without reading logs"
- "We keep re-learning the same lessons. 'Oh we solved this in HART OS 4 months ago but forgot.'"

**Goals:**
- Unified development platform with shared intelligence
- Cross-project learning (patterns discovered in one project available to all)
- Team dashboard showing all projects, costs, telemetry, task status
- Onboarding new team members with consistent GAIA structure

**Success Metrics:**
- All team projects use MYCEL (100% adoption within 2 weeks)
- All projects send telemetry to ARGUS (single dashboard for entire team)
- Memory patterns shared via MNEMIS (reduce "we already solved this" discovery time from 30+ min to <2 min)
- Onboarding time for new team member: <4 hours to understand full ecosystem (vs 2+ days learning 7 different architectures)

#### 1.4.3 Tertiary Persona: The Enterprise AGENTIC Team (Future)

**Profile:**
- 2+ engineers, product managers, compliance officers
- Regulated industry (healthcare, finance) requiring audit trails
- Need multi-tenant isolation, role-based access control, compliance reporting

**Pain Points:**
- "We need SOC2 compliance. Can't audit 50 isolated Python projects."
- "Healthcare data — need to prove no PHI in logs, all LLM calls are anonymized"
- "Multi-team coordination — Team A's agent needs to call Team B's service safely"

**Goals:**
- Enterprise governance (RBAC, audit logs, compliance dashboards)
- Multi-tenant GAIA (isolated PROJECT-tier memory per team)
- Centralized policy enforcement (no one can commit secrets, all tests must pass)

**Success Metrics:**
- Audit trail: 100% of LLM calls logged with user, project, cost, timestamp
- Secrets safety: 0 API keys committed across 50+ projects
- Compliance reporting: Generate SOC2 evidence with 1 command

*(Note: Enterprise persona is Phase 4+, not current scope)*

---

### 1.5 Value Proposition Matrix

| Stakeholder | Problem Solved | GAIA Solution | Measurable Impact |
|-------------|----------------|---------------|-------------------|
| **Creator (Solo)** | Fragmentation, chaos, fear of losing work | VULCAN standardizes project creation, MYCEL eliminates duplication, WARDEN enforces version control | Time to create new project: 60 min → 10 min. API key security incidents: 2 in 6 months → 0. |
| **Creator (Solo)** | Black-box AI dependency, can't modify confidently | ARGUS provides 59 mental models with layered explainability (Simple → Advanced). LOOM provides visual agent editor. | Time to modify agent workflow: 30+ min (ask AI to rewrite) → 5 min (LOOM visual editor). User capability growth: "I understand the change I'm making." |
| **Team (2-5)** | Duplicate work, no shared learning | MYCEL shared intelligence, MNEMIS cross-project memory with promotion discipline | LLM client code: 5 implementations → 1 (MYCEL). Pattern discovery: 30+ min → <2 min (MNEMIS search). |
| **Team (2-5)** | No observability, can't track costs or activity | ARGUS unified dashboard, telemetry from all projects | Cost tracking: "unknown total spend" → real-time per-project breakdown. Telemetry query: "which project called OpenAI 847 times?" answered in <10 sec. |
| **End User (HART OS therapist)** | Black-box therapy recommendations, trust requires blind faith | ARGUS layered explainability maps to Growth Rungs. "Why this technique?" answered at user's comprehension level. | Therapist confidence: "I trust it" → "I understand why it recommended CBT for this case (cognitive distortion detected via mental model X)". |
| **End User (VIA analyst)** | AI synthesis with no confidence indicators | ARGUS mental model annotations show "high confidence (3 sources, consistent pattern)" vs "speculative (1 source, minority view)" | Analyst efficiency: 50% time savings on research (maintained). Decision quality: improved because confidence is explicit. |
| **End User (PROTEUS job seeker)** | Manual resume adaptation takes 30+ min per application, no feedback loop | PROTEUS adapts resume in <1 min with ATS scoring, tracks application outcomes, learns from feedback | Application volume: 250 apps in 6 months (manual) → 25+ apps/week (automated). Time per application: 30 min → 1 min. Interview rate: 0% → target 2-5%. |

---

## Section 2: Architecture Overview

### 2.1 Three Pillars Framework

GAIA uses a **Three Pillars + Shared Spine** architecture. Projects flow through three stages with constitutional governance at every step:

```
┌─────────────────────────────────────────────────────────────────────┐
│                         GAIA ECOSYSTEM                              │
│                  Constitutional Governance Layer                    │
│         (5 Trust Principles enforced across all pillars)            │
└─────────────────────────────────────────────────────────────────────┘
                                   │
          ┌────────────────────────┼────────────────────────┐
          │                        │                        │
     ┌────▼─────┐            ┌────▼─────┐            ┌────▼─────┐
     │ VULCAN   │            │  LOOM    │            │  ARGUS   │
     │  (The    │  creates   │  (The    │ observes   │  (The    │
     │  Forge)  │───────────>│  Loom)   │<───────────│ Watchman)│
     │          │            │          │            │          │
     │ PROJECT  │            │ WORKFLOW │            │ MENTAL   │
     │ CREATOR  │            │  ENGINE  │            │  MODELS  │
     │          │            │          │            │ PATTERNS │
     └────┬─────┘            └────┬─────┘            └────┬─────┘
          │                       │                       │
          │        ENFORCES       │        ENFORCES       │
          │     Constitution      │      Glass-Box        │
          │      at Birth         │     Transparency      │
          │                       │                       │
          └───────────────────────┼───────────────────────┘
                                  │
                      ┌───────────▼────────────┐
                      │    SHARED SPINE        │
                      ├────────────────────────┤
                      │ MYCEL (Intelligence)   │
                      │   - LLM clients        │
                      │   - Chunking/retrieval │
                      │   - Configuration      │
                      ├────────────────────────┤
                      │ MNEMIS (Memory)        │
                      │   - 5-tier hierarchy   │
                      │   - Promotion protocol │
                      │   - Provenance         │
                      ├────────────────────────┤
                      │ WARDEN (Enforcement)   │
                      │   - Git safety         │
                      │   - Test coverage      │
                      │   - Secrets scanning   │
                      ├────────────────────────┤
                      │ Mental Models (59)     │
                      │   - Context-aware      │
                      │   - 4-level explain    │
                      │   - Growth Rung map    │
                      └────────────────────────┘
                                  │
          ┌───────────────────────┼────────────────────────┐
          │                       │                        │
     ┌────▼─────┐           ┌────▼─────┐           ┌─────▼─────┐
     │ HART OS  │           │   VIA    │           │  PROTEUS  │
     │ Therapy  │           │Investment│           │  Resume   │
     │ v6.2.8   │           │  v6.4    │           │  v0.2.1   │
     └──────────┘           └──────────┘           └───────────┘
     Production             Production             Active Dev
```

#### 2.1.1 VULCAN — The Forge (Project Creator)

**Purpose:** Project creation is the only way new projects enter the ecosystem.

**Method:**
1. User launches VULCAN Streamlit UI (`streamlit run ui/main.py`)
2. Answers 7-step HITL (Human-in-the-Loop) questionnaire:
   - Project name, description, type (Deterministic / Creative / Processor)
   - LLM providers (OpenAI / Anthropic / Gemini / None)
   - Constraints (cost budget, compliance requirements, test coverage target)
   - Decision gates (user approves file generation before write)
3. VULCAN selects appropriate adapter (`DeterministicAdapter` / `CreativeAdapter` / `ProcessorAdapter`)
4. Adapter generates project scaffold with GAIA meta-structure
5. User reviews generated files (Glass-Box Decision Gate)
6. VULCAN writes files, initializes git, registers in `X:\Projects\_GAIA\registry.json`

**Enforces at Birth:**
- ✅ `logs/` directory for JSONL telemetry (ARGUS-ready)
- ✅ `CLAUDE.md` with structured context (project role, constraints, coding patterns)
- ✅ Standard GAIA structure (`config.py` with GaiaSettings, `main package/`, `tests/`, `docs/`)
- ✅ MYCEL integration (`from rag_intelligence import create_llm_client, GaiaSettings`)
- ✅ `.gitignore` with hardened secrets protection (`.env`, `.env.*`, `*.jsonl`, `logs/`, `output/`)
- ✅ `requirements.txt` including `rag-intelligence>=0.3.1`
- ✅ Git repository initialized (`git init`, `git add .`, `git commit -m "Initial scaffold"`)

**Status:**
- v0.4.0 Operational (Feb 4, 2026)
- 19,830 lines of production code
- 137 tests, 85% coverage
- 3 adapters fully implemented
- Retroactive registration modes (Registry-Only, GAIA-Lite for existing projects)

**Files:**
- `X:\Projects\_GAIA\_VULCAN\vulcan_forge\project_creator.py` (main orchestrator)
- `X:\Projects\_GAIA\_VULCAN\vulcan_forge\questionnaire.py` (7-step HITL)
- `X:\Projects\_GAIA\_VULCAN\vulcan_forge\adapters\*.py` (3 adapters)

---

#### 2.1.2 LOOM — The Loom (Workflow Engine)

**Purpose:** Projects get modified and evolved after creation.

**Method:**
1. User opens LOOM visual editor (Streamlit UI — planned)
2. Loads existing agent workflow from project (e.g., HART OS decision engine)
3. Visualizes agents as nodes, wires as connections
4. Edits agent logic, rewires connections, modifies memory schemas
5. LOOM enforces contracts (input/output schemas) and governance rules (cost limits, rate limits)
6. User previews changes (Glass-Box Decision Gate)
7. LOOM saves modified workflow, logs telemetry to ARGUS

**Enforces at Runtime:**
- ✅ **Agent Contracts:** Every agent has explicit `AgentInputSchema` and `AgentOutputSchema` (Pydantic models)
- ✅ **Governance Rules:** `GovernanceRule` enforces cost_limit, rate_limit, approval_required per agent
- ✅ **Agent Authority:** Hierarchical execution (Executor → Observer → Coordinator → Transformer)
- ✅ **State Management:** `StateManager` persists workflow versions, allows rollback
- ✅ **Audit Trail:** All workflow executions logged to ARGUS telemetry

**Status:**
- v1.0.0 Operational (Phase 3 complete, Feb 4, 2026)
- Workflow engine implemented
- Agent authority system operational
- Telemetry hooks integrated with ARGUS
- Visual editor UI: Phase 4 (planned)

**Files:**
- `X:\Projects\_GAIA\_LOOM\loom\core\workflow_engine.py` (execution engine)
- `X:\Projects\_GAIA\_LOOM\loom\models\agent_models.py` (AgentNode, AgentWorkflow)
- `X:\Projects\_GAIA\_LOOM\loom\core\state_manager.py` (persistence)

---

#### 2.1.3 ARGUS — The Watchman (Mental Models + Observability)

**Purpose:** Mental model library and subconscious pattern detection.

**Method:**
1. **Mental Model Library:** 59 models across 6 categories (Cognitive, Behavioral, Systems, Decision-Making, Communication, Business)
2. **Context-Aware Selection:** `MentalModelSelector.select_for_context()` scores models based on problem domain, user expertise, constraints
3. **Layered Explainability:** 4 levels (Simple → Detailed → Technical → Debug) mapped to Growth Rungs 1-4
4. **Subconscious Layer:** External memory (SQLite), pattern detection, hypothesis generation
5. **Process Observer:** Read-only telemetry from LOOM workflows, post-mortem analysis

**Enforces at Explanation:**
- ✅ **Growth Rung Mapping:** Explanations adapt to user's technical level (Rung 1 = Simple metaphors, Rung 4 = Full technical detail)
- ✅ **Hypothesis-Driven:** ARGUS never states certainties, always "suggests", "may indicate", "hypothesis"
- ✅ **Read-Only Observer:** ARGUS cannot execute tools or modify state
- ✅ **Memory Boundaries:** Cannot write to GAIA-tier memory without promotion proposal

**Status:**
- v0.5.0 Operational (Phase 2 complete, Feb 4, 2026)
- 59 mental models implemented
- Layered explainability system operational
- Subconscious layer (external memory, pattern detection) operational
- Trust dashboard: Phase 4 (planned)

**Files:**
- `X:\Projects\_GAIA\_ARGUS\mental_models\registry.json` (59 model definitions)
- `X:\Projects\_GAIA\_ARGUS\argus\subconscious\memory.py` (external memory SQLite)
- `X:\Projects\_GAIA\_ARGUS\argus\explainability\explainer.py` (4-level system)

---

### 2.2 Shared Spine

The Shared Spine provides unified services to all three pillars and all GAIA products.

#### 2.2.1 MYCEL — Shared Intelligence Library

**Purpose:** Unified LLM clients, configuration, chunking, retrieval.

**Provides:**
- `create_llm_client(provider: str, model: str)` → returns unified client for OpenAI / Anthropic / Gemini
- `GaiaSettings` (pydantic-settings) — standardized config loading from `.env`
- `Chunk` model with `source`, `timestamp`, `metadata` — chunking/retrieval abstraction
- Embedding utilities, vector search utilities (Phase 4+)

**Status:**
- v0.3.1 Operational
- Supports 3 providers (OpenAI, Anthropic, Gemini)
- Used by: HART OS, VIA, DATA FORGE, PROTEUS

**Files:**
- `X:\Projects\_GAIA\_MYCEL\rag_intelligence\llm_client.py`
- `X:\Projects\_GAIA\_MYCEL\rag_intelligence\config.py`
- `X:\Projects\_GAIA\_MYCEL\rag_intelligence\chunking.py`

---

#### 2.2.2 MNEMIS — Cross-Project Memory

**Purpose:** Hierarchical memory with explicit promotion discipline.

**Memory Hierarchy (5 Tiers):**
1. **Ephemeral** (AGENT tier, in-memory, TTL < 1 hour)
2. **Working** (AGENT tier, in-memory, TTL < 4 hours)
3. **Session** (PROJECT tier, JSONL persistence, TTL < 24 hours)
4. **Long-term** (PROJECT tier, JSONL persistence, TTL unlimited)
5. **Permanent** (GAIA tier, JSONL persistence, shared across all projects)

**Access Control:**
- **Read Permissions (DOWN):** GAIA agents → read GAIA + PROJECT + AGENT. PROJECT agents → read PROJECT + AGENT. AGENT → read AGENT only.
- **Write Permissions (EXACT LEVEL):** GAIA agents → write GAIA only. PROJECT agents → write PROJECT only. AGENT → write AGENT only.
- **Promotion Workflow:** AGENT → proposes to PROJECT → PROJECT agent reviews → approve/reject. PROJECT → proposes to GAIA → Human/GAIA reviews → approve/reject.

**Status:**
- v1.0.0 Operational (Phase 3 complete, Feb 4, 2026)
- 5-tier hierarchy implemented
- Memory contracts enforced at runtime
- Promotion protocol operational
- Cross-project search via `MemorySearchEngine.search_by_tags()`

**Files:**
- `X:\Projects\_GAIA\_MNEMIS\mnemis\core\memory_store.py`
- `X:\Projects\_GAIA\_MNEMIS\mnemis\core\contracts.py` (access control)
- `X:\Projects\_GAIA\_MNEMIS\mnemis\core\promotion.py` (promotion engine)

---

#### 2.2.3 WARDEN — Governance & Enforcement

**Purpose:** Constitutional compliance enforcement.

**Validates (Planned):**
1. **Git Status:** No uncommitted changes in production projects
2. **Test Suite:** All tests passing, coverage > 80%
3. **Secrets Safety:** No API keys in code or git history
4. **Dependency Freshness:** No critical CVEs in dependencies
5. **Documentation:** CLAUDE.md exists, config.py has GaiaSettings, tests/ directory exists

**Pre-Commit Hooks (Phase 4):**
- Block commits if secrets detected
- Block commits if tests fail
- Block commits if coverage drops below threshold

**Status:**
- v0.1.0 Planned
- Architecture defined in GAIA_BIBLE.md
- No implementation yet

**Files:**
- `X:\Projects\_GAIA\_WARDEN\` (placeholder)

---

#### 2.2.4 Mental Models (59 Models)

**Categories:**
1. **Cognitive (12 models):** Mental Accounting, Anchoring Bias, Availability Heuristic, Confirmation Bias, Dunning-Kruger Effect, Sunk Cost Fallacy, Loss Aversion, Recency Bias, Hindsight Bias, Framing Effect, Cognitive Load Theory, Working Memory Model
2. **Behavioral (10 models):** Operant Conditioning, Classical Conditioning, Social Learning Theory, Habit Loop, Implementation Intentions, Temporal Discounting, Goal Setting Theory, Self-Efficacy Theory, Reactance Theory, Nudge Theory
3. **Systems Thinking (11 models):** Feedback Loops, Stock and Flow, System Archetypes, Leverage Points, Causal Loop Diagrams, Tragedy of the Commons, Limits to Growth, Network Effects, Emergent Properties, Homeostasis, System Boundaries
4. **Decision-Making (10 models):** Expected Utility Theory, Prospect Theory, Decision Trees, Cost-Benefit Analysis, Multi-Criteria Decision Analysis, Six Thinking Hats, Eisenhower Matrix, SWOT Analysis, Decision Matrix, Pareto Principle
5. **Communication (8 models):** Active Listening, Nonviolent Communication, Socratic Questioning, Storytelling Framework, Feedback Loops, Persuasion Techniques, Conflict Resolution, Negotiation Strategies
6. **Business (8 models):** Business Model Canvas, Value Chain Analysis, Porter's Five Forces, Blue Ocean Strategy, Lean Startup, OKRs, SWOT Analysis, Scenario Planning

**Context-Aware Selection:**
- `MentalModelSelector.select_for_context(problem_description, user_expertise, constraints)`
- Scores models based on relevance, complexity, prerequisites, use cases
- Returns ranked list with explanation

**Integration:** Used by ARGUS explainability layer, LOOM workflow hints, HART OS therapeutic reasoning

---

### 2.3 Data Flow Diagrams

#### 2.3.1 Creation Flow

```
User Intent
    │
    ▼
┌─────────────────────────────┐
│  VULCAN Streamlit UI        │
│  (7-step questionnaire)     │
└──────────┬──────────────────┘
           │
           │ User answers:
           │ - Project name, description, type
           │ - LLM providers
           │ - Constraints (cost, compliance)
           │
           ▼
┌─────────────────────────────┐
│  Adapter Selection          │
│  (Deterministic/Creative/   │
│   Processor)                │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  Scaffold Generation        │
│  - config.py (GaiaSettings) │
│  - main package structure   │
│  - tests/, docs/, logs/     │
│  - CLAUDE.md                │
│  - .gitignore               │
│  - requirements.txt         │
└──────────┬──────────────────┘
           │
           │ Glass-Box Decision Gate
           │ (User reviews files)
           │
           ▼
┌─────────────────────────────┐
│  File Write + Git Init      │
│  - Write files to disk      │
│  - git init                 │
│  - git add .                │
│  - git commit               │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  Registry Update            │
│  - Add entry to registry.json│
│  - Status: development      │
│  - Dependencies: [mycel]    │
└──────────┬──────────────────┘
           │
           ▼
    GAIA-Compliant
    Project Ready
```

---

#### 2.3.2 Execution Flow

```
Product (HART OS / VIA / PROTEUS)
    │
    │ Needs LLM inference
    │
    ▼
┌─────────────────────────────┐
│  MYCEL LLM Client           │
│  create_llm_client(         │
│    provider="anthropic",    │
│    model="claude-3-haiku"   │
│  )                          │
└──────────┬──────────────────┘
           │
           │ API call
           │
           ▼
┌─────────────────────────────┐
│  LLM Provider               │
│  (OpenAI / Anthropic /      │
│   Gemini)                   │
└──────────┬──────────────────┘
           │
           │ Response
           │
           ▼
┌─────────────────────────────┐
│  LOOM Workflow Engine       │
│  (if workflow-based)        │
│  - Execute agent with       │
│    contracts enforced       │
│  - Log to telemetry         │
└──────────┬──────────────────┘
           │
           │ Telemetry event
           │ {timestamp, agent_id,
           │  model, tokens, cost}
           │
           ▼
┌─────────────────────────────┐
│  ARGUS Event Bus            │
│  - Write to logs/*.jsonl    │
│  - Pattern detection        │
└──────────┬──────────────────┘
           │
           │ Detected pattern
           │
           ▼
┌─────────────────────────────┐
│  MNEMIS Memory Store        │
│  - Store to AGENT tier      │
│  - Proposal for promotion   │
│    if pattern recurs        │
└──────────┬──────────────────┘
           │
           ▼
    Result returned
    to Product
```

---

#### 2.3.3 Learning Flow

```
ARGUS Pattern Detector
    │
    │ Monitors logs/*.jsonl
    │ across all projects
    │
    ▼
┌─────────────────────────────┐
│  Pattern Detection          │
│  Example:                   │
│  "VIA called Anthropic 47   │
│   times with same 3-chunk   │
│   context pattern"          │
└──────────┬──────────────────┘
           │
           │ Detected pattern
           │
           ▼
┌─────────────────────────────┐
│  MNEMIS Promotion Proposal  │
│  - Pattern stored in AGENT  │
│    tier (VIA)               │
│  - Proposal: promote to     │
│    PROJECT tier             │
│  - Rationale: occurred 47x  │
│    in 3 sessions             │
└──────────┬──────────────────┘
           │
           │ Human/Agent review
           │
           ▼
┌─────────────────────────────┐
│  Promotion Approved         │
│  - Pattern moved to PROJECT │
│    tier (VIA long-term      │
│    memory)                  │
│  - Proposal: promote to GAIA│
│    tier if pattern appears  │
│    in 2+ projects           │
└──────────┬──────────────────┘
           │
           │ Pattern appears in
           │ DATA FORGE (same 3-chunk
           │ context pattern)
           │
           ▼
┌─────────────────────────────┐
│  GAIA-Tier Promotion        │
│  - Pattern now shared       │
│    ecosystem-wide           │
│  - VULCAN suggests pattern  │
│    when creating new        │
│    RAG-based projects       │
└──────────┬──────────────────┘
           │
           ▼
    Institutional
    Learning Complete
```

---

#### 2.3.4 Governance Flow

```
Pre-Commit Hook (WARDEN)
    │
    ▼
┌─────────────────────────────┐
│  Git Pre-Commit Checks      │
│  1. Secrets scan            │
│  2. Test suite              │
│  3. Coverage check          │
│  4. CLAUDE.md exists        │
└──────────┬──────────────────┘
           │
           │ Violation detected
           │ (e.g., ".env" in
           │  staged files)
           │
           ▼
┌─────────────────────────────┐
│  Commit Blocked             │
│  - Log to ARGUS telemetry   │
│  - Display error to user    │
│  - Exit code 1              │
└──────────┬──────────────────┘
           │
           │ User fixes issue
           │ (removes .env from
           │  staged files)
           │
           ▼
┌─────────────────────────────┐
│  Re-run Pre-Commit Checks   │
│  - All checks pass          │
│  - Commit allowed           │
└──────────┬──────────────────┘
           │
           ▼
┌─────────────────────────────┐
│  ARGUS Trust Dashboard      │
│  - Show compliance status   │
│  - "HART OS: 14 days since  │
│    last violation"          │
│  - "VIA: 100% test coverage"│
└─────────────────────────────┘
```

---

### 2.4 Module Dependency Graph

```
                    ┌──────────────┐
                    │   WARDEN     │
                    │ (governance) │
                    └───────┬──────┘
                            │
                            │ enforces
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
        ▼                   ▼                   ▼
   ┌─────────┐         ┌─────────┐        ┌─────────┐
   │ VULCAN  │         │  LOOM   │        │ ARGUS   │
   │ depends │         │ depends │        │ depends │
   │   on    │         │   on    │        │   on    │
   └────┬────┘         └────┬────┘        └────┬────┘
        │                   │                   │
        └─────────┬─────────┴─────────┬─────────┘
                  │                   │
                  ▼                   ▼
            ┌──────────┐        ┌──────────┐
            │  MYCEL   │        │ MNEMIS   │
            │          │◄───────│          │
            └────┬─────┘        └────┬─────┘
                 │                   │
                 │ provides          │ depends on
                 │ LLM client        │ for storage
                 │                   │
                 └───────────┬───────┘
                             │
        ┌────────────────────┼────────────────────┐
        │                    │                    │
        ▼                    ▼                    ▼
   ┌─────────┐         ┌─────────┐         ┌─────────┐
   │ HART OS │         │   VIA   │         │ PROTEUS │
   │ v6.2.8  │         │  v6.4   │         │ v0.2.1  │
   └─────────┘         └─────────┘         └─────────┘

Dependency Rules:
- MYCEL has ZERO dependencies (foundational library)
- MNEMIS depends ONLY on MYCEL (for config)
- VULCAN/LOOM/ARGUS depend on MYCEL + MNEMIS
- WARDEN depends on nothing (scans projects externally)
- Products (HART OS/VIA/PROTEUS) depend on MYCEL (required)
  and optionally MNEMIS (Phase 3+)
```

---

### 2.5 GAIA Naming Registry

| Name | Role | Status | Version | Origin | Dependencies |
|------|------|--------|---------|--------|--------------|
| **GAIA** | Ecosystem master layer | Operational | v0.4.3 | Greek primordial earth deity | — |
| **VULCAN** | Project Creator (The Forge) | Operational | v0.4.0 | Roman god of the forge | MYCEL |
| **LOOM** | Workflow Engine (The Loom) | Operational | v1.0.0 | Mythic weaver of fate | MYCEL, MNEMIS |
| **ARGUS** | Mental Models + Observability (The Watchman) | Operational | v0.5.0 | Greek 100-eyed watchman | MYCEL |
| **MYCEL** | Shared Intelligence Library | Operational | v0.3.1 | Mycelium—nature's neural network | — |
| **MNEMIS** | Cross-Project Memory | Operational | v1.0.0 | Mnemosyne, titan of memory | MYCEL |
| **WARDEN** | Governance & Enforcement | Planned | v0.0.0 | Forest guardian | — |
| **RAVEN** | Autonomous Research Agent | Planned | v0.0.1 | Odin's knowledge-gathering ravens | MYCEL |

**Existing Products (Keep Names):**
- HART OS (therapy assistant)
- VIA (investment research)
- DATA FORGE (data processing)
- THE PALACE (case study)
- jSeeker (formerly PROTEUS, resume engine)
- GPT_ECHO (ChatGPT archaeology, reclassified from shared service)

---

### 2.6 Technology Stack

| Layer | Technology | Version | Purpose | Used By |
|-------|------------|---------|---------|---------|
| **Language** | Python | 3.10+ | All GAIA modules and products | All |
| **UI Framework** | Streamlit | 1.28+ | Web interfaces | VULCAN, ARGUS, HART OS, VIA, PROTEUS |
| **LLM Providers** | OpenAI | Latest | GPT-4, GPT-3.5 | HART OS, DATA FORGE |
| | Anthropic | Latest | Claude 3 (Haiku/Sonnet/Opus) | VIA, PROTEUS |
| | Google Gemini | Latest | Gemini 1.5 | VIA, GPT_ECHO |
| **Config** | pydantic-settings | 2.x | Type-safe config loading | All (GaiaSettings) |
| **Data Validation** | Pydantic | 2.x | Schemas, validation | All (AgentNode, MemoryEntry, Chunk) |
| **Testing** | pytest | 7.x | Unit/integration tests | All |
| | pytest-cov | 4.x | Coverage reporting | All |
| **Storage** | SQLite | 3.x | External memory, telemetry | ARGUS, MNEMIS, PROTEUS |
| | JSONL | — | Structured logs | All (logs/*.jsonl) |
| **Document Generation** | Playwright | 1.40+ | PDF rendering | PROTEUS |
| | python-docx | 0.8+ | DOCX generation | PROTEUS |
| **Version Control** | Git | 2.x | All GAIA projects | Required by WARDEN |
| **RAG/Embeddings** | OpenAI Embeddings | Latest | Vector search | VIA (Phase 4+) |
| **HTTP** | httpx | 0.25+ | Async API calls | MYCEL |
| **CLI** | argparse | stdlib | Command-line tools | WARDEN |

---

## Section 3: Trust Contract & Constitutional Governance

### 3.1 Five Trust Principles

GAIA's constitutional governance is built on five immutable trust principles. These are not aspirational values—they are enforced at design-time, runtime, and audit-time across every module.

---

#### 3.1.1 Trust Principle 1: GAIA Never Lies

**Definition:**
GAIA components never fabricate data, hallucinate facts, or present speculation as certainty. All claims are traceable to source data, and uncertainty is made explicit.

**Implementation (Current):**

| Module | How Enforced |
|--------|--------------|
| **VULCAN** | Adapter templates include citation placeholders. All generated code comments include `# Source: [adapter_name]`. No marketing claims ("best", "fastest") allowed in generated CLAUDE.md. |
| **LOOM** | Agent output schemas require `source_agent_id` field. Workflow execution logs capture full provenance chain (Agent A → Agent B → Agent C). |
| **ARGUS** | Mental model explanations include `confidence_level` (Low/Medium/High). Pattern detection returns `DetectedPattern.evidence` with raw data. Hypothesis generator ALWAYS uses "suggests", "may indicate", never "proves". |
| **MNEMIS** | Every `MemoryEntry` has `provenance: ProvenanceMetadata` (creator_agent_id, created_at, source_data_hash). Promotion proposals include rationale with evidence. |
| **MYCEL** | LLM responses include `response_metadata` (model, tokens, timestamp, cost). Chunk model includes `source: str` field (required, no default). |
| **HART OS** | All therapy plans include canonical citations (DSM-5-TR §X.Y, modality name). Scoring rubrics show formula + inputs in UI. |
| **VIA** | Semantic claims link to source documents. Synthesis indicates "3 sources agree" vs "1 source, minority view". |
| **jSeeker** (formerly PROTEUS) | Resume adaptation logs show before/after diff. ATS scoring shows per-platform breakdown (Greenhouse: 78/100, Workday: 82/100). |

**Monitoring (How ARGUS Tracks):**
- ARGUS scans all JSONL telemetry for unattributed claims
- Pattern detector flags responses with no `source` metadata
- Monthly audit report: % of LLM calls with provenance vs without

**Enforcement (How WARDEN Ensures):**
- Pre-commit hook scans Python files for hardcoded claims ("best", "always", "never" without citation)
- Pytest fixtures require `source` field in test data
- Production deployment blocked if >5% of recent LLM calls lack provenance

**Example (Good vs Bad):**

❌ **Bad (ARGUS Hypothesis Generator):**
```python
return "The user is experiencing cognitive overload."
```

✅ **Good:**
```python
return DetectedPattern(
    hypothesis="User may be experiencing cognitive overload (confidence: Medium)",
    evidence=[
        "3 consecutive questions without action taken",
        "Average response time: 4.2 min (baseline: 1.8 min)",
        "Mental model invoked: Cognitive Load Theory"
    ],
    confidence_level=ConfidenceLevel.MEDIUM,
    model_id="cognitive_load_theory"
)
```

---

#### 3.1.2 Trust Principle 2: GAIA Admits Limits

**Definition:**
GAIA components explicitly declare their boundaries. They don't silently fail, degrade without warning, or pretend competence they lack.

**Implementation (Current):**

| Module | How Enforced |
|--------|--------------|
| **VULCAN** | Adapters declare supported features (`supports_streaming: bool`, `supports_function_calling: bool`). Questionnaire warns if user selects incompatible options ("Gemini 1.0 does not support function calling"). |
| **LOOM** | Agent contracts declare `required_inputs` and `optional_inputs`. Execution fails early if required input missing. Governance rules include `max_retries: int` (explicit limit). |
| **ARGUS** | Mental model metadata includes `prerequisites: List[str]`. Selector refuses to invoke model if prerequisites not met ("Causal Loop Diagrams requires Systems Thinking Fundamentals"). Explainer declares `max_complexity: GrowthRung` (won't oversimplify). |
| **MNEMIS** | Access controller declares read/write permissions per agent tier. Promotion proposals include `approval_required: bool`. TTL expiration is HARD (not soft—memory is deleted, not archived). |
| **MYCEL** | LLM client declares `supports_vision: bool`, `max_tokens: int`, `supports_streaming: bool`. Raises `UnsupportedFeatureError` if user requests vision from text-only model. |

**Monitoring (How ARGUS Tracks):**
- ARGUS logs all `UnsupportedFeatureError` exceptions
- Pattern detector flags repeated attempts to use unsupported features
- Dashboard shows per-project "limits encountered" count

**Enforcement (How WARDEN Ensures):**
- Pytest tests for all declared limits (e.g., test that Gemini 1.0 raises error if function_calling=True)
- Integration tests verify graceful degradation (e.g., LOOM workflow continues if optional input missing)
- Production deployment blocked if any module lacks declared limits

**Example (Good vs Bad):**

❌ **Bad (MYCEL LLM Client):**
```python
def create_llm_client(provider: str, model: str):
    if provider == "gemini":
        # Silently ignores function_calling parameter
        return GeminiClient(model)
```

✅ **Good:**
```python
def create_llm_client(
    provider: str,
    model: str,
    function_calling: bool = False
):
    if provider == "gemini" and function_calling:
        if model.startswith("gemini-1.0"):
            raise UnsupportedFeatureError(
                f"Model {model} does not support function calling. "
                "Use gemini-1.5-pro or later."
            )
    return GeminiClient(model)
```

---

#### 3.1.3 Trust Principle 3: GAIA Degrades Gracefully

**Definition:**
When GAIA components encounter errors, they degrade gracefully with clear explanations, not silent failures or catastrophic crashes.

**Implementation (Current):**

| Module | How Enforced |
|--------|--------------|
| **VULCAN** | Adapter generation failures show per-file status (✅ config.py generated, ❌ tests/test_main.py failed: reason). User can retry individual files or skip. Partial project is NEVER registered—rollback on failure. |
| **LOOM** | Agent execution failures captured in `ExecutionResult.error: Optional[str]`. Workflow continues with fallback agent or user notification. StateManager allows rollback to last known-good state. |
| **ARGUS** | Mental model selection failures fall back to "Default" explanation (no mental model). Pattern detector failures logged but don't block observation. Explainer degradation: Technical → Detailed → Simple (never fails to explain). |
| **MNEMIS** | Memory write failures logged, operation retried 3x, then stored in dead-letter queue for manual review. Memory read failures return `None` + log warning (never crash). Promotion proposal failures → proposal saved, human review required. |
| **MYCEL** | LLM API failures: retry with exponential backoff (3x), then fall back to cheaper model (Opus → Sonnet → Haiku), then raise clear error with next steps. Network errors: cache last response for 5 min. |

**Monitoring (How ARGUS Tracks):**
- ARGUS logs all degradation events (`DegradationEvent.module`, `.severity`, `.fallback_action`)
- Dashboard shows "graceful degradations in last 24h" per module
- Alert if >10 degradations/hour (indicates systemic issue)

**Enforcement (How WARDEN Ensures):**
- Pytest tests for all failure modes (e.g., mock API timeout, verify fallback)
- Integration tests verify partial failures don't corrupt state
- Production deployment blocked if any module has silent failure path (must log + degrade explicitly)

**Example (Good vs Bad):**

❌ **Bad (LOOM Workflow Engine):**
```python
def execute_agent(agent: AgentNode, inputs: dict):
    result = agent.run(inputs)  # Crashes if agent.run() raises exception
    return result
```

✅ **Good:**
```python
def execute_agent(agent: AgentNode, inputs: dict) -> ExecutionResult:
    try:
        result = agent.run(inputs)
        return ExecutionResult(
            success=True,
            output=result,
            error=None
        )
    except AgentExecutionError as e:
        # Log to ARGUS telemetry
        telemetry.log_agent_failure(agent.id, str(e))

        # Attempt fallback
        if agent.fallback_agent_id:
            return execute_agent(
                get_agent(agent.fallback_agent_id),
                inputs
            )

        # No fallback—return clear error
        return ExecutionResult(
            success=False,
            output=None,
            error=f"Agent {agent.id} failed: {e}. No fallback available."
        )
```

---

#### 3.1.4 Trust Principle 4: GAIA Learns Explicitly

**Definition:**
GAIA components learn from patterns, but learning is NEVER silent. All pattern detection, memory promotion, and model updates are logged and auditable.

**Implementation (Current):**

| Module | How Enforced |
|--------|--------------|
| **VULCAN** | When creating new project, shows "ARGUS detected similar project: VIA (RAG-based). Suggest CreativeAdapter?" User approves or overrides. Learning source logged to registry (`created_with_suggestion_from: "via"`). |
| **LOOM** | Workflow state changes logged to `workflow_versions/` JSONL. User sees diff before applying (e.g., "Agent 3 memory schema changed: +long_term_tier"). Learning from past workflows shown as "MNEMIS suggests: this workflow pattern succeeded 4x in HART OS". |
| **ARGUS** | Pattern detection creates `DetectedPattern` with full provenance. User reviews patterns before promotion to PROJECT tier. Mental model selection shows "Selected Cognitive Load Theory because: [scoring breakdown]". |
| **MNEMIS** | Promotion proposals require human approval. Promotion log shows (`memory_id`, `from_tier`, `to_tier`, `approved_by`, `rationale`, `timestamp`). Automatic expiration logged (`memory_id`, `expired_at`, `reason: "TTL exceeded"`). |
| **MYCEL** | No learning (stateless library by design). Configuration changes logged to `config_history.jsonl`. |

**Monitoring (How ARGUS Tracks):**
- ARGUS dashboard shows "learning events in last 7 days" (pattern promotions, workflow modifications, model selection overrides)
- Trend analysis: "VIA promoted 3 patterns to GAIA tier this week (vs avg 1/week)"
- Audit trail for all learning: who approved, when, rationale

**Enforcement (How WARDEN Ensures):**
- Pre-commit hook verifies all learning events are logged (no silent pattern updates)
- Pytest tests verify promotion workflow requires approval
- Production deployment blocked if learning bypasses approval workflow

**Example (Good vs Bad):**

❌ **Bad (MNEMIS Memory Promotion):**
```python
def promote_memory(memory_id: str, from_tier: MemoryTier, to_tier: MemoryTier):
    # Silent promotion—no approval, no logging
    memory = store.get(memory_id)
    memory.tier = to_tier
    store.save(memory)
```

✅ **Good:**
```python
def propose_promotion(
    memory_id: str,
    from_tier: MemoryTier,
    to_tier: MemoryTier,
    rationale: str
) -> PromotionProposal:
    """Propose memory promotion (requires approval)."""
    proposal = PromotionProposal(
        id=generate_id(),
        memory_id=memory_id,
        from_tier=from_tier,
        to_tier=to_tier,
        rationale=rationale,
        proposed_at=datetime.now(),
        status=ProposalStatus.PENDING
    )
    store.save_proposal(proposal)
    telemetry.log_promotion_proposed(proposal)
    return proposal

def approve_promotion(proposal_id: str, approved_by: str) -> bool:
    """Approve promotion proposal (explicit learning)."""
    proposal = store.get_proposal(proposal_id)
    proposal.status = ProposalStatus.APPROVED
    proposal.approved_by = approved_by
    proposal.approved_at = datetime.now()

    # Execute promotion
    memory = store.get(proposal.memory_id)
    memory.tier = proposal.to_tier
    store.save(memory)

    # Log learning event
    telemetry.log_promotion_approved(proposal)
    return True
```

---

#### 3.1.5 Trust Principle 5: GAIA Remains Inspectable

**Definition:**
GAIA components are glass-box by default. Every decision, every workflow, every memory operation can be inspected, explained, and traced to source logic.

**Implementation (Current):**

| Module | How Enforced |
|--------|--------------|
| **VULCAN** | Generated projects include `CLAUDE.md` with full context. Adapter logic visible in `vulcan_forge/adapters/*.py`. User reviews all generated files before write (Glass-Box Decision Gate). Registry entry shows full metadata (providers, dependencies, tags). |
| **LOOM** | Agent contracts (`AgentInputSchema`, `AgentOutputSchema`) are explicit Pydantic models. Workflow graph visualized (planned Streamlit UI). Execution logs show full trace (Agent A [inputs] → Agent B [outputs]). State versions stored in JSONL (can diff any two versions). |
| **ARGUS** | Mental model library is human-readable JSON (`mental_models/registry.json`). Selector scoring algorithm is transparent (shows per-model score breakdown). Explainer provides 4 levels (user chooses complexity). Pattern detection shows evidence, not just conclusion. |
| **MNEMIS** | Memory access logs show (agent_id, operation, tier, timestamp). Promotion proposals include full provenance. Search results show relevance score + reasoning. Memory schemas are Pydantic models (inspectable in Python REPL). |
| **MYCEL** | LLM client is thin wrapper (no magic). Configuration is pydantic-settings (inspectable via `.model_dump()`). Chunk model is transparent (source, timestamp, metadata all required fields). |

**Monitoring (How ARGUS Tracks):**
- ARGUS dashboard shows "inspections requested in last 7 days" (how many times user drilled into telemetry, viewed mental model, diffed workflow versions)
- User behavior: "Users spending avg 4.2 min inspecting agent traces" (indicates engagement with transparency)

**Enforcement (How WARDEN Ensures):**
- Pre-commit hook verifies all Pydantic models have docstrings
- Pytest tests verify all decision points have logged reasoning
- Production deployment blocked if any critical decision lacks audit trail

**Example (Good vs Bad):**

❌ **Bad (ARGUS Mental Model Selection):**
```python
def select_model(problem: str) -> str:
    # Black-box selection—no explanation
    return "cognitive_load_theory"
```

✅ **Good:**
```python
def select_for_context(
    problem_description: str,
    user_expertise: GrowthRung,
    constraints: List[str]
) -> ModelSelectionResult:
    """Select mental model with transparent scoring."""

    # Score all models
    scores = []
    for model in registry.get_all_models():
        score = 0.0
        reasoning = []

        # Relevance scoring
        if model.category in extract_categories(problem_description):
            score += 0.4
            reasoning.append(f"+0.4: Category match ({model.category})")

        # Complexity scoring
        if model.complexity <= user_expertise:
            score += 0.3
            reasoning.append(f"+0.3: Complexity appropriate ({model.complexity} <= {user_expertise})")
        else:
            score -= 0.2
            reasoning.append(f"-0.2: Too complex ({model.complexity} > {user_expertise})")

        # Prerequisites scoring
        if all(p in user_knowledge for p in model.prerequisites):
            score += 0.3
            reasoning.append(f"+0.3: Prerequisites met")

        scores.append(ModelScore(
            model_id=model.id,
            score=score,
            reasoning=reasoning
        ))

    # Return top model with full transparency
    top = max(scores, key=lambda s: s.score)
    return ModelSelectionResult(
        selected_model_id=top.model_id,
        score=top.score,
        reasoning=top.reasoning,
        alternatives=sorted(scores, key=lambda s: s.score, reverse=True)[:3]
    )
```

---

### 3.2 Authority Graph — Agent Levels 0-4

GAIA uses a hierarchical authority system mapped to memory tiers. Higher authority = broader read access + ability to promote memory UP.

```
┌──────────────────────────────────────────────────────────────────┐
│  AUTHORITY LEVEL 4: HUMAN / GAIA CORE                           │
│  ─────────────────────────────────────────────────────────────  │
│  Read: GAIA + PROJECT + AGENT (all tiers)                       │
│  Write: GAIA tier only                                          │
│  Promote: PROJECT → GAIA (with rationale)                       │
│  Examples: Human user, VULCAN, WARDEN                           │
└──────────────────────────────────────────────────────────────────┘
                              │
                              │ delegates to
                              ▼
┌──────────────────────────────────────────────────────────────────┐
│  AUTHORITY LEVEL 3: COORDINATOR                                 │
│  ─────────────────────────────────────────────────────────────  │
│  Read: PROJECT + AGENT                                          │
│  Write: PROJECT tier only                                       │
│  Promote: AGENT → PROJECT (with evidence)                       │
│  Examples: LOOM orchestrator, HART OS decision engine           │
└──────────────────────────────────────────────────────────────────┘
                              │
                              │ orchestrates
                              ▼
┌──────────────────────────────────────────────────────────────────┐
│  AUTHORITY LEVEL 2: EXECUTOR                                    │
│  ─────────────────────────────────────────────────────────────  │
│  Read: AGENT tier only                                          │
│  Write: AGENT tier only                                         │
│  Promote: Cannot promote (must propose to Coordinator)          │
│  Examples: HART OS therapy generator, VIA synthesis agent        │
└──────────────────────────────────────────────────────────────────┘
                              │
                              │ observes
                              ▼
┌──────────────────────────────────────────────────────────────────┐
│  AUTHORITY LEVEL 1: OBSERVER                                    │
│  ─────────────────────────────────────────────────────────────  │
│  Read: AGENT tier only (read-only)                              │
│  Write: Cannot write (external memory only)                     │
│  Promote: Cannot promote                                        │
│  Examples: ARGUS pattern detector, ARGUS hypothesis generator   │
└──────────────────────────────────────────────────────────────────┘
                              │
                              │ transforms
                              ▼
┌──────────────────────────────────────────────────────────────────┐
│  AUTHORITY LEVEL 0: TRANSFORMER                                 │
│  ─────────────────────────────────────────────────────────────  │
│  Read: Input data only (no memory access)                       │
│  Write: Output data only (no memory persistence)                │
│  Promote: N/A (stateless)                                       │
│  Examples: Data validators, format converters, parsers          │
└──────────────────────────────────────────────────────────────────┘
```

**Memory Tier Mapping:**

| Agent Level | Memory Read Access | Memory Write Access | Can Promote? |
|-------------|--------------------|---------------------|--------------|
| **Level 4 (HUMAN/GAIA)** | GAIA + PROJECT + AGENT | GAIA only | Yes (PROJECT → GAIA) |
| **Level 3 (COORDINATOR)** | PROJECT + AGENT | PROJECT only | Yes (AGENT → PROJECT) |
| **Level 2 (EXECUTOR)** | AGENT only | AGENT only | Propose only |
| **Level 1 (OBSERVER)** | AGENT (read-only) | None (external memory) | No |
| **Level 0 (TRANSFORMER)** | None | None | No |

---

### 3.3 Glass-Box Transparency — Concrete Implementation

**What "Glass-Box" Means:**

Glass-box transparency is not a metaphor—it's a concrete set of technical requirements enforced at design-time and runtime.

#### 3.3.1 For VULCAN (Project Creation)

✅ **User sees all files before write**
- Streamlit UI shows file tree with expandable previews
- User can click any file to view full contents
- "Approve" button disabled until user scrolls to bottom (forces review)

✅ **Generated code includes source attribution**
```python
# Generated by VULCAN v0.4.0
# Adapter: DeterministicAdapter
# Date: 2026-02-08
# Source template: templates/deterministic/main.py.jinja2
```

✅ **CLAUDE.md documents all decisions**
- "Why this adapter? DeterministicAdapter selected because project type = 'therapy' (requires deterministic pipeline)"
- "Why these dependencies? rag-intelligence>=0.3.1 (MYCEL integration), pydantic-settings>=2.0 (GaiaSettings)"

#### 3.3.2 For LOOM (Workflow Editing)

✅ **Agent contracts are explicit schemas**
```python
class TherapyGeneratorInput(BaseModel):
    patient_id: str
    session_number: int
    growth_rung: GrowthRung
    previous_session_notes: Optional[str] = None

class TherapyGeneratorOutput(BaseModel):
    therapy_plan: str
    canonical_citations: List[str]
    confidence_score: float
    reasoning: str  # WHY this plan was generated
```

✅ **Workflow execution trace is persistent**
```json
{
  "workflow_id": "therapy_session_1847",
  "started_at": "2026-02-08T14:23:01Z",
  "trace": [
    {
      "agent_id": "intake_parser",
      "inputs": {"raw_intake": "..."},
      "outputs": {"parsed_data": {...}},
      "duration_ms": 124
    },
    {
      "agent_id": "therapy_generator",
      "inputs": {"parsed_data": {...}},
      "outputs": {"therapy_plan": "...", "reasoning": "Selected CBT because..."},
      "duration_ms": 2847,
      "llm_calls": [
        {"model": "claude-3-haiku", "tokens": 1247, "cost": 0.00312}
      ]
    }
  ],
  "completed_at": "2026-02-08T14:23:04Z"
}
```

#### 3.3.3 For ARGUS (Mental Models)

✅ **Model selection is scored transparently**
```python
# User sees this in UI:
ModelSelectionResult(
    selected_model="cognitive_load_theory",
    score=0.87,
    reasoning=[
        "+0.40: Category match (Cognitive)",
        "+0.30: Complexity appropriate (Rung 2 <= Rung 3)",
        "+0.17: 2/3 use cases match problem description"
    ],
    alternatives=[
        ("working_memory_model", 0.74),
        ("dunning_kruger_effect", 0.61)
    ]
)
```

✅ **Explanations adapt to user expertise**
- Rung 1 (Simple): "Your brain is full. Like a backpack with too many books."
- Rung 2 (Detailed): "Cognitive Load Theory says working memory has limited capacity (7±2 chunks). When overloaded, new information can't be processed."
- Rung 3 (Technical): "CLT (Sweller, 1988) models working memory as limited-capacity processor. Intrinsic load (task complexity) + extraneous load (presentation) + germane load (schema construction) must stay below threshold."
- Rung 4 (Debug): [Full citation, formula, experimental evidence, alternative theories]

#### 3.3.4 For MNEMIS (Memory)

✅ **Every memory operation is logged**
```json
{
  "operation": "write",
  "memory_id": "mem_1847",
  "agent_id": "therapy_generator",
  "tier": "AGENT",
  "timestamp": "2026-02-08T14:23:04Z",
  "data_hash": "sha256:a1b2c3...",
  "provenance": {
    "creator_agent_id": "therapy_generator",
    "source_workflow_id": "therapy_session_1847",
    "created_at": "2026-02-08T14:23:04Z"
  }
}
```

✅ **Promotion proposals show evidence**
```python
PromotionProposal(
    memory_id="pattern_3chunk_context",
    from_tier=MemoryTier.AGENT,
    to_tier=MemoryTier.PROJECT,
    rationale="Pattern occurred 47x in 3 sessions (threshold: 30x in 2 sessions)",
    evidence=[
        "Session 1: 18 occurrences",
        "Session 2: 15 occurrences",
        "Session 3: 14 occurrences"
    ],
    proposed_by="via_coordinator",
    status=ProposalStatus.PENDING
)
```

---

### 3.4 Constitutional Boundaries — What GAIA Will NOT Do

GAIA's trust contract includes explicit negative boundaries. These are commitments to users about capabilities GAIA will NEVER provide, even if technically feasible.

#### 3.4.1 GAIA Will NOT Modify Code Without Glass-Box Review

**Boundary:**
- No "auto-fix" button that silently rewrites agent logic
- No "AI suggests improvement → auto-apply" workflow
- All code changes require human review (Glass-Box Decision Gate)

**Rationale:** Trust requires understanding. Silent modifications violate Principle 5 (Remains Inspectable).

**Enforcement:** LOOM workflow engine ALWAYS shows diff before applying changes. WARDEN blocks deployments if workflow modifications lack user approval timestamp.

---

#### 3.4.2 GAIA Will NOT Promote Memory Automatically

**Boundary:**
- Pattern detection is automatic
- Memory promotion requires explicit approval
- No "background learning" that changes behavior without user knowledge

**Rationale:** Learning must be explicit (Principle 4). Silent drift erodes trust over time.

**Enforcement:** MNEMIS promotion engine requires `approved_by: str` field. Pytest tests verify promotion without approval raises `UnauthorizedPromotionError`.

---

#### 3.4.3 GAIA Will NOT Hide Errors or Degrade Silently

**Boundary:**
- All errors logged to ARGUS telemetry
- Degradation events shown in UI with clear explanation
- No "it just stopped working" experiences

**Rationale:** Graceful degradation must be visible (Principle 3). Silent failures destroy trust.

**Enforcement:** WARDEN scans logs for exceptions without telemetry. ARGUS dashboard alerts if degradation rate exceeds threshold.

---

#### 3.4.4 GAIA Will NOT Store Secrets or Credentials

**Boundary:**
- API keys ONLY in `.env` (never in code, config files, logs, or git)
- .gitignore ALWAYS includes `.env*`
- Pre-commit hooks block commits with secrets

**Rationale:** Security is constitutional. One leaked key violates user trust permanently.

**Enforcement:** WARDEN pre-commit hook scans for patterns (`sk-`, `api_key`, `token`, etc.). Deployment blocked if secrets detected in git history.

---

#### 3.4.5 GAIA Will NOT Export User Data Externally

**Boundary:**
- All telemetry stored locally (`X:\Projects\_GAIA\logs\*.jsonl`)
- No cloud backend, no external API calls (except LLM providers)
- User data NEVER leaves local filesystem

**Rationale:** GAIA is local-first by design. Cloud exports introduce privacy risks incompatible with healthcare/finance use cases (HART OS, VIA).

**Enforcement:** WARDEN scans network calls. Pytest tests verify no external URLs except documented LLM providers.

---

**END OF SECTIONS 1-3**

*Next sections (4-7) will cover: Platform Capabilities, Product Deep Dives (HART OS, VIA, PROTEUS), Roadmap, and Success Metrics.*

---

# Part II: Platform & Products

## Section 4: Platform Capabilities

GAIA provides eight shared service components that work together to create, orchestrate, monitor, and govern AI products. Each component addresses a specific layer of the ecosystem architecture, from project scaffolding through runtime enforcement.

**Shared Services:**
1. **VULCAN** — Project Creation Engine
2. **LOOM** — Workflow Orchestration
3. **ARGUS** — Monitoring & Sense-Making
4. **MYCEL** — Shared Intelligence
5. **MNEMIS** — Cross-Project Memory
6. **WARDEN** — Compliance & Enforcement
7. **ABIS** — Visual System Builder
8. **RAVEN** — Autonomous Research Agent (planned)

**Note:** PROTEUS was previously listed as a shared service but has been reclassified as a product (jSeeker) to avoid confusion. ECHO as a shared service has been reclassified as GPT_ECHO, a standalone product.

---

### 4.0 Product Co-Evolution Model

Products evolve through two feedback loops:

**Loop 1: User-Driven (Explicit)**
User provides feedback → ARGUS detects pattern → LOOM proposes edit →
User reviews and approves → WARDEN validates → Product updated →
ARGUS monitors new behavior → Cycle continues

**Loop 2: GAIA-Driven (Proactive)**
ARGUS Meta-Observer detects cross-project pattern →
MNEMIS promotes pattern from PROJECT to GAIA tier →
GAIA suggests improvement to relevant products →
User reviews suggestion → If approved, LOOM applies change →
All benefiting products receive the pattern

**Example:** jSeeker discovers that "action verb + quantified result" bullet points score 40% higher on ATS. MNEMIS promotes this to GAIA tier. GAIA suggests HART OS could use similar structured output for therapy session summaries. User approves. LOOM applies the pattern to HART OS's output formatter.

---

### 4.1 VULCAN — Project Creation Engine

**Version:** v0.4.0-dev
**Status:** Operational (85% test coverage, 19,830 LOC across core + docs)
**Location:** `X:\Projects\_GAIA\_VULCAN`
**Framework:** Streamlit (port 8501)
**Dependencies:** MYCEL (GaiaSettings, registry integration)

#### What It Does

VULCAN is the entry point to the GAIA ecosystem. It transforms project ideas into production-ready, GAIA-compliant codebases through a structured questionnaire and intelligent scaffolding system. Rather than providing generic templates, VULCAN adapts its output based on project type, capturing architectural constraints and user intent upfront.

**Core Capabilities:**

1. **Human-in-the-Loop Questionnaire (7-Step Intake)**
   - Step 1: Project identity (name, description, target users)
   - Step 2: Project type selection (Deterministic/Creative/Processor)
   - Step 3: Technical stack (Python version, framework, providers)
   - Step 4: LLM configuration (provider selection, model routing)
   - Step 5: Constraints capture (performance, budget, compliance)
   - Step 6: GAIA integration level (Full/Lite/Registry-Only)
   - Step 7: Confirmation and preview

2. **Three Adapter Types** (Specialized Scaffolding)
   - **Deterministic Adapter** (`deterministic_adapter.py`, 1,047 LOC)
     - For: Therapy systems, scoring algorithms, compliance engines
     - Generates: Rules engine, canonical data pipeline, validation framework
     - Example: HART OS (6-component scoring with canonical source traceability)

   - **Creative Adapter** (`creative_adapter.py`, 938 LOC)
     - For: RAG systems, synthesis engines, content generation
     - Generates: Multi-provider LLM integration, chunking pipeline, retrieval system
     - Example: VIA (investment intelligence with 3-provider ensemble)

   - **Processor Adapter** (`processor_adapter.py`, 1,758 LOC)
     - For: Data pipelines, document generation, ETL systems
     - Generates: Pipeline orchestration, transformation library, validation gates
     - Example: DATA FORGE (taxonomy builder, memory bank compiler)

3. **GAIA-Compliant Scaffolding** (Every Project Gets)
   - `config.py` inheriting from `GaiaSettings` (centralized API keys, model configs)
   - `logs/` directory with JSONL telemetry hooks (ARGUS-ready)
   - `CLAUDE.md` with structured context (role, constraints, patterns)
   - `tests/` with pytest structure and coverage targets
   - `.gitignore` with hardened secrets protection (13 patterns)
   - `requirements.txt` including `rag-intelligence>=0.3.1` (MYCEL integration)
   - `README.md` with quickstart and architecture overview
   - `.env.example` with required environment variables

4. **Registry Integration**
   - Automatic registration in `X:\Projects\_GAIA\registry.json`
   - Metadata capture: version, status, git remote, dependencies, tags
   - Retroactive registration modes:
     - **Registry-Only:** Minimal metadata, no code changes
     - **GAIA-Lite:** Add config.py and telemetry hooks only
     - **Full Integration:** Complete GAIA compliance transformation

5. **Git Initialization** (Optional)
   - Creates `.git` repository with initial commit
   - Adds remote if provided (GitHub/GitLab/BitBucket)
   - Initializes branch structure (main + development optional)

#### User Workflow (Step-by-Step)

```
User → Launch VULCAN → Answer Questionnaire → Review Preview → Confirm
         ↓
     VULCAN creates:
     1. Project directory structure
     2. Adapter-specific files (rules/pipeline/processor)
     3. Configuration files (config.py, .env.example)
     4. Documentation (README.md, CLAUDE.md)
     5. Test scaffolding (tests/, conftest.py)
     6. Git initialization (optional)
     7. Registry entry
         ↓
     User → Navigate to project → Install dependencies → Run tests → Start development
```

**Time to Testable Project:** 10 minutes (from questionnaire start to `pytest` passing)

#### Technical Constraints

- **Python Version:** 3.10+ (required for Pydantic v2 and structural pattern matching)
- **Platform:** Windows/Linux/MacOS (filesystem operations use `pathlib` for cross-platform compatibility)
- **Deployment:** Local filesystem only (no cloud deployment or containerization yet)
- **Streamlit Dependency:** Requires Streamlit 1.28+ for UI
- **Write Access:** Requires write permissions to `X:\Projects\` root

#### Key Files and Entry Points

| File | Purpose | Lines | Role |
|------|---------|-------|------|
| `vulcan_forge/project_creator.py` | Main orchestrator | 595 | Coordinates questionnaire → adapter → scaffold pipeline |
| `vulcan_forge/questionnaire.py` | HITL intake forms | 180 | Collects user intent, constraints, technical requirements |
| `vulcan_forge/registry_manager.py` | Registry CRUD operations | 120 | Reads/writes `registry.json`, validates schema |
| `vulcan_forge/project_validator.py` | Compliance checker | 185 | Validates GAIA structure compliance post-creation |
| `ui/main.py` | Streamlit landing page | 312 | Entry point: `streamlit run ui/main.py` |
| `ui/pages/1_new_project.py` | New project wizard | 428 | Step-by-step questionnaire UI |
| `ui/pages/2_register_existing.py` | Retroactive registration | 356 | Adds existing projects to GAIA |
| `tests/test_project_creator.py` | Core engine tests | 892 | 137 tests, 85% coverage |

#### Current Status and Known Gaps

**Operational:**
- ✅ All three adapters tested and functional
- ✅ Registry integration complete
- ✅ Git initialization working
- ✅ 85% test coverage across core modules

**Missing/Planned:**
- ❌ Multi-language support (currently Python-only)
- ❌ Docker/containerization scaffolding
- ❌ CI/CD pipeline templates (GitHub Actions, GitLab CI)
- ❌ Cloud deployment scaffolding (AWS Lambda, Google Cloud Run)
- ❌ Database schema generation (for projects needing persistence)

---

### 4.2 LOOM — Workflow Orchestration

**Version:** v1.0.0
**Status:** Partial (models operational, governance not enforced at runtime)
**Location:** `X:\Projects\_GAIA\_LOOM`
**Framework:** Library (no UI)
**Dependencies:** MYCEL (config), MNEMIS (memory contracts)

#### What It Does

LOOM provides hierarchical agent coordination with built-in governance. It enforces agent authority levels, tracks workflow state across multi-step executions, and integrates memory-aware execution so agents cannot exceed their permissions.

**Core Capabilities:**

1. **Agent Authority System (5 Levels, 0-4)**

   Agents are assigned authority levels that determine their memory access and operational scope:

   ```python
   @dataclass
   class Agent:
       name: str
       authority_level: int  # 0-4
       capabilities: List[str]
       memory_access: MemoryAccess
   ```

   **Authority Levels:**
   - **Level 0: Ephemeral Agents** (Single-Use Functions)
     - Scope: Execute once and discard
     - Memory: Read-only access to Ephemeral tier (Tier 0)
     - Use Cases: One-off calculations, temporary transformations
     - Example: Format conversion function, validation check

   - **Level 1: Task Agents** (Single Workflow Task)
     - Scope: Complete a discrete task within a workflow
     - Memory: Read/write to Working Memory (Tier 1)
     - Use Cases: Parse document, extract entities, score candidate
     - Example: JD parser in PROTEUS, chunk embedder in VIA

   - **Level 2: Session Agents** (Conversation Management)
     - Scope: Manage multi-turn interactions within a session
     - Memory: Read/write to Session Memory (Tier 2)
     - Use Cases: Chatbot orchestrator, multi-step wizard, feedback collector
     - Example: PROTEUS resume wizard, HART OS intake flow

   - **Level 3: Project Agents** (Cross-Session Coordination)
     - Scope: Coordinate workflows across multiple sessions
     - Memory: Read/write to Long-term Memory (Tier 3)
     - Use Cases: Pattern learner, optimization engine, feedback synthesizer
     - Example: PROTEUS feedback loop, VIA investment thesis learner

   - **Level 4: System Agents** (Architectural Changes)
     - Scope: Modify system architecture, promote knowledge ecosystem-wide
     - Memory: Read/write to Permanent Memory (Tier 4), GAIA-tier promotion
     - Use Cases: Schema migration, constitutional rule updates, cross-project pattern promotion
     - Example: ARGUS pattern promoter, MNEMIS promotion validator

2. **Workflow State Management**

   LOOM tracks execution state across multi-step workflows with automatic persistence:

   ```python
   class WorkflowEngine:
       def execute_workflow(
           workflow: Workflow,
           context: Dict[str, Any]
       ) -> WorkflowResult:
           """
           Execute workflow with:
           - Agent authority validation (prevent privilege escalation)
           - Memory contract enforcement (cannot write above tier)
           - State tracking (persist across crashes)
           - Explainability integration (log reasoning at each step)
           """
   ```

   **State Tracking Features:**
   - Checkpoint creation at each step boundary
   - Rollback support if step fails validation
   - Resume from last checkpoint on crash recovery
   - State serialization to JSONL for audit trail

3. **Memory-Aware Execution**

   Every agent operation validates memory access before execution:

   ```python
   def validate_memory_access(
       agent: Agent,
       operation: str,  # "read" or "write"
       target_tier: int
   ) -> bool:
       if operation == "read":
           return target_tier <= agent.authority_level  # Can read down hierarchy
       elif operation == "write":
           return target_tier <= agent.authority_level  # Cannot write above authority
       else:
           raise ValueError(f"Unknown operation: {operation}")
   ```

   **Contract Enforcement:**
   - Agents can READ from any tier ≤ their authority
   - Agents can WRITE only to tiers ≤ their authority
   - Agents can PROMOTE memory up one tier (with validation)
   - Violations raise `MemoryContractViolation` exception

4. **Glass-Box Transparency**

   Every workflow step logs its reasoning and decisions:
   - Input state snapshot
   - Agent selection rationale (why this agent?)
   - Memory accesses (read/write operations)
   - Transformation logic (what changed and why?)
   - Output state snapshot
   - Mental model applied (if using ARGUS integration)

#### Current Limitation: Governance Not Enforced at Runtime

**Status:** Models and validators exist, but enforcement not wired into production systems.

**What Works:**
- ✅ Agent authority levels defined in code
- ✅ Memory contracts modeled in MNEMIS
- ✅ Validation logic implemented (`WorkflowValidator.validate_agent_authority()`)
- ✅ Exception types defined (`MemoryContractViolation`, `AuthorityViolation`)

**What's Missing:**
- ❌ Runtime validation hooks not called in HART OS, VIA, DATA FORGE
- ❌ No enforcement at LLM call sites (agents can currently write anywhere)
- ❌ No pre-commit hooks to block violations before runtime
- ❌ ARGUS dashboard shows authority graph, but doesn't enforce it

**Impact:**
- Agents in production systems (HART OS, VIA) currently have unrestricted memory access
- Memory promotion happens manually, not governed by authority levels
- No audit trail of authority violations (because they're not detected)

**Remediation Plan:**
- Phase 4: Wire LOOM validators into HART OS and VIA LLM call wrappers
- Phase 5: Add WARDEN pre-commit hooks to detect authority violations in code review
- Phase 6: Integrate ARGUS telemetry to track attempted violations in production

---

### 4.3 ARGUS — Monitoring & Sense-Making

**Version:** v0.5.0-dev
**Status:** Partial (mental models + dashboard operational, Process Observer missing)
**Location:** `X:\Projects\_GAIA\_ARGUS`
**Framework:** Streamlit dashboard + library
**Dependencies:** MYCEL (config), SQLite (telemetry)

#### What It Does

ARGUS is the ecosystem's monitoring and explainability layer. It provides context-aware mental model selection, layered explainability (4 levels), a subconscious layer for pattern detection, and a centralized event bus for telemetry. Currently, only PROTEUS actively sends telemetry to ARGUS.

**Core Capabilities:**

1. **Mental Model Library (59 Models, 6 Categories)**

   ARGUS maintains a curated library of mental models for decision support and sense-making:

   **Categories:**
   - **Systems Thinking (8 models):** Feedback loops, emergence, bottlenecks, leverage points, system boundaries, stocks/flows, resilience, second-order effects
   - **Decision-Making (10 models):** First principles, inversion, opportunity cost, reversibility, margin of safety, Pareto principle, Occam's Razor, expected value, sunk cost, planning fallacy
   - **Cognitive (12 models):** Confirmation bias, availability heuristic, anchoring, dunning-kruger, hindsight bias, fundamental attribution error, recency bias, survivorship bias, framing effect, halo effect, loss aversion, status quo bias
   - **Behavioral (9 models):** Incentive alignment, social proof, commitment consistency, scarcity, authority, reciprocity, loss framing, default effect, choice architecture
   - **Communication (10 models):** Signal vs noise, information asymmetry, common knowledge, context collapse, semantic drift, inference chains, narrative coherence, steel-manning, clarifying questions, socratic method
   - **Business (10 models):** Jobs to be done, value chain, competitive moat, switching costs, network effects, unit economics, compound growth, constraints theory, innovation diffusion, TAM/SAM/SOM

   **Model Metadata Structure:**
   ```json
   {
     "id": "feedback_loops",
     "name": "Feedback Loops",
     "category": "systems_thinking",
     "description": "Circular causal relationships where output feeds back as input",
     "when_to_use": ["detecting circular dependencies", "analyzing agent behavior patterns"],
     "output_format": "loop_diagram",
     "confidence_threshold": 0.7,
     "examples": ["hallucination loops", "cost spiral detection", "memory drift patterns"],
     "prerequisites": [],
     "complexity_level": 2,
     "growth_rung_min": 2
   }
   ```

   **Context-Aware Selection Algorithm:**
   ```python
   def select_model(context: str, user_rung: int) -> RankedModel:
       """
       Score models based on:
       1. Context match (keyword overlap with when_to_use)
       2. User Growth Rung (filter out too complex)
       3. Prerequisites (user has needed background?)
       4. Historical success (model helped before?)

       Returns: Top 3 ranked models with confidence scores
       """
   ```

2. **Subconscious Layer (Memory, Pattern Detection, Hypothesis Generation)**

   ARGUS maintains an "external memory" layer that observes system behavior and generates hypotheses:

   **Components:**
   - **External Memory Store:** Persistent storage of system observations beyond working memory
   - **Pattern Detector:** Identifies recurring patterns across events (e.g., "LLM call fails every 50 requests" → rate limit hypothesis)
   - **Hypothesis Generator:** Proposes explanations for anomalies using mental models
   - **Background Processor:** Runs asynchronously, doesn't block main workflow

   **Example Workflow:**
   ```
   System Event: Resume generation fails for 3rd time on same JD
                         ↓
   Subconscious Layer observes pattern
                         ↓
   Hypothesis Generator applies "Feedback Loops" mental model
                         ↓
   Hypothesis: JD contains term causing retrieval loop in block matcher
                         ↓
   ARGUS dashboard surfaces hypothesis to user
                         ↓
   User confirms → Pattern promoted to GAIA memory
   ```

3. **Layered Explainability (4 Levels)**

   ARGUS explains system decisions at 4 complexity levels, mapped to user Growth Rungs:

   | Level | Target Rung | Audience | Detail |
   |-------|-------------|----------|--------|
   | **Simple** | Rung 1 | Non-technical users | "Resume scored 87/100 because it matched 9 of 10 key requirements" |
   | **Detailed** | Rung 2 | Power users | "ATS scorer found 9/10 keywords present (Python, AWS, API, React, ...), 2 exact title matches, and 85% semantic overlap in experience section" |
   | **Technical** | Rung 3 | Developers | "Block matcher: 0.89 cosine similarity on embed(jd_requirements) vs embed(resume_blocks), filtered to top 5 blocks with overlap > 0.7, then LLM adaptation pass with prompt template #3" |
   | **Debug** | Rung 4 | System designers | "LLM call trace: model=claude-sonnet-4-5, tokens_in=3,421, tokens_out=892, cost=$0.023, latency=2.1s, cache_hit=false, prompt_hash=abc123, reasoning_trace=[...]" |

   **Growth Rung Mapping:**
   - Rung 1: "I want it to work" → Simple explanations
   - Rung 2: "I want to understand it" → Detailed explanations
   - Rung 3: "I want to modify it" → Technical explanations
   - Rung 4: "I want to architect it" → Debug-level traces

   **Implementation:** Each GAIA component method decorated with `@explain_decision(level)` that logs reasoning at all 4 levels.

4. **Event Bus (SQLite Telemetry)**

   Centralized SQLite database for all ecosystem telemetry:

   **Schema:**
   ```sql
   CREATE TABLE events (
       id INTEGER PRIMARY KEY AUTOINCREMENT,
       timestamp TEXT NOT NULL,
       component TEXT NOT NULL,      -- "jSeeker", "HART_OS", "VIA", etc.
       event TEXT NOT NULL,           -- "resume_generated", "llm_call_failed", etc.
       details_json TEXT,             -- Full event payload
       user_id TEXT,                  -- (optional) User identifier
       session_id TEXT,               -- (optional) Session identifier
       cost_usd REAL,                 -- (optional) Cost if LLM call
       latency_ms REAL                -- (optional) Latency if timed operation
   );
   ```

   **API:**
   ```python
   from argus.event_bus import EventBus

   bus = EventBus()
   bus.emit(
       component="PROTEUS",
       event="resume_generated",
       details={"jd_id": "abc123", "ats_score": 87, "blocks_used": 9}
   )

   # Query recent events
   recent = bus.get_recent(limit=100)

   # Filter by component
   proteus_events = bus.get_by_component("PROTEUS", limit=50)
   ```

   **Current Integration Status:**
   - ✅ PROTEUS: Fully integrated (emits events for resume generation, LLM calls, ATS scoring)
   - ❌ HART OS: Not integrated (telemetry hooks exist in code but not called)
   - ❌ VIA: Not integrated
   - ❌ DATA FORGE: Not integrated

5. **Dashboard (Streamlit)**

   **Location:** `X:\Projects\_GAIA\_ARGUS\dashboard\`
   **Entry Point:** `streamlit run dashboard/app.py`

   **Dashboard Views:**
   - **Ecosystem Graph:** Component status grid (active/degraded/error)
   - **Event Stream:** Real-time feed of telemetry events
   - **Cost Tracker:** LLM spend by component and time period
   - **Mental Model Explorer:** Browse, search, and apply models
   - **Agent Trace Viewer:** Step-by-step workflow execution (when integrated)
   - **Memory Hierarchy:** Visualize memory promotion and tier distribution

#### Missing Components

1. **Process Observer (Not Implemented)**
   - **Goal:** Observe Claude Code agent executions in real-time
   - **Challenge:** Claude Code runs in separate process, no IPC hooks available
   - **Workaround:** Manual log parsing of `.claude/logs/` directory

2. **Post-Mortem Analyzer (Not Implemented)**
   - **Goal:** Auto-analyze task failures and generate "what went wrong" reports
   - **Current State:** Event bus captures failures, but no automatic analysis

3. **Trust Dashboard (Empty Directory)**
   - **Goal:** Visualize constitutional compliance across ecosystem
   - **Metrics Planned:** Authority violations, memory contract breaches, secret exposure, test coverage, git hygiene
   - **Status:** `dashboard/trust/` directory exists but contains no files

#### Key Files

| File | Purpose | Lines | Status |
|------|---------|-------|--------|
| `mental_models/registry.json` | 59 model definitions | 2,800+ | ✅ Complete |
| `mental_models/selector.py` | Context-aware model scoring | 312 | ✅ Complete |
| `subconscious/memory_store.py` | External memory layer | 428 | ✅ Complete |
| `subconscious/pattern_detector.py` | Recurring pattern identification | 356 | ✅ Complete |
| `explainability/layered_explainer.py` | 4-level explanation engine | 289 | ✅ Complete |
| `event_bus.py` | SQLite telemetry API | 187 | ✅ Complete |
| `dashboard/app.py` | Streamlit dashboard entry | 524 | ✅ Complete |
| `dashboard/components/ecosystem_graph.py` | Component status grid | 156 | ✅ Complete |

---

### 4.4 MYCEL — Shared Intelligence

**Version:** v0.2.0
**Status:** Operational (92-100% test coverage, 200+ tests)
**Location:** `X:\Projects\_GAIA\_MYCEL`
**Framework:** Library (Python package: `rag-intelligence`)
**Published As:** `rag-intelligence` (installable via pip from local path)

#### What It Does

MYCEL (Mycelium – nature's neural network) is the shared intelligence spine of GAIA. It provides unified LLM clients, core RAG algorithms (chunking, embedding, retrieval), and centralized configuration management so products don't duplicate infrastructure code.

**Core Capabilities:**

1. **Unified LLM Clients (3 Providers)**

   Single interface for OpenAI, Anthropic, and Gemini with automatic fallback:

   ```python
   from rag_intelligence.integrations import create_llm_client

   # Automatic provider detection from config
   client = create_llm_client()  # Uses GaiaSettings to determine provider

   # Explicit provider
   client = create_llm_client(provider="anthropic", model="claude-sonnet-4-5")

   # Unified API
   response = client.generate(
       prompt="Summarize this document",
       context=document_text,
       max_tokens=500,
       temperature=0.7
   )
   ```

   **Provider Implementations:**
   - `OpenAIClient` (`openai_client.py`, 287 LOC): Wraps `openai>=1.0.0` SDK
   - `AnthropicClient` (`anthropic_client.py`, 312 LOC): Wraps `anthropic>=0.18.0` SDK
   - `GeminiClient` (`gemini_client.py`, 298 LOC): Wraps `google-generativeai` SDK

   **Features:**
   - Automatic retry with exponential backoff (3 attempts, 1s/2s/4s delays)
   - Provider fallback (if primary fails, try secondary)
   - Cost tracking (tokens + estimated USD)
   - Response caching (optional, in-memory LRU cache)
   - Streaming support (for real-time UI feedback)

2. **Core Algorithms (Chunking, Embedding, Retrieval)**

   **RecursiveCharacterChunker** (`chunker.py`, 412 LOC):
   - Splits text by paragraphs, then sentences, then characters
   - Preserves semantic boundaries (doesn't split mid-sentence)
   - Configurable overlap (default: 200 characters)
   - Metadata preservation (source file, page number, section)

   ```python
   from rag_intelligence.core import RecursiveCharacterChunker

   chunker = RecursiveCharacterChunker(
       chunk_size=1000,
       overlap=200,
       separators=["\n\n", "\n", ". ", " "]
   )
   chunks = chunker.chunk(document_text)
   # Returns: List[Chunk] with text, metadata, source
   ```

   **OpenAIEmbedder** (`embedder.py`, 289 LOC):
   - Generates embeddings via `text-embedding-3-small` (default) or `text-embedding-3-large`
   - Batch processing (up to 2048 chunks per request)
   - Dimension reduction (optional, 1536 → 256 for storage optimization)
   - Caching (avoid re-embedding identical text)

   ```python
   from rag_intelligence.core import OpenAIEmbedder

   embedder = OpenAIEmbedder(model="text-embedding-3-small")
   embeddings = embedder.embed([chunk.text for chunk in chunks])
   # Returns: List[List[float]] with shape (N, 1536)
   ```

   **VectorRetriever** (`retriever.py`, 456 LOC):
   - Hybrid search: semantic (cosine similarity) + keyword (BM25)
   - Configurable weights (default: 70% semantic, 30% keyword)
   - Reranking (optional, uses cross-encoder model)
   - Metadata filtering (by source, date, category, etc.)

   ```python
   from rag_intelligence.core import VectorRetriever

   retriever = VectorRetriever(
       embeddings=embeddings,
       chunks=chunks,
       semantic_weight=0.7,
       keyword_weight=0.3
   )
   results = retriever.retrieve(
       query="What are the investment risks?",
       top_k=5,
       metadata_filter={"source": "10-K"}
   )
   # Returns: List[Chunk] ranked by relevance
   ```

3. **Configuration Management (GaiaSettings)**

   **Base Class for All Projects:**

   ```python
   from rag_intelligence.config import GaiaSettings
   from pydantic_settings import SettingsConfigDict

   class HartOSConfig(GaiaSettings):
       """HART OS specific configuration."""

       # Inherits from GaiaSettings:
       # - openai_api_key: str
       # - anthropic_api_key: str | None
       # - gemini_api_key: str | None
       # - default_provider: str = "openai"
       # - log_level: str = "INFO"

       # Project-specific settings
       hart_library_path: str = "data/hart_library_manifest.md"
       therapy_model: str = "gpt-4o"
       output_language: str = "es"

       model_config = SettingsConfigDict(
           env_file=".env",
           env_file_encoding="utf-8",
           case_sensitive=False
       )
   ```

   **Benefits:**
   - All projects use identical config structure
   - API keys managed in `.env` (never in code)
   - Type-safe (Pydantic validation)
   - Auto-documentation (Pydantic schema export)

4. **Only Module with TDD in pyproject.toml**

   MYCEL is the only GAIA component with test-driven development configuration:

   **`pyproject.toml` Test Section:**
   ```toml
   [tool.pytest.ini_options]
   testpaths = ["tests"]
   python_files = "test_*.py"
   addopts = "-v --cov=rag_intelligence --cov-report=term-missing"

   [tool.coverage.run]
   branch = true
   omit = ["*/tests/*", "*/migrations/*"]

   [tool.coverage.report]
   precision = 2
   show_missing = true
   skip_covered = false
   ```

   **Test Coverage:**
   - `test_chunker.py`: 98% coverage (47 tests)
   - `test_embedder.py`: 95% coverage (38 tests)
   - `test_retriever.py`: 92% coverage (52 tests)
   - `test_openai_client.py`: 100% coverage (41 tests)
   - `test_anthropic_client.py`: 100% coverage (39 tests)
   - `test_gemini_client.py`: 97% coverage (35 tests)

   **Total:** 200+ tests, 92-100% coverage across all modules

#### Technical Details

**Dependencies:**
```toml
dependencies = [
    "openai>=1.0.0",
    "anthropic>=0.18.0",
    "google-generativeai>=0.3.0",
    "numpy>=1.26.0",
    "rank-bm25>=0.2.2",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
    "python-dotenv>=1.0.0"
]
```

**Build System:** Poetry (for local development), setuptools (for distribution)

**Python Version:** 3.10+ (required for Pydantic v2 and union type syntax)

**Installation:**
```bash
pip install -e "X:\Projects\_GAIA\_MYCEL"  # Editable install for development
```

#### Key Files

| File | Purpose | Lines | Coverage |
|------|---------|-------|----------|
| `rag_intelligence/config.py` | GaiaSettings base class | 127 | 100% |
| `rag_intelligence/integrations/base_client.py` | Abstract LLM interface | 89 | 100% |
| `rag_intelligence/integrations/openai_client.py` | OpenAI wrapper | 287 | 100% |
| `rag_intelligence/integrations/anthropic_client.py` | Anthropic wrapper | 312 | 100% |
| `rag_intelligence/integrations/gemini_client.py` | Gemini wrapper | 298 | 97% |
| `rag_intelligence/integrations/llm_factory.py` | Factory pattern | 124 | 100% |
| `rag_intelligence/core/chunker.py` | Recursive character splitter | 412 | 98% |
| `rag_intelligence/core/embedder.py` | OpenAI embeddings | 289 | 95% |
| `rag_intelligence/core/retriever.py` | Hybrid semantic + keyword search | 456 | 92% |
| `rag_intelligence/core/models.py` | Data models (Chunk, Document, Embedding) | 203 | 100% |

---

### 4.5 MNEMIS — Cross-Project Memory

**Version:** v1.0.0
**Status:** Operational (architecture complete, promotion manual)
**Location:** `X:\Projects\_GAIA\_MNEMIS`
**Framework:** Library
**Dependencies:** MYCEL (config)

#### What It Does

MNEMIS (Mnemosyne – titan of memory) provides a three-tier memory hierarchy with runtime contract enforcement. It enables cross-project knowledge sharing while preventing unauthorized access through authority-based governance.

**Core Capabilities:**

1. **Three-Tier Memory Hierarchy**

   MNEMIS organizes memory into three scopes with different persistence and visibility:

   **Tier Structure:**
   ```
   PUBLIC (Tier 4)     ← Shareable knowledge (exportable to community)
      ↑
   GAIA (Tier 3)       ← Cross-project patterns (ecosystem-wide)
      ↑
   PROJECT (Tier 2)    ← Project-specific knowledge (local only)
      ↑
   SESSION (Tier 1)    ← Current conversation context
      ↑
   EPHEMERAL (Tier 0)  ← Temporary calculations
   ```

   **Tier Definitions:**

   - **PROJECT (Tier 2):**
     - **Scope:** Single project (e.g., HART OS knowledge stays in HART OS)
     - **Persistence:** Permanent (stored in `{project}/memory/project.jsonl`)
     - **Access:** Any agent in project with authority ≥ 2
     - **Examples:** User preferences, project constraints, learned patterns
     - **Promotion:** To GAIA if pattern observed in 3+ projects

   - **GAIA (Tier 3):**
     - **Scope:** All projects in ecosystem
     - **Persistence:** Permanent (stored in `X:\Projects\_GAIA\mnemis\shared_memory\gaia.jsonl`)
     - **Access:** Any agent in any project with authority ≥ 3
     - **Examples:** Universal optimization patterns, common failure modes, constitutional rules
     - **Promotion:** To PUBLIC if user explicitly approves for sharing

   - **PUBLIC (Tier 4):**
     - **Scope:** Exportable to community (blog posts, open-source libraries)
     - **Persistence:** Permanent (stored in `X:\Projects\_GAIA\mnemis\shared_memory\public.jsonl`)
     - **Access:** Any agent with authority 4 (System Agent)
     - **Examples:** Anonymized case studies, reusable algorithms, design patterns
     - **Promotion:** Never (highest tier)

2. **Memory Contracts (Runtime Enforcement)**

   Every memory operation validates access rights before execution:

   ```python
   class MemoryContract:
       """
       Enforces memory tier access rules:
       - Agents can READ from any tier ≤ their authority
       - Agents can WRITE only to tiers ≤ their authority
       - Agents can PROMOTE memory up one tier (with validation)
       """

       def validate_write(
           agent_authority: int,
           target_tier: int,
           memory_content: Dict[str, Any]
       ) -> bool:
           """Raise MemoryContractViolation if agent tries to write above authority."""
           if target_tier > agent_authority:
               raise MemoryContractViolation(
                   f"Agent authority {agent_authority} cannot write to tier {target_tier}. "
                   f"Maximum allowed tier: {agent_authority}"
               )
           return True

       def validate_promotion(
           agent_authority: int,
           from_tier: int,
           to_tier: int,
           criteria: PromotionCriteria
       ) -> bool:
           """Validate promotion meets criteria and agent has authority."""
           if to_tier != from_tier + 1:
               raise ValueError("Can only promote up one tier at a time")

           if agent_authority < to_tier:
               raise MemoryContractViolation(
                   f"Agent authority {agent_authority} cannot promote to tier {to_tier}"
               )

           if not criteria.is_satisfied():
               raise PromotionCriteriaNotMet(
                   f"Criteria not satisfied: {criteria.missing_requirements()}"
               )

           return True
   ```

   **Contract Rules:**
   - **Read Access:** Agents can read from any tier ≤ authority (read down hierarchy)
   - **Write Access:** Agents can write only to tiers ≤ authority (cannot write above)
   - **Promotion:** Agents can promote memory up one tier if criteria met
   - **Violations:** Raise exception immediately, log to ARGUS telemetry

3. **Promotion Protocol (Criteria and Workflow)**

   **Promotion Criteria (5 Dimensions):**

   ```python
   @dataclass
   class PromotionCriteria:
       access_count: int = 3              # How many times accessed?
       time_span: timedelta | None = None # Over what time period?
       pattern_strength: float = 0.7      # How consistent is pattern?
       user_confirmation: bool = False    # Explicit user approval required?
       importance_score: float = 0.6      # Calculated importance (0-1)

       def is_satisfied(self) -> bool:
           """Check if all criteria met."""
           checks = [
               self.access_count >= 3,
               self.pattern_strength >= 0.7,
               self.importance_score >= 0.6
           ]

           if self.user_confirmation:
               checks.append(self._has_user_approval())

           if self.time_span:
               checks.append(self._occurred_over_time_span())

           return all(checks)
   ```

   **Promotion Workflow:**

   ```
   1. DETECTION
      System detects memory accessed ≥ 3 times
      ↓
   2. CRITERIA VALIDATION
      Check: access_count, time_span, pattern_strength, importance_score
      ↓
   3. USER CONFIRMATION (if promoting to GAIA or PUBLIC)
      UI prompt: "Promote pattern '{name}' to ecosystem-wide memory?"
      User: [Approve] [Reject] [Review Details]
      ↓
   4. EXECUTION
      - Copy memory to higher tier with updated metadata
      - Preserve provenance (where it came from, when, why)
      - Update access tracking
      ↓
   5. AUDIT
      Log promotion event to ARGUS:
      - from_tier, to_tier
      - promotion_reason
      - criteria_satisfied
      - user_confirmation (if applicable)
   ```

   **Example Promotion Scenario:**

   ```python
   # HART OS learns a pattern: "Users who score high on anxiety also need
   # grounding techniques regardless of primary diagnosis"

   memory = {
       "pattern": "anxiety_grounding_correlation",
       "description": "High anxiety score → always include grounding techniques",
       "context": "HART OS therapy session planning",
       "evidence": ["session_123", "session_456", "session_789"],
       "access_count": 7,
       "first_accessed": "2026-01-15",
       "last_accessed": "2026-02-08",
       "pattern_strength": 0.85
   }

   # System detects promotion candidate (accessed 7 times over 24 days)
   criteria = PromotionCriteria(
       access_count=7,
       time_span=timedelta(days=24),
       pattern_strength=0.85,
       user_confirmation=True,  # Required for PROJECT → GAIA
       importance_score=0.78
   )

   if criteria.is_satisfied():
       # Prompt user: "This pattern may apply to other therapy projects. Share?"
       if user_approves():
           promote_memory(
               memory=memory,
               from_tier=2,  # PROJECT
               to_tier=3,    # GAIA
               reason="Generalizable therapy pattern, validated across 7 sessions"
           )
           # Now VIA, DATA FORGE, and future products can access this insight
   ```

#### Current Limitation: Promotion Manual

**Status:** Promotion workflow implemented, but not automated from ARGUS error patterns.

**What Works:**
- ✅ Three-tier hierarchy implemented
- ✅ Memory contracts enforced in MNEMIS library
- ✅ Promotion criteria validation working
- ✅ User confirmation UI exists

**What's Missing:**
- ❌ Automatic promotion detection (ARGUS should trigger promotions based on access patterns, but doesn't)
- ❌ Cross-project pattern analysis (no automated "this pattern appears in HART OS and VIA" detection)
- ❌ Promotion recommendations in ARGUS dashboard (UI shows memory tiers, but doesn't suggest promotions)

**Workaround:**
- User manually inspects `{project}/memory/project.jsonl`
- User manually calls `promote_memory()` when pattern noticed
- No automation = patterns stay siloed in projects longer than optimal

**Remediation Plan:**
- Phase 4: Wire ARGUS pattern detector to scan PROJECT memory and suggest promotions
- Phase 5: Implement auto-promotion for low-risk patterns (e.g., performance optimizations)
- Phase 6: Cross-project pattern analyzer (find common patterns across projects)

#### Key Files

| File | Purpose | Lines | Status |
|------|---------|-------|--------|
| `mnemis/core/memory_store.py` | Main memory store implementation | 687 | ✅ Complete |
| `mnemis/models/memory_models.py` | Data models (MemoryEntry, MemoryContract, PromotionCriteria) | 298 | ✅ Complete |
| `mnemis/contracts/validator.py` | Contract enforcement logic | 234 | ✅ Complete |
| `mnemis/promotion/criteria.py` | Promotion criteria evaluation | 189 | ✅ Complete |
| `mnemis/promotion/workflow.py` | Promotion orchestration | 312 | ✅ Complete |
| `mnemis/persistence/jsonl_store.py` | JSONL serialization | 156 | ✅ Complete |
| `tests/test_memory_store.py` | Core memory tests | 523 | ✅ 87% coverage |
| `tests/test_contracts.py` | Contract enforcement tests | 412 | ✅ 92% coverage |

---

### 4.6 WARDEN — Compliance & Enforcement

**Version:** v0.1.0
**Status:** Minimal (scanner exists, not integrated)
**Location:** `X:\Projects\_GAIA\_WARDEN`
**Framework:** CLI tool + library
**Dependencies:** None (standalone)

#### What It Does

WARDEN is the compliance scanner for GAIA projects. It validates git status, test suite health, secret safety, dependency freshness, and documentation completeness. Currently exists as a standalone CLI tool but is not integrated into workflows.

**Core Capabilities:**

1. **Compliance Scanner (5 Checks)**

   **Check 1: Git Status**
   - Uncommitted changes? (fail if dirty working tree)
   - Untracked files? (warn if present)
   - Remote tracking? (warn if no remote configured)
   - Recent commits? (info if last commit > 7 days ago)

   **Check 2: Test Suite**
   - Tests exist? (fail if `tests/` directory empty)
   - Tests passing? (fail if `pytest` returns non-zero)
   - Coverage threshold? (warn if coverage < 80%)
   - Stale tests? (warn if no test modifications in 30 days)

   **Check 3: .env Safety**
   - `.env` in `.gitignore`? (fail if not)
   - Hardcoded API keys? (fail if secrets found in code)
   - `.env.example` exists? (warn if missing)
   - Secret patterns in git history? (warn if found)

   **Check 4: Dependency Freshness**
   - `requirements.txt` or `pyproject.toml` exists? (fail if neither)
   - Outdated dependencies? (warn if packages > 6 months old)
   - Security vulnerabilities? (warn if `pip-audit` finds issues)
   - Unpinned versions? (info if using `package>=1.0` instead of `package==1.0.2`)

   **Check 5: Documentation Completeness**
   - `README.md` exists? (fail if missing)
   - `CLAUDE.md` exists? (warn if missing, GAIA standard)
   - Docstrings on public functions? (info if coverage < 70%)
   - Changelog maintained? (info if no `CHANGELOG.md` or git tags)

2. **CLI Interface**

   **Usage:**
   ```bash
   # Scan current directory
   warden scan

   # Scan specific project
   warden scan --path X:\Projects\hart_os

   # Output JSON (for CI/CD integration)
   warden scan --format json --output scan_results.json

   # Fail on warnings (strict mode for CI)
   warden scan --strict
   ```

   **Output Format:**
   ```
   WARDEN Compliance Scan
   =====================
   Project: HART OS v6.2.8
   Scanned: 2026-02-08 14:32:15

   [FAIL] Git Status
   └─ Uncommitted changes detected (3 files modified)

   [PASS] Test Suite
   └─ 47 tests passing, 82% coverage

   [WARN] .env Safety
   └─ .env.example missing (recommended for onboarding)

   [PASS] Dependency Freshness
   └─ All dependencies up to date

   [WARN] Documentation
   └─ 12 public functions missing docstrings

   Summary: 2 failures, 2 warnings, 0 info
   Exit code: 1 (failures detected)
   ```

#### User Value

**Before Production:**
- Catch secrets exposure before they reach git history
- Ensure tests passing before deployment
- Validate documentation completeness before handoff

**In CI/CD Pipeline:**
- Automated gate in GitHub Actions / GitLab CI
- Block merge if compliance fails
- Generate compliance report for code review

**Periodic Audits:**
- Scheduled scans (weekly/monthly)
- Track compliance trends over time
- Identify projects needing maintenance

#### Critical Gap: Not Integrated

**Status:** Scanner exists (2 files, ~500 LOC) but not wired into any workflow.

**What Exists:**
- ✅ `scanner.py` with 5 compliance checks implemented
- ✅ CLI entry point (`warden scan` command)
- ✅ JSON output format (for CI/CD)
- ✅ EventBus integration (can emit telemetry to ARGUS)

**What's Missing:**
- ❌ No pre-commit hooks (can't block commits with secrets)
- ❌ No CI/CD integration (not in GitHub Actions workflows)
- ❌ No ARGUS telemetry (scanner runs but doesn't send events)
- ❌ No VULCAN integration (new projects don't get WARDEN configured)
- ❌ No scheduled scans (no cron job or task scheduler setup)

**Impact:**
- HART OS committed OpenAI API key to git history (scan would have caught this)
- ECHO has 19 manual UI versions (scan would have warned about lack of git discipline)
- Projects go weeks without test runs (no CI enforcement)

**Remediation Plan:**
- Phase 4: Add pre-commit hook to VULCAN project template
- Phase 4: Wire WARDEN telemetry into ARGUS event bus
- Phase 5: Create GitHub Actions workflow template with WARDEN gate
- Phase 6: Scheduled compliance scans via Windows Task Scheduler

#### Key Files

| File | Purpose | Lines | Status |
|------|---------|-------|--------|
| `scanner.py` | Main compliance scanner | 387 | ✅ Complete |
| `__init__.py` | Package init + CLI entry | 98 | ✅ Complete |
| `tests/test_scanner.py` | Scanner tests | 412 | ⚠️ Incomplete (58% coverage) |

---

### 4.7 ABIS — Visual System Builder

**Version:** v0.0.1
**Status:** Planning
**Location:** `X:\Projects\_GAIA\_ABIS`
**Framework:** React + React Flow (frontend), Python FastAPI (backend)
**Dependencies:** MYCEL, LOOM, ARGUS, MNEMIS

#### What It Does

ABIS provides a visual canvas where users design multi-agent systems by dragging, connecting, and configuring nodes. Unlike LOOM (which edits individual agents and workflows), ABIS enables designing entire systems visually.

**Core Capabilities (Planned):**

1. **Visual Node Editor (React Flow Canvas)**
   - Drag-and-drop interface for agent system design
   - Node types: Agents, Data Sources, LLM Calls, Memory Operations, Decision Points
   - Connection validation (type-safe edges, contract enforcement)
   - Real-time validation feedback

2. **System Graph Spec v1 (Canonical Schema)**
   - JSON schema for user-designed systems
   - Nodes: agent definitions, capabilities, authority levels
   - Edges: data flow, control flow, memory access
   - Metadata: version, author, GAIA integration level

3. **Graph Compiler (Visual → Executable)**
   - Compile System Graph Spec to executable LOOM workflow
   - Generate Python code from visual design
   - Validation: circular dependency detection, deadlock prevention
   - Output: GAIA-compliant project structure

4. **Design-Time Validation**
   - Authority level violations (agent trying to access higher-tier memory)
   - Type mismatches (string output connected to number input)
   - Circular dependencies (agent A → agent B → agent A)
   - Contract violations (agent exceeding capability boundaries)

5. **Replay Engine (Re-Execute Past Workflows)**
   - Load ARGUS Run Records (past workflow executions)
   - Replay on canvas with step-by-step visualization
   - Compare expected vs actual behavior
   - Debug failed workflows visually

**Distinction from LOOM:**

| | LOOM | ABIS |
|---|------|------|
| **Scope** | Individual agents/workflows | Entire multi-agent systems |
| **Interface** | Code + NL editing | Visual node canvas |
| **User** | Developer modifying agents | Non-technical user designing systems |
| **Output** | Modified agent configs | System Graph Spec JSON |

**Tech Stack (Planned):**
- **Frontend:** React + React Flow (visual node editor)
- **Backend:** Python FastAPI (graph compiler, validation engine)
- **Storage:** SQLite (system designs, version history)
- **Integration:** LOOM (workflow execution), ARGUS (replay engine), MNEMIS (pattern storage)

**Dependencies:** `mycel`, `loom`, `argus`, `mnemis`
**Tags:** `shared-service`, `visual-editor`, `no-code`, `system-builder`

#### User Workflow (Step-by-Step)

```
User → Launch ABIS Canvas → Drag Nodes → Connect Edges → Configure Agents
         ↓
     ABIS validates:
     1. Type safety (edges match input/output types)
     2. Authority levels (agents have proper memory access)
     3. Circular dependencies (no deadlocks)
     4. Contract violations (agents within capability bounds)
         ↓
     User → Compile to Workflow → Preview Generated Code → Export
         ↓
     ABIS generates:
     1. System Graph Spec JSON (canonical schema)
     2. LOOM workflow definition (executable)
     3. Python project structure (GAIA-compliant)
     4. Test scaffolding (pytest structure)
         ↓
     User → Execute Workflow → View Results in ARGUS Dashboard
```

**Time to Executable System:** 30 minutes (from blank canvas to working multi-agent system)

#### Current Status and Known Gaps

**Planned (Not Yet Implemented):**
- ❌ React Flow canvas UI
- ❌ System Graph Spec v1 schema definition
- ❌ Graph compiler (visual → executable)
- ❌ Replay engine (ARGUS Run Record visualization)
- ❌ LOOM integration (compile to workflow)

**ABIS-ARGUS Integration:**
ABIS queries ARGUS Mental Model Library (59 models) when users design agent nodes. When a user adds a "decision" node, ABIS suggests relevant mental models (e.g., Prospect Theory for financial decisions, Six Thinking Hats for brainstorming). This enables non-technical users to apply proven reasoning frameworks through visual configuration.

**Open Questions:**
- Should ABIS use LOOM's workflow engine directly, or have its own execution layer?
- System Graph Spec v1 — should it be a superset of LOOM's workflow spec or independent?
- ABIS frontend: React standalone or embedded in Streamlit via st.components?

---

### 4.8 RAVEN — Autonomous Research Agent

**Version:** v0.0.1
**Status:** Planning (not yet implemented)
**Location:** `X:\Projects\_GAIA\_RAVEN` (to be created)
**Framework:** Python CLI + MYCEL integration
**GAIA Role:** Ad-hoc research investigations across the ecosystem

#### What It Does

RAVEN is an autonomous research agent that performs investigations across the ecosystem. Given a research question, RAVEN searches documents (via MYCEL RAG), scans code repositories, queries external sources, and produces structured research reports.

**Use Cases:**
- Competitive analysis and market intelligence
- Dependency audits and vulnerability scanning
- Regulatory change scanning and compliance monitoring
- Knowledge gap discovery across projects
- Cross-project pattern analysis

**Architecture:**
- Uses MYCEL for document retrieval and RAG
- Stores findings in MNEMIS (PROJECT tier, promotable to GAIA tier)
- Tracked by ARGUS for research quality metrics
- Orchestrated by LOOM for multi-step investigations

**Dependencies:** `mycel`, `mnemis`, `argus`

---

### 4.9 Python Tools Governance

Standalone Python tools are not full GAIA products but still participate in ecosystem governance:

- **ARGUS telemetry:** Tools emit JSONL events for cost and usage tracking
- **MYCEL LLM:** Tools use MYCEL for LLM calls where applicable
- **MNEMIS patterns:** Tools contribute learned patterns to MNEMIS
- **WARDEN validation:** Tools are subject to secrets scanning and dependency checks
- **Registry tracking:** Tools are registered in registry.json with version and status
- **No LOOM orchestration:** Tools run independently, not as part of agent workflows

---

## Section 5: Product Catalog

GAIA ecosystem currently contains 5 production/active products, 2 planned products, and 1 stale product. This section provides detailed product cards for each, with emphasis on GAIA integration levels and success metrics.

### Product Comparison Table

| Product | Version | Status | Port | Tech Stack | GAIA Integration | LOC (Est.) | Test Coverage |
|---------|---------|--------|------|------------|------------------|------------|---------------|
| **HART OS** | v6.2.8 | Production | None | Streamlit, OpenAI | Standalone (pre-dates GAIA) | ~8,000 | 35% (target: 80%) |
| **VIA** | v6.4 | Production | 8503 | Streamlit, Gemini/OpenAI/Anthropic | MYCEL (LLM clients) | ~6,500 | Unknown |
| **DATA FORGE** | v1.1 | Production | None | Streamlit, OpenAI | Standalone (pre-dates GAIA) | ~4,200 | Unknown |
| **jSeeker** (formerly PROTEUS) | v0.2.1 | Active Dev | None | Streamlit, Anthropic | **Full GAIA** (MYCEL + ARGUS + MNEMIS) | ~5,800 | 67% |
| **DOS** | v0.0.1 | Planning | None | Streamlit (planned), Anthropic/OpenAI | Full GAIA (planned) | N/A | N/A |
| **GPT_ECHO** (reclassified from shared service) | v0.1.0 | Stale | None | Streamlit, Gemini | None (planned: MYCEL for RAG) | ~2,100 | None |

**GAIA Integration Legend:**
- **Standalone:** Pre-dates GAIA, not integrated
- **MYCEL:** Uses MYCEL for LLM clients and/or RAG
- **Full GAIA:** Uses MYCEL + sends telemetry to ARGUS + uses MNEMIS memory

---

### 5.1 HART OS — Therapeutic Scoring System

**Status Badge:** 🟢 Production
**Version:** v6.2.8
**Port:** None (local Streamlit app)
**Repository:** `https://github.com/ZoeDepthTokyo/hart-os.git`
**Location:** `X:\Projects\hart_os_v6`
**GAIA Integration Level:** Standalone (pre-dates GAIA, candidate for migration)

#### Target Users

- **Primary:** Art therapists conducting group therapy sessions with adolescents
- **Secondary:** Clinical supervisors reviewing therapist session plans
- **Tertiary:** Program designers developing curriculum for art therapy programs

#### Tech Stack

**Core:**
- Python 3.9+ (Pydantic v1, legacy version)
- Streamlit (single-page app, state management issues)
- OpenAI API (`gpt-4o` for optional copilot evaluation)

**Data Sources:**
- `hart_mapping_ledger_vFinal.md` (canonical phase mapping, 2,800+ lines)
- `hart_library_manifest_techniques_vFinal.md` (canonical technique library, 3,200+ lines)
- `config/locales/es.json` (all Spanish user-facing text)

**Localization:**
- Output language: Spanish (Mexico)
- Localization rule: **Zero Spanish in Python code** (constitutional constraint)
- All user-facing text in JSON config files

#### Key Features

1. **6-Component Scoring Algorithm** (75% Deterministic, 25% AI Augmentation)
   - Component 1: Dominant emotion identification
   - Component 2: Developmental phase selection (deterministic lookup from canonical ledger)
   - Component 3: Technique recommendation (ranked list from canonical manifest)
   - Component 4: Therapeutic approach selection
   - Component 5: Session structure generation
   - Component 6: Optional LLM copilot evaluation (clearly labeled as AI-generated)

2. **Deterministic Phase Selection**
   - Input: Patient intake form (age, presenting issue, emotional state, cognitive level)
   - Process: Lookup in `hart_mapping_ledger_vFinal.md` using exact match algorithm
   - Output: HART Phase (1-8) with confidence score
   - Constraint: Never guesses – if no match, prompts user for more info

3. **Technique Recommendation**
   - Input: Selected HART Phase + session goals
   - Process: Filter `hart_library_manifest_techniques_vFinal.md` by phase, rank by relevance
   - Output: Top 5 techniques with rationale, materials needed, time estimate
   - Constraint: Only recommends techniques present in canonical manifest

4. **Copilot LLM Evaluation** (Optional)
   - Input: Deterministic session plan (all 6 components)
   - Process: LLM synthesizes narrative summary and suggests adjustments
   - Output: "Guía para la Terapeuta" (Spanish-language session plan) with clear "AI-generated" label
   - Constraint: Copilot cannot override deterministic components, only augment

5. **Guía para la Terapeuta** (Session Plan Output)
   - Format: Spanish-language PDF (A4 size)
   - Sections:
     - Patient demographics (anonymized)
     - HART Phase and rationale
     - Recommended techniques (with materials list)
     - Session timeline (warm-up, main activity, closing)
     - Therapeutic goals and expected outcomes
     - Safety considerations and crisis protocols
   - Export: PDF download or print

6. **Glass-Box Traceability**
   - Every recommendation cites canonical source (file, line number, section)
   - Deterministic decisions show exact lookup path
   - LLM decisions show prompt, response, and model version
   - Audit trail stored in `logs/sessions.jsonl` (not currently sent to ARGUS)

#### Success Metrics (Proposed)

**Performance:**
- Generate Session Plan (deterministic only): < 3 seconds
- Generate Session Plan (with LLM copilot): < 8 seconds
- 90% of plans accepted without modification (therapist satisfaction)

**Quality:**
- 100% traceability to canonical sources (no hallucinated techniques)
- 95% of recommended techniques reported as "appropriate" by therapists
- Zero instances of Spanish text in Python code (localization compliance)

**Coverage:**
- 80% test coverage (target, current: 35%)
- All 8 HART phases covered by test cases
- All 42 canonical techniques tested for proper rendering

#### Known Issues

**Critical:**
- ⚠️ OpenAI API key exposed in git history (requires manual revocation and history rewrite)
- ⚠️ CI failing on GitHub Actions (blocking confidence in deployments)

**High:**
- Output does not always conform to approved Guía structure (formatting inconsistencies)
- State lost across navigation (Streamlit multi-page app state management issue)

**Medium:**
- No telemetry integration with ARGUS (logs exist but not sent to event bus)
- No MYCEL integration (uses direct OpenAI SDK, duplicates LLM client code)

**Low:**
- Test coverage 35% (target: 80%)
- No pre-commit hooks (allows commits with Spanish strings in Python)

#### Localization Rule (Constitutional)

**Zero Spanish in Python Code**

This is a **constitutional constraint** for HART OS:

**Rationale:**
- Spanish is a **data concern**, not a code concern
- Output language may change (Spanish → English for US therapists)
- Code should be language-agnostic

**Enforcement:**
```bash
# Pre-commit hook (not currently installed)
grep -r "[áéíóúñ¿¡]" src/hart_os/*.py
# Should return nothing (exit code 0)
```

**Correct Pattern:**
```python
# CORRECT ✅
from config.locales import load_locale

locale = load_locale("es")  # Spanish text in JSON file
print(locale["welcome_message"])  # "Bienvenida a HART OS"

# INCORRECT ❌
print("Bienvenida a HART OS")  # Spanish hardcoded in Python
```

**All Spanish text stored in:**
- `config/locales/es.json` (user-facing strings)
- `data/hart_library_manifest_techniques_vFinal.md` (canonical technique descriptions)
- `data/hart_mapping_ledger_vFinal.md` (canonical phase descriptions)

#### Future Roadmap

**Phase 4 (GAIA Migration):**
- Migrate to MYCEL LLM clients (remove direct OpenAI SDK dependency)
- Add ARGUS telemetry (emit events for session generation, technique selection)
- Upgrade to Pydantic v2 (align with GAIA ecosystem)

**Phase 5 (Quality):**
- Increase test coverage to 80%
- Fix CI/CD pipeline
- Add pre-commit hooks for localization compliance

**Phase 6 (Features):**
- Multi-language support (English, Portuguese)
- Therapist feedback loop (learn which techniques work best)
- MNEMIS integration (cross-project therapy patterns)

---

### 5.2 VIA — Investment Intelligence

**Status Badge:** 🟢 Production
**Version:** v6.4
**Port:** 8503
**Repository:** None (no GitHub remote configured)
**Location:** `X:\Projects\VIA`
**GAIA Integration Level:** MYCEL (LLM abstraction)

#### Target Users

- **Primary:** Investment analysts researching public companies
- **Secondary:** Portfolio managers building investment theses
- **Tertiary:** Financial researchers synthesizing market intelligence

#### Tech Stack

**Core:**
- Python 3.10+ (Pydantic v2)
- Streamlit (multi-page app, port 8503)
- Gemini (primary LLM, 1.5 Pro)
- OpenAI (fallback, gpt-4o)
- Anthropic (secondary fallback, claude-sonnet-4-5)

**RAG Pipeline:**
- MYCEL integration (LLM clients, chunking, embedding, retrieval)
- ChromaDB (vector database, local persistence)
- RecursiveCharacterChunker (1000-char chunks, 200-char overlap)
- Hybrid search (70% semantic, 30% keyword BM25)

**Data Sources:**
- 10-K annual reports (SEC EDGAR API)
- 10-Q quarterly reports
- Earnings call transcripts (manual upload)
- Analyst reports (manual upload, PDF parsing)

#### Key Features

1. **Multi-Source RAG Synthesis**
   - Ingests 10-Ks, 10-Qs, earnings calls, analyst reports
   - Chunks documents preserving section boundaries
   - Embeds using OpenAI `text-embedding-3-small`
   - Retrieves top-k chunks using hybrid search (semantic + keyword)
   - Synthesizes cross-document insights ("10-K mentions supply chain risk, earnings call confirms 3 supplier delays")

2. **Semantic Claims Tracking**
   - Every claim in synthesis cites specific source with page number or timestamp
   - Example: "Revenue grew 23% YoY [10-K 2024, p.42] driven by enterprise segment [Earnings Call Q3 2024, 14:32]"
   - No unsupported claims allowed (LLM prompted to cite or abstain)
   - Citation validation: user can click citation to see original source chunk

3. **Three-Provider Ensemble** (Cross-Validation)
   - Gemini generates initial synthesis
   - OpenAI validates claims (checks for consistency)
   - Anthropic provides alternative interpretation (contrarian view)
   - User sees all 3 outputs with agreement score (0-100%)
   - High agreement (>80%) = high confidence, low agreement (<50%) = investigate further

4. **Investment Thesis Generation**
   - **Bull Case:** Synthesizes positive signals (revenue growth, margin expansion, competitive advantages)
   - **Bear Case:** Synthesizes risks (debt load, regulatory exposure, competitive threats)
   - **Base Case:** Probabilistic weighted average of bull/bear
   - Each case cites specific evidence from source documents

5. **Evidence-Based Reasoning**
   - LLM prompted with: "Never make claims without citing source document, page, and section"
   - Prompt engineering: "If you cannot find evidence, say 'No evidence found in available documents' instead of guessing"
   - Post-processing: Filter out any claim without citation marker `[source, location]`

#### Success Metrics (Proposed)

**Quality:**
- Synthesis quality score ≥ 4.0/5.0 (expert evaluation by financial analysts)
- Citation accuracy ≥ 95% (cited page numbers match claim content)
- Cross-provider agreement ≥ 80% (Gemini, OpenAI, Anthropic align on key claims)

**Performance:**
- 50% time savings vs. manual research (analyst self-report)
- Process 100-page 10-K in < 5 minutes (ingest + embed + ready for queries)
- Query response time < 10 seconds (retrieve + synthesize)

**Coverage:**
- 100% of claims cited (zero unsupported claims)
- Top-5 retrieval precision ≥ 70% (relevant chunks in top 5)

#### GAIA Integration

**MYCEL Usage:**
- LLM client abstraction (switch providers without code changes)
- `RecursiveCharacterChunker` for document splitting
- `OpenAIEmbedder` for vectorization
- `VectorRetriever` for hybrid search

**Benefits:**
- No duplicate LLM client code (reuses MYCEL implementations)
- Provider fallback (if Gemini rate-limited, auto-switch to OpenAI)
- Consistent configuration (all API keys in `.env` via `GaiaSettings`)

**Missing GAIA Integration:**
- ❌ No ARGUS telemetry (query patterns, synthesis quality, cost tracking not visible)
- ❌ No MNEMIS integration (learned investment patterns stay in VIA, not shared with other projects)

#### Known Issues

**Medium:**
- No git remote configured (local-only, no backup)
- Citation parsing occasionally fails for PDFs with complex layouts (tables, multi-column)
- Gemini 1.5 Pro occasionally hallucinates citations (cites page that doesn't exist)

**Low:**
- No test coverage metrics (tests exist but not measured)
- Streamlit state management issues (query history lost on page refresh)

#### Future Roadmap

**Phase 4 (GAIA Integration):**
- Add ARGUS telemetry (track synthesis quality, citation accuracy, provider agreement)
- Add MNEMIS integration (store validated investment patterns for reuse)

**Phase 5 (Quality):**
- Implement citation validation layer (check cited page exists and contains relevant text)
- Add test coverage measurement (target: 70%)
- Configure git remote (GitHub or GitLab)

**Phase 6 (Features):**
- Real-time data ingestion (SEC EDGAR API auto-fetch new filings)
- Automated thesis updates (re-synthesize when new 10-Q published)
- Comparison view (compare 2 companies side-by-side)

---

### 5.3 DATA FORGE — Data Processing Engine

**Status Badge:** 🟢 Production
**Version:** v1.1
**Port:** None (local Streamlit app)
**Repository:** None (no GitHub remote configured)
**Location:** `X:\Projects\Python tools\data-forge-v1.1`
**GAIA Integration Level:** Standalone (pre-dates GAIA, candidate for MYCEL migration)

#### Target Users

- **Primary:** Data engineers building ETL pipelines
- **Secondary:** Analysts transforming unstructured data into structured formats
- **Tertiary:** Researchers generating taxonomies from document collections

#### Tech Stack

**Core:**
- Python 3.10+
- Streamlit (multi-page app)
- OpenAI API (`gpt-4o` for taxonomy generation and pipeline compilation)
- Pandas (data transformation)
- JSON/YAML (pipeline definitions)

**Data Processing:**
- No external database dependencies (local file operations)
- Supports CSV, JSON, XML, TXT input formats
- Outputs CSV, JSON, Parquet

#### Key Features

1. **Memory Bank Architecture** (Reusable Transformation Patterns)
   - Library of pre-built data transformations (e.g., "extract dates from text", "normalize phone numbers", "deduplicate records")
   - User can save custom transformations to memory bank
   - Transformations compose into pipelines
   - Memory bank stored as JSON definitions in `data/memory_bank/`

2. **Taxonomy Builder** (Auto-Generate Taxonomies from Unstructured Data)
   - Input: Collection of documents (e.g., 500 customer support tickets)
   - Process: LLM extracts recurring themes, clusters into categories, generates hierarchy
   - Output: JSON taxonomy with categories, subcategories, example terms
   - Use case: "What are the main customer complaint categories?"

   **Example Output:**
   ```json
   {
     "root": "Customer Support Issues",
     "categories": [
       {
         "name": "Billing",
         "subcategories": ["Payment Failed", "Incorrect Charge", "Refund Request"],
         "frequency": 127,
         "examples": ["credit card declined", "charged twice", "want my money back"]
       },
       {
         "name": "Technical",
         "subcategories": ["Login Issues", "Performance", "Bug Reports"],
         "frequency": 203,
         "examples": ["can't log in", "app is slow", "feature not working"]
       }
     ]
   }
   ```

3. **Data Processing Compiler** (Natural Language → Data Pipeline)
   - Input: Natural language description ("Extract all email addresses from column A, normalize to lowercase, remove duplicates, export to CSV")
   - Process: LLM generates pipeline definition (sequence of transformations)
   - Validation: Deterministic checks (column exists? output format valid?)
   - Output: Executable Python script or JSON pipeline definition

4. **Pipeline Validation** (Deterministic Pre-Execution Checks)
   - Schema validation: Input columns exist?
   - Type validation: Transformation expects string, receives int?
   - Output validation: Output path writable?
   - Dry-run mode: Show preview of first 10 rows without executing full pipeline

#### Success Metrics (Proposed)

**Performance:**
- Pipeline generation < 30 seconds (natural language → executable pipeline)
- Generated pipelines pass validation ≥ 90%
- User editing time < 5 minutes per pipeline (minimal manual corrections needed)

**Quality:**
- Taxonomy accuracy ≥ 80% (user confirms generated categories are meaningful)
- Transformation correctness ≥ 95% (pipeline produces expected output)

**Coverage:**
- All common data transformations in memory bank (50+ pre-built transformations)
- Supports 10+ input/output formats

#### GAIA Integration Status

**Current:** Standalone (pre-dates GAIA ecosystem)

**Candidate for MYCEL Migration:**
- Replace direct OpenAI SDK calls with MYCEL LLM clients
- Use `GaiaSettings` for configuration
- Add ARGUS telemetry (track pipeline generation, execution time, error rates)

**Benefits of Migration:**
- Provider flexibility (switch from OpenAI to Anthropic if needed)
- Consistent error handling (reuse MYCEL retry logic)
- Cost tracking (automatically logged to ARGUS)

#### Known Issues

**Medium:**
- No git remote configured (local-only, no backup)
- Direct OpenAI SDK usage (duplicates LLM client code)
- No telemetry (pipeline usage patterns not tracked)

**Low:**
- No test coverage metrics
- Memory bank not versioned (transformations can be accidentally overwritten)

#### Future Roadmap

**Phase 4 (GAIA Migration):**
- Migrate to MYCEL LLM clients
- Add ARGUS telemetry
- Configure git remote

**Phase 5 (Quality):**
- Add test coverage (target: 70%)
- Version control for memory bank (git-track transformation definitions)

**Phase 6 (Features):**
- Streaming data support (real-time pipelines)
- Pipeline scheduling (cron-like execution)
- Collaboration features (share memory bank transformations across team)

---

### 5.4 jSeeker — Job Seeking & Resume Adaptation (formerly PROTEUS)

**Status Badge:** 🟡 Active Development
**Version:** v0.2.1
**Port:** None (local Streamlit app, co-launches with ARGUS dashboard)
**Repository:** None (no GitHub remote configured)
**Location:** `X:\Projects\_GAIA\_PROTEUS` (to be moved to `X:\Projects\jSeeker`)
**GAIA Integration Level:** 🏆 **Full GAIA** (MYCEL + ARGUS + MNEMIS) — Most Integrated Product
**Note:** Renamed from PROTEUS to avoid confusion with shared services naming

#### Target Users

- **Primary:** Job seekers in product design and technical leadership roles
- **Secondary:** Career coaches assisting clients with resume optimization
- **Tertiary:** Recruiters testing ATS compatibility of resumes

#### Tech Stack

**Core:**
- Python 3.10+ (Pydantic v2)
- Streamlit (multi-page app with wizard workflow)
- Anthropic API (`claude-haiku-4` for cheap tasks, `claude-sonnet-4-5` for quality)

**Document Generation:**
- Playwright (headless browser for PDF rendering)
- python-docx (DOCX generation for ATS compatibility)
- Jinja2 (HTML template rendering)

**Persistence:**
- SQLite (`data/proteus.db`): Application tracking, job status, resume versions

**GAIA Integration:**
- **MYCEL:** LLM client via `integrations/mycel_bridge.py` (fallback to direct Anthropic SDK)
- **ARGUS:** Telemetry via `integrations/argus_telemetry.py` (build + runtime events)
- **MNEMIS:** Pattern storage via `integrations/mnemis_bridge.py` (Phase 3+, planned)

#### Key Features

1. **JD-Optimized Resume Generation** (< 1 Minute)
   - **Input:** User pastes job description (any format)
   - **Process:**
     - Parse JD → extract requirements, keywords, seniority level, company context
     - Prune irrelevant sections (e.g., "About Us" fluff)
     - Match resume blocks to JD requirements (semantic similarity)
     - Adapt block phrasing to mirror JD language (Claude Sonnet pass)
   - **Output:** Tailored resume in 2 formats (PDF + DOCX) in < 1 minute

2. **ATS Platform Intelligence** (Platform-Specific Scoring)
   - Supports 6 ATS platforms: Workday, Greenhouse, Lever, iCIMS, Ashby, Taleo
   - Platform profiles stored in `data/ats_profiles/`:
     - Keyword extraction algorithm (exact match vs. semantic)
     - Formatting restrictions (tables allowed? multi-column?)
     - Parsing quirks (dates format, bullet point symbols)
   - Scoring algorithm:
     ```python
     score = (
         keyword_match_score * 0.40 +
         formatting_compliance * 0.25 +
         semantic_overlap * 0.20 +
         experience_relevance * 0.15
     )
     ```
   - Target: ATS score ≥ 85/100 for selected platform

3. **Dual-Format Rendering** (Human-Facing PDF + ATS-Facing DOCX)
   - **PDF (Human-Facing):**
     - Two-column layout (left: contact + skills, right: experience + education)
     - Custom fonts (Source Sans Pro)
     - Color accents (subtle, professional)
     - Rendered via Playwright (headless Chrome)
   - **DOCX (ATS-Facing):**
     - Single-column layout (no complex formatting)
     - Standard fonts (Arial, Calibri)
     - No tables, images, or headers/footers
     - Generated via python-docx
   - User downloads both formats: PDF for recruiter inbox, DOCX for ATS upload

4. **Recruiter Outreach** (Auto-Generated Personalized Messages)
   - **Input:** Job posting URL + company research (manual or scraped)
   - **Process:** Claude generates 3 outreach variants:
     - **Cold Email:** For recruiters not yet contacted
     - **LinkedIn Message:** For InMail (200-char limit)
     - **Follow-Up:** For second touchpoint after 1 week
   - **Personalization:** References specific JD details ("I noticed your team is migrating to microservices...")
   - **Output:** Copy-pasteable text, stored in SQLite for tracking

5. **Application Tracking** (SQLite-Backed Status Management)
   - **Schema:**
     ```sql
     CREATE TABLE applications (
         id INTEGER PRIMARY KEY,
         job_title TEXT,
         company TEXT,
         jd_url TEXT,
         status TEXT,  -- "active", "closed", "expired"
         applied_date DATE,
         resume_version TEXT,  -- filename of generated resume
         ats_platform TEXT,
         ats_score INTEGER,
         outreach_sent BOOLEAN,
         notes TEXT
     );
     ```
   - **UI:** Kanban board (Active / Interviewing / Offered / Rejected / Ghosted)
   - **Tracking:** User updates status, PROTEUS logs events to ARGUS

6. **Job Discovery** (Tag-Based Search Across Job Boards)
   - **Input:** User defines tags (e.g., "product-design", "remote", "series-a")
   - **Process:** Scrapes job boards (LinkedIn, AngelList, Y Combinator) for matching posts
   - **Output:** Ranked list of jobs with match score (0-100%)
   - **Status:** Phase 4 in progress (basic scraper operational, ranking incomplete)

7. **Feedback Loop** (Learn Which Resume Patterns → Interviews)
   - **Hypothesis:** Resumes with certain patterns (e.g., "metrics-heavy bullet points") yield more interviews
   - **Tracking:** User marks applications as "interviewed" or "rejected"
   - **Analysis:** PROTEUS correlates resume patterns with outcomes
   - **Learning:** Successful patterns promoted to MNEMIS (ecosystem-wide knowledge)
   - **Status:** Phase 5 planned (tracking exists, analysis not implemented)

#### Pipeline Architecture

```
┌─────────────────┐
│ 1. JD Parse     │  Parse job description, extract requirements
└────────┬────────┘
         ↓
┌─────────────────┐
│ 2. Block Match  │  Match resume blocks to JD (semantic similarity)
└────────┬────────┘
         ↓
┌─────────────────┐
│ 3. Adapt        │  Claude Sonnet rewrites blocks to mirror JD language
└────────┬────────┘
         ↓
┌─────────────────┐
│ 4. ATS Score    │  Score against target ATS platform profile
└────────┬────────┘
         ↓
┌─────────────────┐
│ 5. Render       │  Generate PDF (Playwright) + DOCX (python-docx)
└────────┬────────┘
         ↓
┌─────────────────┐
│ 6. Track        │  Save to SQLite, emit telemetry to ARGUS
└────────┬────────┘
         ↓
┌─────────────────┐
│ 7. Feedback     │  User updates status, learn patterns (Phase 5)
└─────────────────┘
```

**Execution Time:**
- Steps 1-2: 10 seconds (parsing + matching)
- Step 3: 35 seconds (Claude Sonnet adaptation, main cost)
- Steps 4-6: 15 seconds (scoring + rendering + persistence)
- **Total:** ~60 seconds (1 minute) from JD paste to PDF download

#### Success Metrics (Current)

**Performance:**
- ✅ Resume generation < 1 minute (target: met)
- ✅ ATS score ≥ 85/100 for target platform (met in 72% of cases)
- 🟡 Application-to-interview rate: 0% baseline → 2-5% target (in progress, insufficient data)
- ✅ User editing time < 5 minutes post-generation (self-report)

**Quality:**
- ✅ Zero hallucinated experience (all content from resume_blocks YAML)
- ✅ 100% of adaptations preserve factual accuracy (manual review)
- 🟡 ATS parsing accuracy (unknown, needs manual validation)

**Cost:**
- ✅ Average cost per resume: $0.03 (Claude Sonnet call)
- ✅ Monthly budget: $50 (supports ~1,600 resumes/month)

#### GAIA Integration Showcase: MOST GAIA-INTEGRATED PRODUCT

PROTEUS is the **most deeply integrated** GAIA product. Every GAIA component is wired in:

**1. MYCEL Integration (LLM Abstraction)**

   **File:** `proteus/integrations/mycel_bridge.py`

   ```python
   from rag_intelligence.integrations import create_llm_client
   from rag_intelligence.config import GaiaSettings

   class ProteusLLMClient:
       def __init__(self):
           self.config = GaiaSettings()
           self.client = create_llm_client(
               provider="anthropic",
               model=self.config.anthropic_model
           )

       def adapt_resume_block(self, block: str, jd_context: str) -> str:
           """Use MYCEL client to adapt resume block."""
           return self.client.generate(
               prompt=self._build_adaptation_prompt(block, jd_context),
               max_tokens=500,
               temperature=0.7
           )
   ```

   **Benefits:**
   - Switch providers without code changes (anthropic → openai if needed)
   - Automatic retry and error handling (reuses MYCEL logic)
   - Consistent cost tracking (MYCEL logs tokens to ARGUS)

**2. ARGUS Integration (Telemetry + Mental Models)**

   **File:** `proteus/integrations/argus_telemetry.py`

   ```python
   from argus.event_bus import EventBus

   bus = EventBus()

   def emit_resume_generated(jd_id: str, ats_score: int, blocks_used: int, cost_usd: float):
       """Emit event to ARGUS event bus."""
       bus.emit(
           component="PROTEUS",
           event="resume_generated",
           details={
               "jd_id": jd_id,
               "ats_score": ats_score,
               "blocks_used": blocks_used,
               "cost_usd": cost_usd,
               "latency_ms": measure_latency(),
               "model": "claude-sonnet-4-5"
           }
       )
   ```

   **Events Emitted:**
   - `resume_generated`: Full pipeline completion
   - `llm_call_started` / `llm_call_completed`: LLM latency tracking
   - `ats_score_calculated`: ATS platform scoring results
   - `application_status_updated`: User changes job status in tracker
   - `block_match_completed`: Semantic matching results

   **ARGUS Dashboard View:**
   - Real-time event stream shows PROTEUS activity
   - Cost tracker aggregates LLM spend by component (PROTEUS visible)
   - Agent trace viewer (planned) will show step-by-step resume generation

**3. MNEMIS Integration (Pattern Storage) — Phase 3 Planned**

   **File:** `proteus/integrations/mnemis_bridge.py` (stubbed, not operational)

   ```python
   from mnemis.core.memory_store import MnemisStore

   store = MnemisStore()

   def promote_successful_pattern(pattern_id: str, interview_outcome: bool):
       """
       If resume pattern → interview, promote to GAIA memory.
       Other projects (VIA, HART OS) can learn from this.
       """
       if interview_outcome:
           store.promote_memory(
               memory_id=pattern_id,
               from_tier=2,  # PROJECT
               to_tier=3,    # GAIA
               reason="Pattern yielded interview (validated outcome)"
           )
   ```

   **Planned Use Cases:**
   - Store validated resume patterns (e.g., "action verb + metric" bullet format)
   - Promote patterns that yield interviews to GAIA tier
   - Other products (VIA investment memos, HART OS therapy notes) can import writing patterns

**4. VULCAN Compliance (Created by VULCAN)**

   PROTEUS was created using VULCAN Creative Adapter:
   - ✅ `CLAUDE.md` with constitutional constraints
   - ✅ `config.py` inheriting from `GaiaSettings`
   - ✅ `logs/` directory for telemetry
   - ✅ `.gitignore` with secrets protection
   - ✅ `tests/` with pytest structure (67% coverage)
   - ✅ Registry entry in `X:\Projects\_GAIA\registry.json`

**5. Co-Launch with ARGUS Dashboard**

   **File:** `launch.py`

   ```python
   import subprocess
   import webbrowser

   def launch_proteus_ecosystem():
       """Launch PROTEUS + ARGUS dashboard together."""
       # Start PROTEUS Streamlit app
       proteus_proc = subprocess.Popen([
           "streamlit", "run", "ui/main.py",
           "--server.port", "8502"
       ])

       # Start ARGUS dashboard
       argus_proc = subprocess.Popen([
           "streamlit", "run", "X:/Projects/_GAIA/_ARGUS/dashboard/app.py",
           "--server.port", "8504"
       ])

       # Open both in browser
       webbrowser.open("http://localhost:8502")  # PROTEUS
       webbrowser.open("http://localhost:8504")  # ARGUS

   if __name__ == "__main__":
       launch_proteus_ecosystem()
   ```

   **User Experience:**
   - Single command: `python launch.py`
   - Two browser tabs open: PROTEUS (resume generation) + ARGUS (monitoring)
   - Real-time: Generate resume in PROTEUS → see event appear in ARGUS dashboard

#### Application Tracking

**SQLite Schema:**
```sql
CREATE TABLE applications (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_title TEXT NOT NULL,
    company TEXT NOT NULL,
    jd_url TEXT,
    status TEXT DEFAULT 'active',  -- active, closed, expired
    applied_date DATE NOT NULL,
    resume_version TEXT,  -- filename: "resume_abc123.pdf"
    ats_platform TEXT,    -- Workday, Greenhouse, etc.
    ats_score INTEGER,
    outreach_sent BOOLEAN DEFAULT 0,
    interview_date DATE,
    outcome TEXT,  -- "interviewed", "rejected", "ghosted", "offered"
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE resume_versions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    application_id INTEGER,
    version_name TEXT,
    pdf_path TEXT,
    docx_path TEXT,
    blocks_used TEXT,  -- JSON list of block IDs
    ats_score INTEGER,
    generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (application_id) REFERENCES applications(id)
);
```

**UI (Kanban Board):**
- Columns: Active / Interviewing / Offered / Rejected / Ghosted
- Drag-and-drop status updates
- Click card → view details (JD, resume version, notes, outreach history)

#### Job Discovery

**Status:** Phase 4 in progress (basic scraper operational, ranking incomplete)

**Workflow:**
1. User defines tags: `["product-design", "remote", "series-a", "ai-ml"]`
2. PROTEUS scrapes job boards (LinkedIn, AngelList, Y Combinator)
3. Jobs ranked by match score (0-100%):
   - Tag overlap: 4/4 tags → 100%, 3/4 → 75%, etc.
   - Semantic similarity: embed(user_profile) vs embed(jd)
   - Recency: Posted < 7 days → boost score
4. User sees ranked list, clicks to view JD, clicks "Generate Resume" → loops back to main pipeline

**Current Limitation:** Scraping works for Y Combinator (no auth required), fails for LinkedIn (requires login)

#### Feedback Loop

**Hypothesis:** Certain resume patterns (e.g., metrics-heavy, action verbs, role-specific keywords) yield higher interview rates.

**Tracking (Operational):**
- User marks application status: "interviewed", "rejected", "ghosted"
- SQLite stores outcome linked to resume version

**Analysis (Phase 5 Planned):**
- Correlate resume patterns with outcomes
- Example analysis:
  ```python
  # Which bullet point patterns → interviews?
  interviewed_resumes = get_resumes_where(outcome="interviewed")
  rejected_resumes = get_resumes_where(outcome="rejected")

  interviewed_patterns = extract_patterns(interviewed_resumes)
  rejected_patterns = extract_patterns(rejected_resumes)

  # Compare: "Action verb + metric + context" appears in 80% of interviewed
  # resumes but only 40% of rejected resumes → strong signal
  ```

**Learning (Phase 6 Planned):**
- Successful patterns promoted to MNEMIS GAIA tier
- Other products (VIA memos, HART OS therapy notes) can import writing best practices

#### Current Focus (Active Development)

**Phase 4 Milestones:**
- ✅ Application tracking SQLite schema implemented
- ✅ Kanban board UI operational
- 🟡 Job discovery scraper (Y Combinator works, LinkedIn blocked)
- 🟡 Outreach message generation (working, needs polish)

**Next Steps:**
- Complete job discovery ranking algorithm
- Add email integration (auto-send outreach messages)
- Implement feedback loop analysis (correlate patterns with outcomes)

#### Known Issues

**Medium:**
- No git remote configured (local-only, no backup)
- Job discovery blocked by LinkedIn auth (requires workaround or scraping service)
- DOCX rendering occasionally breaks formatting for complex bullet points

**Low:**
- Test coverage 67% (target: 80%)
- Playwright PDF rendering slow (2-3 seconds per resume)
- SQLite database not backed up (risk of data loss)

#### Future Roadmap

**Phase 5 (Feedback Loop):**
- Implement pattern analysis (correlate resume structure with interview outcomes)
- Add A/B testing (generate 2 resume variants, track which performs better)
- Integrate with MNEMIS (promote successful patterns to GAIA tier)

**Phase 6 (Automation):**
- Email integration (auto-send outreach messages)
- Calendar integration (schedule follow-ups)
- Browser extension (one-click resume generation from job posting)

**Phase 7 (Expansion):**
- Multi-user support (career coaches managing multiple clients)
- Template marketplace (share resume templates across users)
- Integration with job boards (auto-apply via API)

---

### 5.5 DOS — Multi-Agent Decision System

**Status:** Planning
**Path:** `X:\Projects\DOS`
**Version:** 0.0.1
**Framework:** Streamlit (planned)
**Providers:** Anthropic, OpenAI

**Description:** DOS is a multi-agent decision system for high-stakes, multi-criteria decisions (investment choices, strategic planning, resource allocation). Multiple specialized agents analyze different dimensions of a decision, debate findings, and produce synthesized recommendations with confidence scoring.

**Key Capabilities (Planned):**
- Multi-agent parallel analysis (financial, regulatory, market, risk dimensions)
- Agent debate protocol with conflict detection
- Decision matrix synthesis with weighted scoring
- Evidence trail linking to source documents
- Integration with ABIS for visual workflow design

**GAIA Integration:**
- Uses MYCEL for LLM clients and RAG retrieval
- Uses MNEMIS for decision history and pattern storage
- Uses ARGUS for cost tracking and explainability
- Uses ABIS for visual agent system design (planned)

**Dependencies:** `mycel`, `mnemis`, `argus`
**Tags:** `product`, `multi-agent`, `decision-system`, `synthesis`

---

### 5.6 jSeeker — Job Seeking & Resume Adaptation

**Status:** Active (v0.2.1)
**Path:** `X:\Projects\jSeeker` (to be moved from `X:\Projects\_GAIA\_PROTEUS`)
**Version:** 0.2.1
**Framework:** Streamlit
**Providers:** Anthropic
**Formerly:** PROTEUS (renamed to avoid confusion with shared services)

**Description:** jSeeker is an automated resume adaptation engine that tailors resumes to specific job descriptions, optimizes for ATS (Applicant Tracking System) scoring, generates recruiter outreach messages, and tracks job applications.

**Key Capabilities:**
- Resume parsing and structured extraction
- JD (Job Description) analysis with requirement matching
- AI-powered resume adaptation with ATS optimization
- Bilingual support (English/Spanish)
- Application tracking with status management
- Recruiter outreach message generation
- PDF and DOCX output with professional templates

**GAIA Integration:**
- Uses MYCEL for LLM client (Anthropic Claude)
- Uses ARGUS telemetry (ONLY product with active telemetry)
- MNEMIS integration planned for Phase 3+ (resume pattern storage)
- Has CLAUDE.md for agent alignment

**Dependencies:** `mycel`
**Tags:** `product`, `job-seeking`, `resume`, `ats`, `automation`

---

### 5.7 GPT_ECHO — ChatGPT Archaeology & Search (Stale, Reclassified as Product)

**Status Badge:** 🔴 Stale (Last Modified: January 2026)
**Version:** v0.1.0
**Port:** None
**Repository:** None
**Location:** `X:\Projects\_GAIA\_ECHO`
**GAIA Integration Level:** None (planned: MYCEL for RAG, ARGUS for monitoring)

**NOTE (Feb 2026):** GPT_ECHO has been reclassified from shared service to product. Originally conceived as shared chat archaeology infrastructure, GPT_ECHO is actually a user-facing ChatGPT conversation mirroring/search application. It uses shared services (MYCEL for RAG, ARGUS for monitoring, MNEMIS for learnings) rather than providing infrastructure to other components.

#### Target Users

- **Original Intent:** Users analyzing ChatGPT conversation history exports

#### Tech Stack

**Core:**
- Python 3.10+
- Streamlit (single-page app, no routing)
- Gemini (primary LLM, 1.5 Pro)

**Data Processing:**
- JSON parsing (ChatGPT export format)
- No persistence (conversations processed in-memory)

#### Key Features (As Designed)

1. **Conversation Taxonomy**
   - Parse ChatGPT JSON export
   - Auto-categorize conversations by topic (e.g., "coding help", "creative writing", "research")
   - Generate taxonomy hierarchy (categories → subcategories → example conversations)

2. **Pattern Detection**
   - Identify recurring conversation types (e.g., "user asks for debugging help")
   - Detect conversation evolution (e.g., "single-turn questions" vs "multi-turn deep dives")

3. **Export Processing**
   - Parse ChatGPT JSON export format (handle malformed JSON)
   - Extract conversation threads (group messages by conversation_id)
   - Render conversation view (markdown formatting)

#### Critical Issue: 19 Manual UI Versions

**Anti-Pattern Detected:**

```
X:\Projects\_GAIA\_ECHO\
├── ui_v0.py      (original version, 2024-12-10)
├── ui_v01.py     (2024-12-12)
├── ui_v02.py     (2024-12-15)
├── ...
├── ui_v012.py    (2025-01-08, latest)
├── ui_latest.py  (symlink? duplicate?)
├── ui.py         (which version is this?)
```

**Diagnosis:**
- **Root Cause:** Lack of version control discipline during development
- **Impact:** Unclear which version is "production", impossible to track changes, no rollback capability
- **Git Status:** Not under version control (`.git` directory does not exist)

**Why This Happened:**
- Developer manually copied files instead of using git branches
- No CI/CD gate to enforce git commits
- No WARDEN compliance scanner to catch this (scanner exists but not integrated)

#### Status: Stale

**Last Modified:** January 8, 2026 (no activity for 30+ days)

**Indicators of Abandonment:**
- No recent commits (git not initialized)
- No issue tracker or roadmap
- No test suite
- Streamlit app doesn't launch (import errors)

#### Recommendation: Rescue vs. Retire Decision Needed

**Option A: Rescue (If Value Justifies Effort)**

**Effort:** 2-3 days

**Steps:**
1. **Consolidate versions:**
   - Identify latest working version (`ui_v012.py` or `ui_latest.py`?)
   - Delete all other versions (`ui_v0.py` through `ui_v011.py`)
   - Rename to canonical `ui.py`

2. **Initialize git:**
   - `git init`
   - `git add .`
   - `git commit -m "Initial commit: consolidate 19 manual versions into single ui.py"`

3. **GAIA integration:**
   - Migrate to MYCEL LLM client (replace direct Gemini SDK)
   - Add `GaiaSettings` for configuration
   - Add ARGUS telemetry (track taxonomy generation, pattern detection)

4. **Fix broken imports:**
   - Install missing dependencies
   - Fix Streamlit state management issues
   - Add error handling for malformed JSON

5. **Add tests:**
   - Pytest structure (`tests/test_parser.py`, `tests/test_taxonomy.py`)
   - Target: 60% coverage minimum

6. **Register in GAIA:**
   - Add entry to `X:\Projects\_GAIA\registry.json`
   - Create `CLAUDE.md` with project context

**Decision Criteria for Rescue:**
- ❓ Is ChatGPT export analysis still valuable? (ChatGPT may have changed export format)
- ❓ Is there user demand? (any requests for this feature?)
- ❓ Could this be generalized? (analyze any conversation history, not just ChatGPT?)

**Option B: Retire (Archive and Document Learnings)**

**Effort:** 1 hour

**Steps:**
1. **Archive:**
   - Move to `X:\Projects\_GAIA\_ARCHIVE\ECHO`
   - Add `ARCHIVED.md` with status and reason

2. **Document learnings:**
   - Create `POSTMORTEM.md`:
     - What went wrong? (lack of git discipline, no version control)
     - What would we do differently? (enforce git from day 1, WARDEN pre-commit hooks)
     - Key lesson: **Version control is not optional**

3. **Extract reusable code:**
   - ChatGPT JSON parser → move to MYCEL (could be useful for other projects)
   - Taxonomy generation prompt → save to ARGUS mental models

4. **Update registry:**
   - Mark status: "archived" in `registry.json`

**Decision Criteria for Retirement:**
- ✅ No user demand (no requests in 30+ days)
- ✅ High rescue effort (19 versions to consolidate)
- ✅ Unclear value (ChatGPT export format may have changed)
- ✅ Better alternatives exist (ChatGPT now has built-in conversation search)

#### Success Metrics (If Rescued)

**Quality:**
- Conversation classification accuracy ≥ 80% (user confirms categories are meaningful)
- Processing time < 10 seconds per 100 conversations
- Single version under git control (zero manual version copies)

**Compliance:**
- Git initialized with remote backup
- Test coverage ≥ 60%
- WARDEN compliance scan passes (no secrets, no duplicate versions)

#### Recommendation

**Retire** unless user demand materializes. Effort to rescue (2-3 days) exceeds likely value. Focus resources on active products (PROTEUS, HART OS, VIA) instead.

---

## Summary: Product Catalog at a Glance

| Product | Users | Status | Integration | Next Action |
|---------|-------|--------|-------------|-------------|
| **HART OS** | Art therapists | Production | Standalone | Migrate to MYCEL, add ARGUS telemetry |
| **VIA** | Investment analysts | Production | MYCEL | Add ARGUS telemetry, add MNEMIS |
| **DATA FORGE** | Data engineers | Production | Standalone | Migrate to MYCEL |
| **jSeeker** (formerly PROTEUS) | Job seekers | Active Dev | **Full GAIA** | Complete Phase 4 (job discovery), implement feedback loop |
| **DOS** | Decision makers | Planning | Planned | Design multi-agent architecture, ABIS integration |
| **ECHO** | ChatGPT users | Stale | None | **Retire** (no demand, high rescue cost) |

**Key Insight:** jSeeker (formerly PROTEUS) is the GAIA integration showcase. Future products should follow jSeeker pattern:
- Use MYCEL for LLM abstraction
- Emit telemetry to ARGUS event bus
- Store patterns in MNEMIS for cross-project learning
- Co-launch with ARGUS dashboard for monitoring

---

**End of Sections 4-5**

**Next Sections (Planned):**
- Section 6: Integration Patterns (how products integrate with GAIA)
- Section 7: Roadmap & Prioritization (Phases 4-7)
- Section 8: Success Metrics & KPIs (ecosystem health)

---

# Part III: Execution

## Section 6: Implementation Roadmap

### Overview

**Total Duration:** 8 weeks (Emergency Fixes → Enforcement → Observability → Learning → Automation)
**Success Metric:** Zero undetected agent failures, 100% CLAUDE.md compliance, 80%+ test coverage ecosystem-wide, cost visibility on every operation.

**Core Philosophy:** Each phase delivers standalone value. Phases 0-2 are **mandatory** (stop bleeding, prevent future wounds, see current state). Phases 3-4 are **force multipliers** (learn from past, automate future).

### Dependency Chain

```
Phase 0 (Week 1): Emergency Fixes
    ↓ Hooks + VULCAN CLAUDE.md + GitHub
Phase 1 (Weeks 2-3): Enforcement Layer
    ↓ WARDEN + CI/CD + Coverage Gates
Phase 2 (Weeks 3-4): Observability
    ↓ Process Observer + Trust Dashboard + Telemetry
Phase 3 (Weeks 4-5): Learning & Memory
    ↓ MNEMIS Auto-Promotion + Cross-Session Rules
Phase 4 (Weeks 5-8): Advanced Automation
    → Auto-Tests + Rollback + MCP + Background Tasks
```

**Prioritization Rationale:**
- Phase 0 targets **PROTEUS v0.2.1 failure root causes** (no hooks, no GitHub backup, VULCAN misaligned)
- Phase 1 adds **preventive gates** (WARDEN enforcement, CI/CD, coverage minimums)
- Phase 2 delivers **visibility** (real-time agent monitoring, cost dashboard)
- Phase 3 enables **learning** (automatic memory promotion, cross-session rules)
- Phase 4 achieves **full autonomy** (auto-generated tests/docs, rollback, background monitoring)

---

### 6.1 Phase 0 — Emergency Fixes (Week 1)

**Goal:** Stop the bleeding. Prevent the exact failures that plagued PROTEUS v0.2.1.

**Success Criteria:**
- [ ] Git hooks active on all repos (pre-commit ruff/mypy/pytest)
- [ ] VULCAN has CLAUDE.md (constitutional compliance)
- [ ] All GECO components pushed to GitHub (backup + CI/CD foundation)
- [ ] ECHO version chaos cleaned (single source of truth)
- [ ] Claude Code hooks configured in settings.json

---

#### Task 0.1: Configure Git Pre-Commit Hooks (ALL REPOS)

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Install pre-commit hooks running ruff (linting), mypy (type checking), and pytest (unit tests) on every commit across _GAIA, _MYCEL, _PROTEUS, _VULCAN repos |
| **Why** | PROTEUS v0.2.1 errors could have been caught by automated linting/type checking before commit. Manual code reviews are insufficient. Hooks enforce quality gates at authoring time, not review time |
| **Files** | `.pre-commit-config.yaml` (NEW) in _GAIA, _MYCEL, _PROTEUS, _VULCAN<br>`CONTRIBUTING.md` (NEW) documenting bypass procedure |
| **Dependencies** | None (foundational) |
| **Acceptance Criteria** | ✅ Commit with missing type hint is rejected with clear error<br>✅ Commit with unused import is auto-fixed by ruff<br>✅ Commit with failing unit test is rejected with test output<br>✅ `--no-verify` bypass works and is documented<br>✅ Hooks run in <10 seconds for typical commits |

**Implementation Details:**
```yaml
# .pre-commit-config.yaml template
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.9
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]
  - repo: local
    hooks:
      - id: pytest-fast
        name: pytest (changed files only)
        entry: pytest tests/ -x --tb=short
        language: system
        pass_filenames: false
```

---

#### Task 0.2: Create VULCAN CLAUDE.md

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Write CLAUDE.md for VULCAN defining agent behavior, coding standards, and project creation protocols |
| **Why** | VULCAN generates CLAUDE.md for new projects (proven by ARGUS/LOOM/MNEMIS) but lacks its own, violating self-alignment principle. VULCAN agents don't know VULCAN conventions, risking invalid project templates |
| **Files** | `_VULCAN\CLAUDE.md` (NEW)<br>Read: `vulcan_forge\project_creator.py`, `_ARGUS\CLAUDE.md`, `_PROTEUS\CLAUDE.md` |
| **Dependencies** | Task 0.1 (VULCAN CLAUDE.md should reference hook requirements) |
| **Acceptance Criteria** | ✅ VULCAN CLAUDE.md passes `claude-md-management` MCP validation<br>✅ Contains project creation checklist (10+ items)<br>✅ Specifies 2+ agent roles with clear responsibilities<br>✅ Includes ARGUS telemetry integration requirement<br>✅ References WARDEN compliance scanning (placeholder for Phase 1)<br>✅ VULCAN agents use CLAUDE.md in next project creation |

**Key Sections to Include:**
- **Agent Roles:** `project-creator` (sonnet), `template-updater` (haiku)
- **Project Standards:** Required files (README, pyproject.toml, tests/, CLAUDE.md), directory structure (src/{module}/, tests/, docs/, examples/)
- **Integration Requirements:** ARGUS telemetry stubs, MNEMIS bridge
- **Quality Gates:** Localization check (`grep -r "[áéíóúñ]" src/` returns nothing), 100% type hints on public functions
- **CI/CD Templates:** GitHub Actions for pytest/ruff/mypy

---

#### Task 0.3: Push All GECO Components to GitHub

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Create GitHub repos for ARGUS, LOOM, MNEMIS, ECHO, WARDEN, RAVEN and push with full git history. Link MYCEL and VULCAN remotes to GitHub |
| **Why** | Currently only HART OS has GitHub remote. Local-only repos risk code loss (drive failure, accidental deletion) and block CI/CD setup. PROTEUS v0.2.1 failure had no remote backup for rollback |
| **Files** | `registry.json` (EDIT — add GitHub URLs)<br>Git remotes for 9 components (NEW/EDIT) |
| **Dependencies** | None (can run parallel with 0.1/0.2) |
| **Acceptance Criteria** | ✅ All 9 components have GitHub remotes (`git remote -v`)<br>✅ `gh repo list GAIA-Ecosystem` shows 9+ repos<br>✅ registry.json contains GitHub URLs for all components<br>✅ Branch protection enabled on all repos<br>✅ Fresh clone includes full git history |

**Implementation:**
```powershell
# Create GitHub organization: GAIA-Ecosystem
# For each component without remote:
cd X:\Projects\_GAIA\_{COMPONENT}
gh repo create GAIA-Ecosystem/{COMPONENT} --private --source=. --remote=origin
git push -u origin main

# Configure branch protection:
# - Require PR reviews (1 reviewer)
# - Require status checks (Phase 1 CI/CD)
# - No force pushes
```

---

#### Task 0.4: ECHO Version Consolidation

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Consolidate 19 ECHO version copies (ui_v0.py through ui_v012.py) into single `echo/ui.py` with git history reconstructed from file timestamps |
| **Why** | ECHO's 19 manual version copies represent total version control breakdown. Each copy is a rollback failure, creating confusion about current version. ECHO cannot be tested or deployed safely in this state |
| **Files** | `ui_v*.py` (MOVE to archive/)<br>`echo/ui.py` (NEW — consolidated)<br>`requirements.txt` (EDIT)<br>`echo/config.py` (NEW — extracted paths)<br>`archive/VERSION_NOTES.md` (NEW) |
| **Dependencies** | Task 0.3 (ECHO history backed up before consolidation) |
| **Acceptance Criteria** | ✅ Single `echo/ui.py` exists as source of truth<br>✅ Git history shows 19 commits with original timestamps<br>✅ `git log --graph --oneline` shows merge structure<br>✅ requirements.txt resolves without errors<br>✅ config.py contains all hardcoded paths<br>✅ Archive contains original files<br>✅ VERSION_NOTES.md documents API changes |

**Implementation Strategy:**
1. **Discovery:** Sort all ui_v*.py by LastWriteTime
2. **Create git branch per version:** Reconstruct chronological history
3. **Merge to main with tags:** ui_v012 becomes canonical ui.py
4. **Archive old versions:** Preserve for reference
5. **Audit dependencies:** Build complete requirements.txt from all versions

---

#### Task 0.5: Update Claude Code Hooks Config

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Configure Claude Code's post-edit and pre-commit hooks in VS Code settings.json |
| **Why** | Claude Code subagents and GAIA agents fail to follow rules. Automated hooks provide immediate feedback on violations before code reaches git pre-commit stage |
| **Files** | `C:\Users\Fede\AppData\Roaming\Code\User\settings.json` (EDIT)<br>`_GAIA\.vscode\settings.json` (NEW)<br>`~/.claude/MEMORY.md` (EDIT — mark pending complete) |
| **Dependencies** | Task 0.1 (Claude hooks align with git hooks) |
| **Acceptance Criteria** | ✅ Python edit with syntax error shows immediate error in Claude Code<br>✅ Agent staging file with type error receives feedback before commit<br>✅ Hooks run in <5 seconds<br>✅ MEMORY.md updated to remove "pending" status<br>✅ Workspace settings propagate to all GAIA projects |

**Configuration:**
```json
"claude.hooks": {
  "post-edit": {
    "python": "python -m py_compile ${file} && echo '✓ Syntax valid'"
  },
  "pre-commit": {
    "python": "ruff check ${file} --fix && mypy ${file}"
  }
}
```

---

### 6.2 Phase 1 — Enforcement Layer (Weeks 2-3)

**Goal:** Prevent violations before they reach production.

**Success Criteria:**
- [ ] WARDEN integrated into all workflows (hooks, CI/CD, agent edits)
- [ ] GitHub Actions running on all repos (pytest, ruff, mypy, WARDEN scan)
- [ ] 60%+ test coverage minimum enforced (gates fail below threshold)
- [ ] Zero commits to main without passing CI

---

#### Task 1.1: WARDEN Full Implementation

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Expand WARDEN from standalone scanner to integrated compliance system with git hooks, CI/CD, and agent edit validation |
| **Why** | WARDEN currently exists but doesn't run automatically. Compliance violations only discovered in production (or never). WARDEN must become a gatekeeper, not an auditor |
| **Files** | `warden/scanner.py` (EDIT — expand capabilities)<br>`warden/cli.py` (NEW)<br>`warden/hooks.py` (NEW)<br>`.github/workflows/warden.yml` (NEW — all repos) |
| **Dependencies** | Phase 0 complete (hooks infrastructure exists) |
| **Acceptance Criteria** | ✅ CLAUDE.md validation (format, agents defined)<br>✅ Test coverage check (60%+ threshold)<br>✅ Type hint coverage (100% public functions)<br>✅ Localization check (no Spanish in Python)<br>✅ Dependency security scan (pip-audit)<br>✅ Docstring coverage (public functions)<br>✅ CLI returns exit code 1 on violations<br>✅ CI fails on WARDEN violations |

**Scanner Capabilities to Add:**
- CLAUDE.md validation (all modules have one, format correct, agents defined)
- Test coverage check (pytest-cov with 60% threshold)
- Type hint coverage (mypy with `--disallow-untyped-defs` on public APIs)
- Localization check (`grep -r "[áéíóúñ]" src/` returns nothing)
- Dependency security scan (known vulnerabilities via `pip-audit`)
- Docstring coverage (public functions only)

---

#### Task 1.2: GitHub Actions CI/CD for All Modules

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Deploy GitHub Actions workflows to all 9 GAIA modules (ARGUS, LOOM, MNEMIS, ECHO, WARDEN, RAVEN, MYCEL, VULCAN, PROTEUS) |
| **Why** | Zero repos have CI/CD. Tests exist but aren't enforced. Phase 0 hooks prevent bad commits but don't prevent bad merges or direct pushes |
| **Files** | `.github/workflows/ci.yml` (NEW — all 9 repos)<br>`.github/workflows/security.yml` (NEW — dependency scanning)<br>`pyproject.toml` (EDIT — add pytest-cov config) |
| **Dependencies** | Task 0.3 (GitHub remotes exist)<br>Task 1.1 (WARDEN scanner available) |
| **Acceptance Criteria** | ✅ All 9 repos have CI workflow<br>✅ CI runs on push to main and all PRs<br>✅ CI runs: pytest, ruff, mypy, WARDEN scan<br>✅ Branch protection requires passing CI<br>✅ CI badge in README shows status<br>✅ Failed CI blocks merge with clear error message |

**Workflow Template:**
```yaml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - run: pip install -r requirements.txt
      - run: ruff check .
      - run: mypy src/
      - run: pytest --cov=src --cov-report=term --cov-fail-under=60
      - run: warden scan --strict
```

---

#### Task 1.3: Pre-Commit Hooks Enforcement + Testing

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Upgrade Phase 0 hooks to enforce minimum test coverage and run WARDEN checks on staged files |
| **Why** | Phase 0 hooks catch syntax/style but not semantic issues. Coverage drops and CLAUDE.md violations slip through |
| **Files** | `.pre-commit-config.yaml` (EDIT — add coverage + WARDEN hooks)<br>`scripts/pre-commit-coverage.sh` (NEW) |
| **Dependencies** | Task 0.1 (base hooks exist)<br>Task 1.1 (WARDEN available) |
| **Acceptance Criteria** | ✅ Commit reducing coverage below 60% is rejected<br>✅ Commit with CLAUDE.md violations is rejected<br>✅ Hook output shows which file/line caused failure<br>✅ Hooks complete in <15 seconds for typical commits |

---

#### Task 1.4: Coverage Gates (60% Phase 1, 80% Phase 2)

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Configure pytest-cov to enforce minimum coverage thresholds: 60% for Phase 1, 80% for Phase 2 |
| **Why** | Current coverage is variable (PROTEUS 85%+, others unknown). No enforcement means coverage can silently degrade |
| **Files** | `pyproject.toml` (EDIT — add pytest coverage config)<br>`pytest.ini` (EDIT — fail-under thresholds) |
| **Dependencies** | Task 1.2 (CI configured) |
| **Acceptance Criteria** | ✅ `pytest` fails if coverage <60% in Phase 1<br>✅ CI reports coverage percentage in logs<br>✅ Coverage report excludes test files themselves<br>✅ Per-module coverage visible in CI output<br>✅ Phase 2 upgrade path documented (increase to 80%) |

**Configuration:**
```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = "--cov=src --cov-report=term-missing --cov-fail-under=60"

[tool.coverage.run]
source = ["src"]
omit = ["*/tests/*", "*/test_*.py"]
```

---

### 6.3 Phase 2 — Observability (Weeks 3-4)

**Goal:** See everything in real-time.

**Success Criteria:**
- [ ] ARGUS Process Observer tracks all Claude Code agent activity
- [ ] Trust Dashboard displays cost, compliance, agent health in real-time
- [ ] Universal telemetry: all 9 modules send events to ARGUS
- [ ] Real-time subagent activity view operational

---

#### Task 2.1: ARGUS Process Observer Implementation

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Build Process Observer to monitor Claude Code agent activity: file edits, tool calls, token usage, errors |
| **Why** | Claude Code agents fail silently. Process Observer surfaces failures immediately and logs context for MNEMIS auto-promotion |
| **Files** | `argus/process_observer.py` (NEW)<br>`argus/event_bus.py` (EDIT — add process events)<br>`.vscode/tasks.json` (NEW — launch observer with Claude Code) |
| **Dependencies** | Phase 1 complete (telemetry infrastructure) |
| **Acceptance Criteria** | ✅ Detects Claude Code agent start/stop<br>✅ Logs every file edit (before/after diff)<br>✅ Captures tool calls (bash, read, write, edit)<br>✅ Tracks token usage per operation<br>✅ Surfaces errors with full stack traces<br>✅ Real-time view in ARGUS UI<br>✅ Observer runs in background (<5% CPU overhead) |

**Technical Approach:**
- Hook into VS Code Extension API (if available)
- Monitor filesystem for changes in workspace
- Parse Claude Code logs (`~/.claude/logs/`)
- WebSocket connection to ARGUS UI for real-time updates

---

#### Task 2.2: Trust Dashboard (Cost + Compliance + Agent Health)

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Create centralized dashboard in ARGUS showing: LLM cost (total + per-project), constitutional compliance score, agent health status |
| **Why** | Currently: cost is invisible until monthly bill arrives, compliance is only audited manually, agent failures are silent. Trust Dashboard makes GAIA trustworthy by showing real-time state |
| **Files** | `argus/dashboard/trust_view.py` (NEW)<br>`argus/metrics/cost_aggregator.py` (NEW)<br>`argus/metrics/compliance_score.py` (NEW) |
| **Dependencies** | Task 2.1 (telemetry available)<br>Task 1.1 (WARDEN scores) |
| **Acceptance Criteria** | ✅ Displays total LLM cost (current month)<br>✅ Cost breakdown by project/provider<br>✅ Constitutional compliance score (0-100)<br>✅ Agent health: active, errored, idle<br>✅ Real-time updates (no page refresh)<br>✅ Drill-down to individual violations<br>✅ Cost alerts when crossing thresholds |

**Dashboard Metrics:**
- **Cost:** Total spend (month-to-date), cost by project, cost by provider (OpenAI/Anthropic/Gemini), tokens consumed
- **Compliance:** WARDEN score (0-100), violations by category (P0/P1/P2), coverage percentage, test pass rate
- **Agent Health:** Active agents, errored agents, average task completion time, failure rate

---

#### Task 2.3: Universal Telemetry (All 9 Modules → ARGUS)

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Implement telemetry in 8 remaining modules (currently only PROTEUS sends telemetry) using PROTEUS pattern as template |
| **Why** | ARGUS is blind to 8/9 modules. Cannot track cost, detect failures, or learn from outcomes without telemetry |
| **Files** | `{module}/telemetry.py` (NEW — 8 modules)<br>`argus/event_bus.py` (EDIT — register new event types) |
| **Dependencies** | Task 2.1 (event bus accepts events) |
| **Acceptance Criteria** | ✅ All 9 modules send telemetry to ARGUS<br>✅ Telemetry includes: operation name, duration, cost, outcome (success/error), context (user input, file paths)<br>✅ No performance impact (async telemetry)<br>✅ Telemetry failure doesn't break module<br>✅ Trust Dashboard shows events from all modules |

**PROTEUS Telemetry Pattern (to replicate):**
```python
from argus.event_bus import EventBus

def emit_event(event_type: str, data: dict):
    try:
        EventBus.emit(event_type, {
            "module": "MODULE_NAME",
            "timestamp": datetime.now().isoformat(),
            "data": data
        })
    except Exception:
        pass  # Telemetry failure is non-fatal
```

---

#### Task 2.4: Real-Time Subagent Activity View

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Add real-time view in ARGUS UI showing: active subagents, current task, progress, token usage, errors |
| **Why** | Long-running Claude Code tasks appear frozen. Real-time view shows what agents are doing and surfaces stuck/errored agents |
| **Files** | `argus/ui/subagent_monitor.py` (NEW)<br>`argus/process_observer.py` (EDIT — stream activity events) |
| **Dependencies** | Task 2.1 (Process Observer operational) |
| **Acceptance Criteria** | ✅ Shows active subagents with role (explain-only, debug-explorer, etc.)<br>✅ Displays current task/file being edited<br>✅ Progress indicator (tokens used, time elapsed)<br>✅ Error highlighting with stack trace<br>✅ Auto-refresh every 2 seconds<br>✅ Notification when agent errors or completes |

---

### 6.4 Phase 3 — Learning & Memory (Weeks 4-5)

**Goal:** System learns from past mistakes.

**Success Criteria:**
- [ ] MNEMIS auto-promotion pipeline operational (ARGUS errors → prevention rules)
- [ ] Cross-session enforcement of learned rules
- [ ] URL/reference knowledge base populated from PROTEUS/VIA/HART OS
- [ ] Mental model outcome tracking (which models led to success/failure)

---

#### Task 3.1: MNEMIS Auto-Promotion Pipeline

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Implement automatic promotion of ARGUS errors into MNEMIS prevention rules using promotion protocol |
| **Why** | Currently errors are logged but never become rules. Same mistakes repeat. Auto-promotion closes learning loop |
| **Files** | `mnemis/auto_promotion.py` (NEW)<br>`mnemis/promotion_pipeline.py` (EDIT)<br>`argus/error_handler.py` (EDIT — trigger promotion) |
| **Dependencies** | Task 2.3 (universal telemetry available) |
| **Acceptance Criteria** | ✅ Error occurring 3+ times triggers promotion review<br>✅ Promotion candidate includes: error pattern, context, suggested prevention rule<br>✅ Human-in-the-loop approval (auto-promotion with manual gate)<br>✅ Approved rule propagates to all modules<br>✅ Rule format: trigger condition + prevention action<br>✅ Audit log of promotions (what/when/why) |

**Promotion Criteria:**
- **Frequency:** Error occurs 3+ times in 7 days
- **Impact:** Error blocks workflow or causes data loss
- **Preventability:** Clear prevention rule can be defined
- **Scope:** Rule applies to multiple projects (not one-off)

---

#### Task 3.2: Cross-Session Enforcement

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Enforce MNEMIS-promoted rules across all Claude Code sessions and GAIA agents |
| **Why** | Rules exist but aren't enforced. Agents forget lessons between sessions. Cross-session enforcement makes learning persistent |
| **Files** | `mnemis/enforcement_engine.py` (NEW)<br>`.vscode/settings.json` (EDIT — load MNEMIS rules on startup)<br>`claude_code_hook.py` (NEW — intercept agent actions) |
| **Dependencies** | Task 3.1 (rules exist to enforce) |
| **Acceptance Criteria** | ✅ MNEMIS rules loaded on Claude Code startup<br>✅ Agent action triggering rule violation is blocked<br>✅ Clear error message explaining violation + rule<br>✅ Override mechanism for false positives (--force flag)<br>✅ Enforcement applies to all GAIA agents (not just Claude Code)<br>✅ Rule updates propagate without restart |

---

#### Task 3.3: URL/Reference Knowledge Base

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Extract all URLs/references from PROTEUS, VIA, HART OS into centralized MNEMIS knowledge base with context |
| **Why** | Products repeatedly re-fetch same URLs, losing context about reliability/relevance. Centralized KB enables citation validation and reference reuse |
| **Files** | `mnemis/knowledge_base/url_store.py` (NEW)<br>`mnemis/extractors/proteus_urls.py` (NEW)<br>`mnemis/extractors/via_citations.py` (NEW) |
| **Dependencies** | None (independent of other Phase 3 tasks) |
| **Acceptance Criteria** | ✅ Extracts all URLs from PROTEUS job postings<br>✅ Extracts all citations from VIA research reports<br>✅ Extracts all canonical sources from HART OS<br>✅ Stores: URL, context, reliability score, last accessed<br>✅ Deduplication (same URL from multiple sources)<br>✅ API for querying: "Have we seen this URL before?"<br>✅ Periodic refresh (re-validate URLs still active) |

---

#### Task 3.4: Mental Model Outcome Tracking

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Track which mental models were invoked and correlate with task success/failure |
| **Why** | ARGUS has 59 mental models but no feedback loop. Can't know which models are useful vs. noise |
| **Files** | `argus/mental_models/outcome_tracker.py` (NEW)<br>`argus/mental_models/selector.py` (EDIT — log invocations)<br>`mnemis/analysis/model_effectiveness.py` (NEW) |
| **Dependencies** | Task 2.3 (telemetry for outcomes) |
| **Acceptance Criteria** | ✅ Logs every mental model invocation (which model, context, timestamp)<br>✅ Correlates invocation with task outcome (success/failure/abandoned)<br>✅ Effectiveness score per model (success rate)<br>✅ Recommendation: promote high-value models, demote low-value<br>✅ Periodic report: most/least useful models<br>✅ A/B testing capability (randomly omit model, measure impact) |

---

### 6.5 Phase 4 — Advanced Automation (Weeks 5-8)

**Goal:** Full autonomy. System maintains itself.

**Success Criteria:**
- [ ] Auto-generated tests for new code (coverage never drops)
- [ ] Auto-generated documentation (MkDocs site always up-to-date)
- [ ] Git rollback capability (known-good tagging + auto-rollback on failures)
- [ ] MCP tool registration for GAIA modules (discoverability)
- [ ] Skill auto-discovery operational
- [ ] Background task infrastructure (async monitoring, scheduled scans)
- [ ] Progressive disclosure / token management (LLM context optimization)

---

#### Task 4.1: Auto-Generated Tests

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Generate unit tests automatically for new code using LLM-powered test generation |
| **Why** | Test writing is slowest part of development. Auto-generation ensures coverage never drops below threshold |
| **Files** | `warden/test_generator.py` (NEW)<br>`.github/workflows/auto-test.yml` (NEW)<br>`scripts/generate_tests.py` (NEW) |
| **Dependencies** | Phase 1 complete (test infrastructure exists) |
| **Acceptance Criteria** | ✅ Detects new functions without tests<br>✅ Generates tests using LLM (Claude Sonnet)<br>✅ Tests cover: happy path, edge cases, error handling<br>✅ Generated tests pass on first run (90%+ success rate)<br>✅ Human review before merge (auto-generated tests in PR)<br>✅ Coverage maintained at 80%+ without manual test writing |

**Technical Approach:**
- Parse AST to find new functions
- Extract function signature + docstring as context
- Prompt LLM: "Generate pytest tests for this function covering happy path, edge cases, and error handling"
- Write tests to `tests/auto_generated/test_{module}.py`
- Run tests, if failures → refine prompt and retry
- Create PR with generated tests for human review

---

#### Task 4.2: Auto-Generated Documentation (MkDocs)

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Generate MkDocs site from docstrings + CLAUDE.md + README files, auto-deploy on push |
| **Why** | Documentation is always stale. Auto-generation from code ensures docs match implementation |
| **Files** | `mkdocs.yml` (NEW — all modules)<br>`.github/workflows/docs.yml` (NEW — build + deploy)<br>`scripts/generate_docs.py` (NEW) |
| **Dependencies** | Phase 1 complete (CI infrastructure) |
| **Acceptance Criteria** | ✅ MkDocs site generated from docstrings<br>✅ API reference auto-generated from code<br>✅ CLAUDE.md content included in docs<br>✅ Deployment on push to main<br>✅ Versioned docs (separate site per release)<br>✅ Search functional<br>✅ Site accessible at `gaia-ecosystem.github.io/{MODULE}` |

---

#### Task 4.3: Git Rollback Capability (Known-Good Tagging)

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Automatically tag commits passing all CI checks as "known-good", enable one-command rollback on failures |
| **Why** | PROTEUS v0.2.1 failure had no easy rollback. Manual rollback requires finding last working commit, testing, etc. Auto-tagging enables instant recovery |
| **Files** | `.github/workflows/tag-known-good.yml` (NEW)<br>`scripts/rollback.sh` (NEW)<br>`.github/workflows/auto-rollback.yml` (NEW — on CI failure) |
| **Dependencies** | Task 1.2 (CI operational) |
| **Acceptance Criteria** | ✅ Commits passing CI are auto-tagged `known-good-{timestamp}`<br>✅ `rollback.sh` reverts to last known-good commit<br>✅ Rollback preserves uncommitted work (stash)<br>✅ Rollback notification (ARGUS + email)<br>✅ Optional: auto-rollback on CI failure (with confirmation)<br>✅ Tag retention policy (keep 10 most recent known-good tags) |

**Workflow:**
```yaml
# .github/workflows/tag-known-good.yml
on:
  push:
    branches: [main]
jobs:
  tag-if-passing:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: pytest && ruff check && mypy src/
      - run: git tag known-good-$(date +%Y%m%d-%H%M%S)
      - run: git push --tags
```

---

#### Task 4.4: MCP Tool Registration for GAIA Modules

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Register GAIA modules (ARGUS, LOOM, MNEMIS, WARDEN, VULCAN) as MCP servers for Claude Code tool discoverability |
| **Why** | Claude Code can't discover GAIA tools without MCP registration. Agents must know tool names/capabilities manually |
| **Files** | `{module}/mcp_server.py` (NEW — 5 modules)<br>`mcp_registry.json` (NEW)<br>`.claude/mcp_config.json` (EDIT) |
| **Dependencies** | None (independent feature) |
| **Acceptance Criteria** | ✅ 5 modules registered as MCP servers<br>✅ Claude Code discovers tools automatically<br>✅ Tool descriptions accurate (no hallucination)<br>✅ Tool invocation works from Claude Code<br>✅ MCP registry browsable in ARGUS UI<br>✅ Hot-reload when tools change (no restart) |

**MCP Server Template:**
```python
# {module}/mcp_server.py
from mcp import MCPServer

server = MCPServer(name="{MODULE}", version="0.1.0")

@server.tool("scan_compliance")
async def scan_compliance(path: str) -> dict:
    """Run WARDEN compliance scan on the given path."""
    # Implementation
```

---

#### Task 4.5: Skill Auto-Discovery

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Automatically discover agent skills from CLAUDE.md files and make them available in skill registry |
| **Why** | Skills exist (explain-only, debug-explorer, etc.) but must be manually registered. Auto-discovery enables skill sharing across projects |
| **Files** | `argus/skills/discovery.py` (NEW)<br>`argus/skills/registry.py` (EDIT)<br>`.claude/skills/` (NEW — skill definitions) |
| **Dependencies** | None (independent feature) |
| **Acceptance Criteria** | ✅ Scans all CLAUDE.md files for skill definitions<br>✅ Extracts: skill name, model, prompt, constraints<br>✅ Registers in skill registry (browsable in ARGUS UI)<br>✅ Claude Code can invoke registered skills<br>✅ Skills work across projects (portable)<br>✅ Skill versioning (track changes over time) |

---

#### Task 4.6: Background Task Infrastructure

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Build task runner for async operations: scheduled compliance scans, cost report generation, telemetry aggregation |
| **Why** | All operations are synchronous/on-demand. Background tasks enable proactive monitoring and scheduled maintenance |
| **Files** | `argus/task_runner.py` (NEW)<br>`argus/tasks/scheduled_scan.py` (NEW)<br>`argus/tasks/cost_report.py` (NEW) |
| **Dependencies** | Task 2.1 (telemetry infrastructure) |
| **Acceptance Criteria** | ✅ Task runner starts on ARGUS launch<br>✅ Scheduled tasks: daily compliance scan, weekly cost report, hourly telemetry aggregation<br>✅ Task failures logged and alerted<br>✅ Task history browsable in ARGUS UI<br>✅ Manual task triggering (on-demand scan)<br>✅ Task priority/queuing (critical tasks first) |

---

#### Task 4.7: Progressive Disclosure / Token Management

| **Attribute** | **Details** |
|--------------|-------------|
| **What** | Implement context window management: summarize old context, load only relevant files, progressive detail expansion |
| **Why** | Large projects exceed LLM context window. Token overflow causes failures or truncated responses |
| **Files** | `mycel/context_manager.py` (NEW)<br>`mycel/summarizer.py` (NEW)<br>`argus/token_tracker.py` (EDIT — warn on high usage) |
| **Dependencies** | Task 2.3 (token telemetry available) |
| **Acceptance Criteria** | ✅ Detects context window approaching limit (80% full)<br>✅ Summarizes old messages (first 20% of conversation)<br>✅ Loads files on-demand (not all upfront)<br>✅ Progressive detail: show summary → user requests full detail<br>✅ Token budget per operation (warn if exceeded)<br>✅ Context compression (remove redundant information) |

---

### 6.6 Dependency Map

The following diagram shows the critical path and dependencies between phases:

```
PHASE 0 (Week 1) - EMERGENCY FIXES
├─ 0.1 Pre-Commit Hooks ────────────────┐
├─ 0.2 VULCAN CLAUDE.md ─────────────┐  │
├─ 0.3 GitHub Push (ALL REPOS) ──────┤  │
├─ 0.4 ECHO Consolidation ───────────┘  │
└─ 0.5 Claude Code Hooks ───────────────┘
            │
            ↓
PHASE 1 (Weeks 2-3) - ENFORCEMENT
├─ 1.1 WARDEN Full Implementation ──────┐
├─ 1.2 GitHub Actions CI/CD ────────────┤
├─ 1.3 Pre-Commit Hooks + Testing ──────┤
└─ 1.4 Coverage Gates (60% → 80%) ──────┘
            │
            ↓
PHASE 2 (Weeks 3-4) - OBSERVABILITY
├─ 2.1 Process Observer ────────────────┐
├─ 2.2 Trust Dashboard ─────────────────┤
├─ 2.3 Universal Telemetry ─────────────┤
└─ 2.4 Real-Time Subagent View ─────────┘
            │
            ↓
PHASE 3 (Weeks 4-5) - LEARNING
├─ 3.1 MNEMIS Auto-Promotion ───────────┐
├─ 3.2 Cross-Session Enforcement ───────┤
├─ 3.3 URL/Reference Knowledge Base ────┤
└─ 3.4 Mental Model Outcome Tracking ───┘
            │
            ↓
PHASE 4 (Weeks 5-8) - AUTOMATION
├─ 4.1 Auto-Generated Tests ────────────┐
├─ 4.2 Auto-Generated Docs (MkDocs) ────┤
├─ 4.3 Git Rollback (Known-Good Tags) ──┤
├─ 4.4 MCP Tool Registration ───────────┤
├─ 4.5 Skill Auto-Discovery ────────────┤
├─ 4.6 Background Task Infrastructure ──┤
└─ 4.7 Progressive Disclosure/Tokens ───┘
```

**Critical Path:** 0.1 → 0.3 → 1.2 → 2.1 → 3.1 → 4.1

**Parallel Workstreams:**
- **Infrastructure:** Phases 0-1 (hooks, CI/CD, GitHub)
- **Visibility:** Phase 2 (telemetry, dashboards, monitoring)
- **Learning:** Phase 3 (memory, knowledge base, outcomes)
- **Autonomy:** Phase 4 (auto-generation, rollback, discovery)

---

### 6.7 Master Cleanup Roadmap

**Objective:** Consolidate all projects under GAIA governance, clean folder structure, eliminate orphans.

**Phase 1: Rename & Reclassify**

1. **Rename PROTEUS → jSeeker**
   - Update product name in all documentation
   - Update imports and module references
   - Update CLAUDE.md with new name
   - Update registry.json entry

2. **Move jSeeker from `_GAIA/_PROTEUS/` to `X:\Projects\jSeeker/`**
   - Create new directory structure
   - Move all source files preserving git history
   - Update absolute paths in code (config paths, log paths)
   - Test import paths and startup scripts

3. **Update all internal references**
   - Search codebase for "PROTEUS" string occurrences
   - Update imports: `from proteus.` → `from jseeker.`
   - Update config files and environment variables
   - Update ARGUS telemetry component names

4. **Update registry.json**
   - Change path from `X:\Projects\_GAIA\_PROTEUS` to `X:\Projects\jSeeker`
   - Update component name: `PROTEUS` → `jSeeker`
   - Add `formerly: PROTEUS` metadata
   - Update tags to include `product` (not shared-service)

**Phase 2: Consolidate**

5. **Move ABIS docs to `_GAIA/_ABIS/`**
   - Create `X:\Projects\_GAIA\_ABIS\` directory
   - Move ABIS planning documents from scattered locations
   - Create initial CLAUDE.md for ABIS
   - Add placeholder README.md with status: planning

6. **Create DOS project folder at `X:\Projects\DOS/`**
   - Use VULCAN to scaffold DOS project structure
   - Set up as GAIA-compliant product (not under `_GAIA/`)
   - Add initial architecture documentation
   - Register in registry.json with status: planning

7. **Move RAG Intelligence root docs to MYCEL**
   - Consolidate scattered RAG documentation
   - Move to `X:\Projects\_GAIA\_MYCEL\docs\`
   - Update cross-references in other modules
   - Archive outdated RAG docs

8. **Merge ChatGPT exports into ECHO**
   - Consolidate ChatGPT conversation exports
   - Store in `X:\Projects\_GAIA\_ECHO\data\exports\`
   - Document export format and parsing logic
   - Create migration guide if ECHO is rescued

**Phase 3: Clean**

9. **Create `_archive/` for deprecated items**
   - Create `X:\Projects\_archive\` directory
   - Move deprecated code, old prototypes, failed experiments
   - Add ARCHIVED.md to each archived item with reason and date
   - Update registry.json with archived status

10. **Create `_reference/` for non-project materials**
    - Create `X:\Projects\_reference\` directory
    - Move tutorials, learning materials, external docs
    - Move research papers, API documentation copies
    - Organize by topic (ai-ml, python, architecture, etc.)

11. **Remove stale symlinks and virtual environments**
    - Scan for broken symlinks: `find X:\Projects\ -type l ! -exec test -e {} \; -print`
    - Remove orphaned virtual environments (`.venv`, `venv`, `env`)
    - Document removal in cleanup log
    - Re-create necessary symlinks with correct targets

12. **Update registry.json with final paths**
    - Verify all paths in registry.json resolve
    - Add `migrated_from` metadata for moved projects
    - Update status fields (active, planning, archived, stale)
    - Add cleanup completion timestamp

**Phase 4: Verify**

13. **All registry.json paths resolve to existing directories**
    - Script to validate all paths: `python validate_registry.py`
    - Fix any broken paths or remove stale entries
    - Generate validation report: paths.md

14. **Import smoke tests pass for all products**
    - Test: `python -c "import hart_os; import via; import jseeker"`
    - Test: `python -c "from rag_intelligence import create_llm_client"`
    - Fix import errors, update PYTHONPATH if needed
    - Document any known import limitations

15. **Streamlit apps launch successfully**
    - Test each app: `streamlit run ui/main.py` (or equivalent)
    - Verify port assignments (no conflicts)
    - Test ARGUS dashboard co-launch with jSeeker
    - Document startup commands in README.md

16. **Test suites pass**
    - Run pytest on all modules: `pytest X:\Projects\_GAIA\`
    - Target: 80%+ coverage maintained or improved
    - Fix broken tests from path changes
    - Update test fixtures with new paths

17. **Cross-reference integrity (Bible, PRD, registry match disk)**
    - Verify GAIA_BIBLE.md references match registry.json
    - Verify GAIA_PRD.md product catalog matches disk structure
    - Verify CLAUDE.md files reference correct paths
    - Generate cross-reference report: integrity.md

**Success Criteria:**
- [ ] Zero broken paths in registry.json
- [ ] All products importable without errors
- [ ] All Streamlit apps launch without errors
- [ ] Test suites pass with 80%+ coverage maintained
- [ ] GAIA_BIBLE.md, GAIA_PRD.md, registry.json in sync with disk

**Timeline:** 2-3 weeks (can overlap with Phase 0-1 of main roadmap)

**Risks:**
- Breaking import paths during moves (mitigation: test imports after each move)
- Git history loss during file moves (mitigation: use `git mv` not manual copy)
- Stale documentation after cleanup (mitigation: update docs in same commit as moves)

---

### 6.8 Path to 100% GAIA Enforcement

**Current State (Feb 2026):** 10% enforcement

**Phase 1: Foundation (ENG Tasks P0) — Target: 60%**
- WARDEN CLI with pre-commit hooks (ENG-001)
- CI/CD on MYCEL + jSeeker (ENG-002)
- Pre-commit framework ecosystem-wide (ENG-009)
- VULCAN CLAUDE.md (ENG-010)
- GitHub Organization + remotes (ENG-014)

**Phase 2: Observability (ENG Tasks P1) — Target: 75%**
- ArgusClient in MYCEL (ENG-004)
- Trust Dashboard (ENG-005)
- Process Observer (ENG-006)
- Cost tracking + budget alerts (ENG-008)
- Run Record schema (ENG-016)

**Phase 3: Integration (ENG Tasks P2) — Target: 85%**
- GPT_ECHO consolidation (ENG-003)
- WARDEN-LOOM integration (ENG-011)
- LOOM change sets + rollback (ENG-012)
- MYCEL migration for all products (ENG-013)
- MNEMIS task store (ENG-017)

**Phase 4: Universal Coverage — Target: 95%**
- All 8 products emit ARGUS telemetry
- MNEMIS auto-promotion from ARGUS patterns
- WARDEN Phase 2: runtime enforcement (not just pre-commit)
- Process Observer live monitoring on all agents
- Branch protection on all GitHub repos

**Phase 5: Self-Healing — Target: 100%**
- ARGUS auto-detects regressions and suggests fixes
- MNEMIS cross-project learning fully automated
- WARDEN auto-blocks violations with clear remediation
- RAVEN proactive research (regulatory changes, dependency updates)
- Meta-Observer continuous quality monitoring

**Timeline:** 60% by March 2026, 85% by May 2026, 100% by Q3 2026

---

## Section 7: Success Criteria

### 7.1 Platform Success Criteria (GAIA Ecosystem)

**P0 (Critical — Must Have):**

- [ ] **100% WARDEN Compliance:** All VULCAN-created projects pass WARDEN compliance scan on first run without manual fixes
- [ ] **Constitutional Alignment:** 100% of projects have CLAUDE.md with constitutional constraints, agent roles defined, and compliance rules documented
- [ ] **CI/CD Enforcement:** CI/CD gates prevent merging code with failing tests (enforcement rate increases from 0% → 100%)
- [ ] **Pre-Commit Hooks:** Pre-commit hooks active on all 9 GAIA modules (currently 0 → target 9 modules)
- [ ] **GitHub Backup:** All 9 modules have GitHub remotes with full git history (currently 1/9 → target 9/9)
- [ ] **Zero Secrets in Code:** Zero hardcoded API keys or secrets in any codebase (detected via WARDEN scan)

**P1 (High — Should Have):**

- [ ] **Universal Telemetry:** All 9 modules send telemetry to ARGUS event bus (currently 1/9 → target 9/9)
- [ ] **Auto-Promotion Pipeline:** MNEMIS auto-promotes errors into prevention rules without manual intervention (manual → automatic)
- [ ] **Trust Dashboard Operational:** Trust Dashboard shows real-time constitutional compliance metrics, cost breakdown, and agent health
- [ ] **Test Coverage Minimum:** 80% test coverage maintained across all modules (enforced via CI gates)
- [ ] **Process Observer Live:** ARGUS Process Observer tracks all Claude Code agent activity in real-time with <5% CPU overhead

**P2 (Medium — Nice to Have):**

- [ ] **Cross-Project Memory:** Cross-project memory sharing adopted by 3+ products (jSeeker, VIA, HART OS)
- [ ] **Mental Model Feedback Loop:** Mental Model Library invoked in 50%+ of ARGUS queries with outcome tracking operational
- [ ] **Skill Registry:** Skill registry with auto-discovery operational, 10+ skills registered and usable across projects
- [ ] **Background Monitoring:** Background task infrastructure running scheduled scans (daily compliance, weekly cost reports)
- [ ] **MCP Integration:** 5+ GAIA modules registered as MCP servers, discoverable by Claude Code

---

### 7.2 Product Success Criteria

#### HART OS (Therapy Scoring System)

**Status:** Production | Version: 6.2.8

- [ ] **Performance:** Generate Session completes in < 3 seconds (deterministic pipeline only, excludes LLM calls)
- [ ] **Quality:** 90% of generated Guías accepted without modification (user-reported satisfaction metric)
- [ ] **Traceability:** 100% of recommendations traceable to canonical sources (citation validation)
- [ ] **Localization:** Zero Spanish strings in Python code (localization rule enforced via WARDEN scan)
- [ ] **CI/CD:** CI passing on all commits with 80%+ test coverage maintained

**Timeline:** Achieve all criteria by end of Phase 1 (Week 3)

---

#### VIA (Investment Intelligence)

**Status:** Production | Version: 6.4

- [ ] **Citation Accuracy:** Multi-source synthesis with 95%+ citation accuracy (validated against source documents)
- [ ] **Time Savings:** 50% time savings vs. manual research (user-reported via time tracking)
- [ ] **Cross-Provider Agreement:** Cross-provider agreement >= 80% (majority consensus on investment recommendations)
- [ ] **MYCEL Integration:** MYCEL integration complete for all LLM calls (currently mixed OpenAI direct + MYCEL)
- [ ] **Test Coverage:** 80%+ test coverage on core synthesis logic

**Timeline:** Achieve all criteria by end of Phase 2 (Week 4)

---

#### DATA FORGE (Data Processing Engine)

**Status:** Production | Version: 1.1

- [ ] **Pipeline Generation Speed:** Pipeline generation completes in < 30 seconds for typical dataset (CSV/JSON/Excel)
- [ ] **Validation Rate:** 90% of generated pipelines pass validation without manual fixes
- [ ] **MYCEL Migration:** MYCEL migration complete (currently standalone OpenAI integration)
- [ ] **Documentation:** Complete API documentation for all data processing functions
- [ ] **Test Coverage:** 80%+ coverage on pipeline generator and validators

**Timeline:** Achieve all criteria by end of Phase 2 (Week 4)

---

#### jSeeker (formerly PROTEUS) — Resume Engine

**Status:** Active | Version: 0.2.1

- [ ] **Speed:** Resume generation completes in < 1 minute per application (currently ~2 minutes)
- [ ] **ATS Optimization:** ATS score >= 85/100 for target platform (Applicant Tracking System compatibility)
- [ ] **Application-to-Interview Rate:** Application-to-interview rate increases from 0% to 2-5% (tracked in MNEMIS)
- [ ] **Feedback Loop:** 100% of application outcomes stored in MNEMIS (outcome tracking operational)
- [ ] **Stability:** Zero undetected errors for 30 consecutive days (vs. v0.2.1 failure after 7 days)

**Timeline:** Achieve all criteria by end of Phase 3 (Week 5)

---

#### ECHO (Chat Archaeology)

**Status:** Stale (19 manual version copies)

- [ ] **Rescue/Retire Decision:** Decision made within 1 sprint (rescue or retire, no mixed state)
- [ ] **If Rescued:** Consolidate to single version with full git history, 80% classification accuracy for chat taxonomy
- [ ] **If Retired:** Learnings documented in post-mortem, module archived with clear deprecation notice, code samples preserved for reference

**Timeline:** Decision by end of Phase 0 (Week 1), implementation by end of Phase 1 (Week 3)

---

### 7.3 Quantitative Targets (6-Month)

| Metric | Current | 3-Month Target | 6-Month Target | Measurement Method |
|--------|---------|----------------|----------------|-------------------|
| **Test Coverage** | Variable (jSeeker 67%+, others unknown) | 60% minimum (all modules) | 80% minimum (all modules) | pytest-cov in CI |
| **Time to Create Project** | 2-4 hours (manual) | < 30 minutes | < 10 minutes | VULCAN telemetry |
| **Documentation Completeness** | ~40% (README only) | 70% (API docs + README) | 95% (full MkDocs site) | WARDEN doc scan |
| **CI/CD Adoption** | 0/9 modules | 9/9 modules | 9/9 modules + auto-rollback | GitHub Actions status |
| **Telemetry Coverage** | 1/9 modules (jSeeker only) | 5/9 modules | 9/9 modules | ARGUS event bus |
| **Cost Visibility** | 0% (monthly surprise bill) | 100% (daily dashboard) | 100% + predictive alerts | Trust Dashboard |
| **Constitutional Compliance** | ~70% documented, 10% enforced | 90% documented, 60% enforced | 100% documented, 90% enforced | WARDEN compliance score |
| **Agent Failure Detection Time** | Hours to days (manual discovery) | < 5 minutes (telemetry alerts) | < 30 seconds (real-time monitoring) | Process Observer |
| **Memory Promotion Latency** | Manual (never) | 48 hours (auto-promotion + review) | < 24 hours (auto-promotion + review) | MNEMIS promotion log |
| **Known-Good Rollback Time** | 30+ minutes (manual git archaeology) | < 5 minutes (tagged rollback) | < 1 minute (one-command rollback) | Rollback script timing |

---

## Section 8: Risks & Mitigations

### Comprehensive Risk Register

| Risk | Impact | Likelihood | Current State | Mitigation | Owner | Timeline |
|------|--------|------------|---------------|------------|-------|----------|
| **1. Enforcement Gap (90% documented, 10% enforced)** | HIGH | CERTAIN | Rules exist but don't run automatically. Violations only caught in production or manual audits | **P0:** Deploy pre-commit hooks (Task 0.1), CI/CD gates (Task 1.2), integrate WARDEN scanner (Task 1.1) within Week 1-2 | WARDEN + CI/CD | Phase 0-1 |
| **2. Single-Developer Bus Factor** | HIGH | MEDIUM | Only one developer knows GAIA architecture. Code loss risk on drive failure or departure | **P1:** GitHub remotes for all modules (Task 0.3), comprehensive documentation (Task 4.2), onboarding guide in GAIA_BIBLE.md | All | Phase 0 |
| **3. Stale Modules (ECHO)** | MEDIUM | CERTAIN | ECHO has 19 version copies, no git history, unclear which version is current. Cannot be safely deployed or tested | **P2:** Rescue/retire decision (Task 0.4) within Week 1, consolidate or archive by end of Phase 1 | ECHO | Phase 0-1 |
| **4. No CI/CD** | HIGH | CERTAIN | Zero repos have automated testing. Tests exist but aren't enforced. Manual testing is incomplete and inconsistent | **P0:** GitHub Actions for all repos (Task 1.2), branch protection requiring passing CI (Task 1.2), coverage gates (Task 1.4) | CI/CD | Phase 1 |
| **5. Telemetry Gaps (8/9 modules silent)** | MEDIUM | CERTAIN | ARGUS is blind to 8/9 modules. Cannot track cost, detect failures, or learn from outcomes without telemetry | **P1:** Follow PROTEUS telemetry pattern (Task 2.3), connect remaining modules to ARGUS event bus | ARGUS | Phase 2 |
| **6. Manual Memory Promotion** | MEDIUM | CERTAIN | Errors are logged but never become prevention rules. Same mistakes repeat across sessions and projects | **P1:** Auto-promotion pipeline (Task 3.1) from ARGUS errors to MNEMIS rules, human-in-the-loop approval | MNEMIS | Phase 3 |
| **7. HART OS Regression Risk** | HIGH | MEDIUM | HART OS in production with no CI. Breaking changes could affect real therapy scoring. State management bugs hard to detect | **P0:** CI enforcement (Task 1.2) blocks merges on test failures, state management tests, smoke tests on prod data | HART OS | Phase 1 |
| **8. Secrets Exposure (git history)** | CRITICAL | KNOWN | HART OS git history contains exposed OpenAI key (discovered Jan 2026). Key still active in production | **Immediate:** User must manually revoke exposed key, rotate quarterly, add WARDEN secret scan (Task 1.1) to prevent future exposure | User + WARDEN | Immediate |
| **9. No Background Tasks** | LOW | CERTAIN | All operations synchronous/on-demand. No scheduled scans, no proactive monitoring, no automated maintenance | **P2:** Task runner (Task 4.6) for async monitoring, scheduled compliance scans, cost report generation | ARGUS | Phase 4 |
| **10. MCP Integration Gap** | LOW | CERTAIN | GAIA modules not discoverable by Claude Code. Agents must know tool names/capabilities manually. Limits adoption | **P3:** GAIA modules as MCP servers (Task 4.4) for Claude Code discoverability, hot-reload on tool changes | All | Phase 4 |
| **11. Context Window Overflow** | HIGH | HIGH | Large projects exceed LLM context window (200K tokens for Sonnet). Token overflow causes failures or truncated responses | **P1:** Context manager (Task 4.7) with progressive disclosure, summarization of old context, on-demand file loading | MYCEL | Phase 4 |
| **12. LLM Cost Blindness** | HIGH | HIGH | No real-time cost tracking. Monthly bill is a surprise. No way to attribute cost to specific projects or operations | **P1:** Trust Dashboard (Task 2.2) with real-time cost tracking, breakdown by project/provider, cost alerts on thresholds | ARGUS | Phase 2 |
| **13. Test Writing Velocity** | MEDIUM | HIGH | Test writing is slowest part of development. Coverage can drop during rapid prototyping | **P2:** Auto-generated tests (Task 4.1) for new code, LLM-powered test generation, human review before merge | WARDEN | Phase 4 |
| **14. Documentation Staleness** | MEDIUM | CERTAIN | Documentation is manually maintained, always lags code changes. API docs missing for most modules | **P2:** Auto-generated docs (Task 4.2) from docstrings + CLAUDE.md, auto-deploy on push to main | All | Phase 4 |
| **15. No Rollback Mechanism** | HIGH | KNOWN | PROTEUS v0.2.1 failure had no easy rollback path. Manual git archaeology to find last working commit | **P1:** Known-good tagging (Task 4.3), one-command rollback script, auto-rollback on CI failure (with confirmation) | All | Phase 4 |
| **16. Skill Fragmentation** | MEDIUM | MEDIUM | Skills exist (explain-only, debug-explorer) but aren't shared across projects. Each project reinvents skills | **P2:** Skill auto-discovery (Task 4.5), centralized skill registry in ARGUS, portable skill definitions | ARGUS | Phase 4 |
| **17. Agent Failure Opacity** | HIGH | CERTAIN | Claude Code agents fail silently. No real-time visibility into what agents are doing or why they failed | **P1:** Process Observer (Task 2.1) tracking all agent activity, real-time subagent view (Task 2.4), error surfacing | ARGUS | Phase 2 |
| **18. Memory Loss Between Sessions** | MEDIUM | CERTAIN | Agents forget lessons between sessions. No cross-session enforcement of learned rules | **P1:** Cross-session enforcement (Task 3.2) of MNEMIS rules, persistent learning across all Claude Code sessions | MNEMIS | Phase 3 |
| **19. VULCAN Misalignment** | HIGH | CERTAIN | VULCAN creates projects but lacks its own CLAUDE.md. Violates self-alignment principle, risks invalid templates | **P0:** Create VULCAN CLAUDE.md (Task 0.2) defining project creation standards, agent roles, quality gates | VULCAN | Phase 0 |
| **20. Reference Re-Fetching** | LOW | HIGH | Products repeatedly re-fetch same URLs, losing context about reliability/relevance. Wastes time and LLM tokens | **P2:** URL/reference knowledge base (Task 3.3) in MNEMIS, centralized citation validation, reference reuse | MNEMIS | Phase 3 |
| **21. Mental Model Effectiveness Unknown** | MEDIUM | CERTAIN | ARGUS has 59 mental models but no feedback loop. Can't know which models are useful vs. noise | **P2:** Mental model outcome tracking (Task 3.4), effectiveness scoring, A/B testing, periodic pruning | ARGUS | Phase 3 |
| **22. Phase Dependency Deadlock** | MEDIUM | MEDIUM | Complex dependencies between phases could create deadlock if tasks block each other unexpectedly | **Mitigation:** Clear dependency map (Section 6.6), parallel workstreams, weekly progress reviews, flexibility to reorder non-critical tasks | PM | All Phases |
| **23. LLM API Changes Breaking Integration** | MEDIUM | LOW | OpenAI/Anthropic/Gemini API changes could break MYCEL integration, affecting all downstream products | **Mitigation:** MYCEL abstraction layer isolates products from provider changes, version pinning, test suite for each provider | MYCEL | Ongoing |
| **24. User Adoption Resistance** | MEDIUM | MEDIUM | User (developer) may resist enforcement layers as "too restrictive" or "slowing down development" | **Mitigation:** Phase 0 hooks with clear bypass mechanism (--no-verify), gradual rollout (60% → 80% coverage), dashboard showing time saved by automation | All | Phase 0-1 |

---

### Risk Probability & Impact Matrix

```
IMPACT
  ^
C | [8. Secrets]              [1. Enforcement Gap]
R |                           [4. No CI/CD]
I |                           [7. HART Regression]
T |                           [12. Cost Blindness]
I | [19. VULCAN Misalign]     [17. Agent Opacity]
C | [3. Stale Modules]        [11. Context Overflow]
A | [9. Background Tasks]     [15. No Rollback]
L | [10. MCP Gap]             [24. User Adoption]
  |
H | [2. Bus Factor]           [6. Manual Memory]
I |                           [5. Telemetry Gaps]
G |                           [13. Test Velocity]
H | [20. Reference Refetch]   [18. Memory Loss]
  | [23. API Changes]         [21. Model Effectiveness]
  |
M | [14. Doc Staleness]       [16. Skill Fragmentation]
E |                           [22. Dependency Deadlock]
D |
  +------------------------------------>
    LOW     MEDIUM    HIGH    CERTAIN
              LIKELIHOOD
```

**Priority for Mitigation:**
1. **CRITICAL/CERTAIN (Top Right):** Tasks 0.1, 0.2, 1.1, 1.2, 2.1 (Phases 0-2)
2. **HIGH/CERTAIN (Right):** Tasks 0.4, 2.2, 2.3, 3.1, 3.2 (Phases 0-3)
3. **CRITICAL/MEDIUM (Top Center):** Task 0.3, immediate secret rotation
4. **HIGH/MEDIUM (Center):** Tasks 4.3, 4.7 (Phase 4)

---

### Risk Mitigation Summary

**Immediate Actions (Week 1):**
- Rotate exposed OpenAI key in HART OS git history
- Deploy pre-commit hooks to all repos (Task 0.1)
- Create VULCAN CLAUDE.md (Task 0.2)
- Push all repos to GitHub (Task 0.3)

**Phase 0-1 Focus (Weeks 1-3):**
- Eliminate top 5 CRITICAL/CERTAIN risks: Enforcement Gap, No CI/CD, VULCAN Misalignment, Agent Opacity, HART Regression

**Phase 2-3 Focus (Weeks 3-5):**
- Address HIGH/CERTAIN risks: Telemetry Gaps, Manual Memory Promotion, Cost Blindness, Memory Loss, Context Overflow

**Phase 4 Focus (Weeks 5-8):**
- Address MEDIUM/LOW risks: Background Tasks, MCP Integration, Test Velocity, Doc Staleness, No Rollback

**Ongoing Monitoring:**
- Weekly risk review in ARGUS dashboard
- Escalation path for new risks discovered during implementation
- Quarterly risk register update with lessons learned

---

## Document End

**Total Word Count:** ~3,400 words

**Next Steps:**
1. Review sections 6-8 with stakeholders
2. Prioritize Phase 0 tasks for immediate execution
3. Assign owners to each task
4. Set up weekly progress tracking in ARGUS
5. Integrate with sections 1-5 for complete PRD

---

**Revision History:**
- v1.0 (2026-02-08): Initial draft of sections 6-8

---

# Part IV: Reference

## Section 9: Technical Specifications

### 9.1 Technology Stack

| Layer | Technology | Version | Purpose | Status |
|-------|-----------|---------|---------|--------|
| **Language** | Python | 3.10+ (3.9 for HART OS) | Primary development language | Active |
| **Frontend** | Streamlit | Latest | Dashboard UI (ARGUS, VULCAN, jSeeker, products) | Active |
| **Frontend** | React + React Flow | Latest | Visual node editor (ABIS) | Planned |
| **Backend** | FastAPI | Latest | API layer (ABIS graph compiler) | Planned |
| **Database** | SQLite | 3.x | Event storage (ARGUS), data storage (jSeeker) | Active |
| **Validation** | Pydantic | 2.x | Data schemas, configuration management | Active |
| **Dependency Mgmt** | Poetry | Latest | Package management (MYCEL only) | Partial |
| **Dependency Mgmt** | pip | Standard | Package management (8/9 modules) | Active |
| **Testing** | pytest | Latest | Unit and integration testing (1,522 tests) | Active |
| **Linting** | ruff | Latest | Code quality (configured in MYCEL) | Partial |
| **Formatting** | black | Latest | Code formatting (configured in MYCEL) | Partial |
| **E2E Testing** | Playwright | Latest | Browser automation (VIA, HART OS, jSeeker) | Active |
| **Document Gen** | python-docx | Latest | DOCX output (jSeeker resumes) | Active |
| **Document Gen** | FPDF2 | Latest | PDF output (jSeeker resumes) | Active |
| **CLI Framework** | Click | Latest | Command-line tools (VULCAN, WARDEN) | Active |
| **ORM** | SQLAlchemy | 2.x | Database abstraction (ARGUS, jSeeker) | Active |
| **Embeddings** | Various | N/A | OpenAI, Anthropic, Gemini (via MYCEL) | Active |
| **Version Control** | Git | Latest | All modules except ECHO, WARDEN, RAVEN | Active |
| **AI Orchestration** | Claude Code | Latest | Agent framework, MCP plugins | Active |

**Key Dependencies:**
- **MYCEL**: Foundation library for RAG, chunking, embedding, LLM clients
- **MNEMIS**: Cross-project memory (3-tier hierarchy)
- **ARGUS**: Telemetry and monitoring backbone

### 9.2 Environment Requirements

#### Operating System
- **Primary:** Windows 11 (developer workstation)
- **Target:** Linux (CI/CD deployment, future)
- **Shell:** PowerShell (primary), Bash (CI/CD scripts)

#### Python Environment
- **Version Policy:** 3.10+ (standard), 3.9 minimum (HART OS backward compatibility)
- **Virtual Environments:** venv (per-project isolation)
- **Package Management:** Poetry (MYCEL), pip (all others)

#### Storage & Filesystem
- **Root:** `X:\Projects\_GAIA\` (ecosystem root)
- **Products:** `X:\Projects\` (HART OS, VIA, DATA FORGE)
- **Logs:** `X:\Projects\_GAIA\logs\` (telemetry, event bus)
- **Memory:** `C:\Users\Fede\.claude\projects\` (MNEMIS integration with Claude Code)
- **Configuration:** `C:\Users\Fede\.claude\settings.json` (Claude Code global config)

#### Database Requirements
- **ARGUS Event Bus:** SQLite at `X:\Projects\_GAIA\logs\argus_events.db`
- **jSeeker Data:** SQLite at `X:\Projects\jSeeker\data\jseeker.db` (currently at `X:\Projects\_GAIA\_PROTEUS\data\proteus.db`, to be migrated)
- **No Cloud:** All data stored locally (no external DB servers)

#### Network Requirements
- **No Cloud Deployment:** Ecosystem runs entirely offline
- **API Access:** Requires API keys for OpenAI, Anthropic, Gemini (via .env files)
- **GitHub:** Optional (currently only HART OS has remote)

### 9.3 API & Integration Points

#### LLM Provider Integration
| Provider | Integration Point | Models Used | Status |
|----------|------------------|-------------|--------|
| OpenAI | MYCEL `llm_clients.py` | GPT-4, GPT-3.5, text-embedding-ada-002 | Active |
| Anthropic | MYCEL `llm_clients.py` | Claude Opus, Sonnet | Active |
| Gemini | MYCEL `llm_clients.py` | Gemini Pro | Active |

**Implementation:** All LLM calls routed through MYCEL for consistency, error handling, and telemetry.

#### Claude Code Integration
| Integration Type | Component | Purpose | Status |
|-----------------|-----------|---------|--------|
| MCP Plugins | 10 external plugins | File system, GitHub, web fetch, etc. | Active |
| Custom Skills | 4 user-level skills | explain-only, phase-update, doc-sync, debug-explorer | Active |
| Agents | 11 defined agents | quick-helper, geco-auditor, etc. | Active |
| Memory | MNEMIS bridge | Cross-session project memory | Active |
| Global Config | `.claude/CLAUDE.md` | Constitutional constraints | Active |
| Project Config | Module-level `CLAUDE.md` | Per-module governance (4/9 modules) | Partial |

**Process Observer (Planned):** ARGUS will monitor Claude Code agent executions via filesystem, MCP, or API integration (design TBD).

---

### 9.4 Agent Model Routing

**Authority-Based Model Selection:**

GAIA uses a three-tier model routing strategy based on agent authority levels and task complexity. This ensures cost efficiency while maintaining quality for critical operations.

| Authority Level | Model | Cost (per 1K tokens) | Use Case |
|----------------|-------|---------------------|----------|
| GAIA-level | claude-opus-4 | $15.00/$75.00 | System-wide decisions, constitutional enforcement |
| PROJECT-level | claude-sonnet-4-5 | $3.00/$15.00 | Resume adaptation, investment analysis, therapy plans |
| AGENT-level | claude-haiku-4-5 | $0.80/$4.00 | Parsing, extraction, formatting, simple classification |

**Task-to-Model Mapping (from jSeeker implementation):**

| Task | Model | Rationale |
|------|-------|-----------|
| JD parsing | Haiku | Structured extraction, low complexity |
| Resume matching | Haiku | Pattern matching, deterministic |
| Resume adaptation | Sonnet | Creative writing, context-sensitive |
| ATS scoring | Sonnet | Multi-criteria evaluation |
| Outreach generation | Sonnet | Persuasive writing, personalization |

**Cost Enforcement:**
- Monthly budget: $10/month (configurable per product)
- Warning threshold: $8/month (80%)
- SHA256 caching: Identical prompts return cached response (zero cost)
- Cost per call logged to ARGUS telemetry (JSONL)

**Implementation Pattern:**
```python
from rag_intelligence.integrations import create_llm_client
from rag_intelligence.config import GaiaSettings

def select_model_for_task(task_type: str, authority_level: int) -> str:
    """Route to appropriate model based on task and authority."""
    if authority_level >= 4:  # GAIA-level
        return "claude-opus-4"
    elif task_type in ["parsing", "extraction", "formatting"]:
        return "claude-haiku-4-5"  # Simple tasks
    else:
        return "claude-sonnet-4-5"  # Complex tasks
```

**Cost Tracking:**
- Every LLM call emits telemetry to ARGUS: `{task, model, input_tokens, output_tokens, cost_usd, latency_ms}`
- Trust Dashboard aggregates cost by project, model, and task type
- Budget alerts sent when approaching monthly limit

#### GitHub Integration
| Repository | Status | Remote URL | Branch Strategy |
|-----------|--------|-----------|-----------------|
| HART OS | Active | `https://github.com/ZoeDepthTokyo/hart-os.git` | External project |
| GAIA Modules (9) | Local only | None | Not configured |
| Target | Future | TBD (monorepo vs. separate repos) | To be decided |

**Known Issue:** HART OS has hardcoded OpenAI key in git history (requires key rotation).

#### Internal GAIA APIs

**MYCEL Public API:**
```python
# RAG Pipeline
from mycel import RAGPipeline, Chunk, ChunkingStrategy
from mycel.llm_clients import get_client

# LLM Abstraction
client = get_client(provider="anthropic", model="claude-opus-4")
response = client.chat(messages=[...])
```

**ARGUS Event Bus API:**
```python
# Telemetry Logging
from argus.dashboard.event_bus import EventBus

bus = EventBus()
bus.log_event(
    source="proteus",
    event_type="resume_generated",
    metadata={"job_id": "12345", "ats_score": 92}
)
```

**MNEMIS Memory API:**
```python
# Cross-Project Memory
from mnemis.core.memory_store import MemoryStore
from mnemis.core.promotion import PromotionProtocol

store = MemoryStore(tier="PROJECT")
store.save(key="vulcan_config", value=config_data)

# Promote to GAIA tier
protocol = PromotionProtocol()
protocol.promote(source="PROJECT", target="GAIA", key="vulcan_config")
```

**LOOM Workflow API (Partial):**
```python
# Workflow Execution (models exist, runtime not integrated)
from loom.models.workflow import Workflow, WorkflowStep
from loom.governance.validator import validate_workflow

workflow = Workflow(steps=[...])
validate_workflow(workflow)  # Not enforced at runtime
```

### 9.4 Data Architecture

#### Registry Schema (`registry.json`)
**Format:** `gaia-registry-v1`
**Location:** `X:\Projects\_GAIA\registry.json`
**Purpose:** Single source of truth for all GAIA modules and products

**Fields:**
```json
{
  "module_name": {
    "name": "Human-readable name",
    "path": "Absolute filesystem path",
    "version": "Semantic version (x.y.z)",
    "status": "production | development | stale | placeholder",
    "git": true | false,
    "git_remote": "GitHub URL or null",
    "python": "Python version requirement",
    "framework": "streamlit | library | static-html",
    "port": "Streamlit port or null",
    "providers": ["openai", "anthropic", "gemini"],
    "depends_on": ["mycel", "mnemis"],
    "tags": ["keyword", "tags"]
  }
}
```

**Current Entries:** 13 (9 GAIA modules, 4 products)

#### Telemetry Data

**ARGUS Event Bus (SQLite):**
- **Schema:** `events` table with `id`, `timestamp`, `source`, `event_type`, `metadata` (JSON)
- **Location:** `X:\Projects\_GAIA\logs\argus_events.db`
- **Active Sources:** PROTEUS only (8/9 modules not integrated)
- **Retention:** Indefinite (local storage)

**PROTEUS Build Log (JSONL):**
- **Schema:** One JSON object per line (resume build events)
- **Location:** `X:\Projects\_GAIA\logs\proteus_build.jsonl`
- **Fields:** `timestamp`, `job_id`, `resume_version`, `ats_score`, `provider_tokens`

**Inconsistency:** ARGUS uses SQLite, PROTEUS uses JSONL. Unified schema needed (see Open Questions).

#### Memory Architecture (MNEMIS)

**Three-Tier Hierarchy:**

| Tier | Scope | Access | Promotion | Location |
|------|-------|--------|-----------|----------|
| **PROJECT** | Single module/product | Module agents only | Manual via CLI | `X:\Projects\[module]\.mnemis\` |
| **GAIA** | Ecosystem-wide | All GAIA modules | Requires authority | `X:\Projects\_GAIA\.mnemis\` |
| **PUBLIC** | External sharing | Anyone | Never promoted from | `X:\Projects\_GAIA\public_memory\` |

**Storage Format:** JSONL (one memory entry per line)

**Promotion Protocol:**
```python
# Manual promotion (current)
protocol.promote(source="PROJECT", target="GAIA", key="error_pattern_123")

# Automated promotion (planned, not implemented)
if error.count > 5 and error.severity == "HIGH":
    protocol.auto_promote(error, justification="Recurring critical error")
```

**Authority Graph:**
- GAIA tier: Read by all, write by LOOM governance only
- PROJECT tier: Read/write by owning module
- PUBLIC tier: Read-only reference library

#### Configuration Management

**GaiaSettings Base Class (Pydantic):**
```python
from pydantic import BaseSettings, Field

class GaiaSettings(BaseSettings):
    module_name: str
    version: str
    gaia_root: Path = Field(default=Path("X:/Projects/_GAIA"))

    class Config:
        env_prefix = "GAIA_"
        case_sensitive = False
```

**Environment Variables:**
- **API Keys:** `.env` files per product (never committed)
- **Paths:** Absolute paths in registry (portable across Windows/Linux)
- **Feature Flags:** Not implemented (future consideration)

### 9.5 Security Considerations

#### Threat Model
**Scope:** Local development environment (single-user, no network exposure)
**Out of Scope:** Cloud deployment, multi-user access, network security

#### Current Security Posture

**Secrets Management:**
- **API Keys:** Stored in `.env` files (not committed to git)
- **Known Vulnerability:** HART OS has OpenAI key in git history (rotation required)
- **WARDEN Scanner:** Can detect hardcoded secrets (not integrated into workflow)

**Access Control:**
- **Filesystem:** Windows user permissions (single developer)
- **Memory Tiers:** Authority graph enforced by MNEMIS (not validated at runtime)
- **Database:** SQLite local files (no authentication)

**Data Protection:**
- **No PHI:** HART OS processes therapy assessment data (must not log to telemetry)
- **No PII:** User resumes in PROTEUS (stored locally only)
- **No Cloud:** All data remains on local filesystem

**Known Risks:**

| Risk | Severity | Likelihood | Impact | Mitigation Status |
|------|----------|-----------|--------|-------------------|
| Hardcoded API keys | CRITICAL | High | Account compromise | WARDEN scanner exists, not enforced |
| Git history secrets | CRITICAL | Confirmed | OpenAI key exposed | Requires key rotation + history rewrite |
| No input validation | HIGH | Medium | LLM prompt injection | Pydantic validates schemas, not content |
| Unrestricted file access | MEDIUM | Low | Data exfiltration | Single-user, trusted environment |
| No audit trail | MEDIUM | Medium | Unable to trace security events | ARGUS event bus exists, not used for security |

**Planned Mitigations:**
1. **Pre-commit Hook:** Integrate WARDEN scanner to block secrets
2. **Key Rotation:** Replace HART OS OpenAI key, add to `.env`
3. **Input Validation:** Extend Pydantic models to sanitize LLM inputs
4. **Security Telemetry:** Log security events to ARGUS (auth failures, secret detection)

#### Compliance Requirements
**Not Applicable:** No regulatory requirements (HIPAA, GDPR, SOC2) for local development.

**Future Consideration:** If HART OS is deployed for clinical use, PHI protection becomes mandatory.

---

## Section 10: Open Questions for Team Review

### 10.1 For Product (PROD)

#### 1. ECHO Rescue vs Retire Decision
**Context:** ECHO (Chat Archaeology & Taxonomy) has 19 manual UI versions, stale since January 2026. No tests, no CLAUDE.md, not in git.

**Question:** Is there user demand for chat archaeology functionality, or should we deprecate ECHO and free up maintenance resources?

**Impact:**
- **Rescue:** Requires consolidation (19 versions → 1), git migration, test coverage, CLAUDE.md
- **Retire:** Reduce technical debt, focus resources on PROTEUS/HART OS

**Recommendation:** If no active users, deprecate. If demand exists, prioritize consolidation sprint (est. 2 weeks).

---

#### 2. Product Roadmap Priority
**Context:** Multiple products in different states:
- **HART OS:** Production but unstable (navigation state loss, OpenAI key in git)
- **PROTEUS:** Active development, telemetry integrated, roadmap clear
- **VIA Intelligence:** Production, feature-complete, stable
- **DATA FORGE:** Production, maintenance mode

**Question:** Where should engineering investment be prioritized for next quarter?

**Options:**
1. **Stabilize HART OS:** Fix state management, rotate keys, add tests (4-6 weeks)
2. **Accelerate PROTEUS:** Build out subagent monitoring, multi-provider support (3-4 weeks)
3. **Expand VIA:** Add new RAG features, improve synthesis quality (3-4 weeks)
4. **Platform Focus:** Build out ARGUS Process Observer, LOOM runtime enforcement (6-8 weeks)

**Recommendation:** Parallel tracks: HART OS stabilization (critical) + PROTEUS acceleration (momentum).

---

#### 3. Success Metrics Validation
**Context:** Proposed metrics from GECO Audit:
- **HART OS:** ATS score ≥ 85, interview rate +40%
- **VIA:** Synthesis quality score ≥ 90, decision confidence +50%
- **PROTEUS:** Resume generation time < 5 min, job application rate +300%

**Question:** Are these the right measures of product value? Do they align with user goals and business outcomes?

**Considerations:**
- Are metrics measurable with current telemetry?
- Do metrics reflect user satisfaction or just technical performance?
- Should we track engagement/retention metrics?

**Recommendation:** Validate with user interviews before locking in Q2 OKRs.

---

#### 4. Market Positioning Clarity
**Context:** GAIA is described as "ecosystem," "platform," and "developer tool" in different documents.

**Question:** What is GAIA? How do we communicate this to users?

**Options:**
1. **Developer Tool:** Like VS Code extensions (target: developers building AI products)
2. **Platform:** Like Zapier for AI (target: non-technical users orchestrating agents)
3. **Ecosystem:** Like AWS (target: enterprises building on GAIA infrastructure)

**Implications:**
- Affects marketing, documentation, UX design
- Determines pricing strategy (if monetization pursued)
- Shapes roadmap (developer tools vs. end-user products)

**Recommendation:** Choose one primary positioning, acknowledge others as secondary use cases.

---

#### 5. Monetization Strategy
**Context:** Currently 100% free, local-only. No SaaS, no enterprise licensing.

**Question:** Is there a path to revenue, or is GAIA purely open source?

**Options:**
1. **Open Source (MIT/Apache):** Maximize adoption, community-driven
2. **Open Core:** Free for individuals, paid for teams/enterprise
3. **Self-Hosted Enterprise:** License GAIA for internal deployment
4. **SaaS (Future):** Cloud-hosted GAIA platform

**Implications:**
- Open source: No revenue, but community growth
- Open core: Requires feature gating, support infrastructure
- Enterprise: Requires security audit, compliance certifications
- SaaS: Requires cloud infrastructure, multi-tenancy, data privacy

**Recommendation:** Start with open source (build community), evaluate monetization once adoption validates demand.

---

### 10.2 For Engineering (ENG)

#### 1. CI/CD Rollout Strategy
**Context:** 1,522 tests exist across ecosystem, but zero are enforced via CI/CD. No GitHub Actions, no pre-commit hooks.

**Question:** Which modules get CI/CD first? What's the rollout order?

**Proposed Priority:**
1. **MYCEL** (foundation library) — ensures no regressions in shared code
2. **VULCAN** (highest LOC at 1,847) — prevents scaffolding bugs
3. **PROTEUS** (active development) — enforces test coverage on new features
4. **ARGUS** → **LOOM** → **MNEMIS** → **WARDEN** → **ECHO** (if rescued)

**Criteria:**
- **MYCEL first:** Break MYCEL = break everything
- **VULCAN second:** Most complex, highest technical debt
- **PROTEUS third:** Active development, needs guardrails

**Estimated Effort:** 1-2 days per module (GitHub Actions setup, pre-commit config)

**Recommendation:** Start with MYCEL this sprint, add VULCAN/PROTEUS next sprint.

---

#### 2. Pre-Commit Hook Standardization
**Context:** MEMORY.md documents pre-commit hooks as "pending post-VULCAN testing." No hooks currently deployed.

**Question:** Should all 9 modules use the same pre-commit hooks, or module-specific configurations?

**Proposed Standard:**
```yaml
# .pre-commit-config.yaml (all modules)
repos:
  - repo: local
    hooks:
      - id: ruff
        name: Lint with ruff
        entry: ruff check
        language: system
        types: [python]

      - id: black
        name: Format with black
        entry: black --check
        language: system
        types: [python]

      - id: pytest
        name: Run tests (min 80% coverage)
        entry: pytest --cov --cov-fail-under=80
        language: system
        types: [python]
        pass_filenames: false

      - id: warden
        name: Scan for secrets
        entry: python _WARDEN/scanner.py
        language: system
        types: [python]
```

**Module-Specific Exceptions:**
- **MYCEL:** Already has Poetry + TDD config (keep existing)
- **HART OS:** May need coverage threshold < 80% (legacy code)
- **ECHO:** Skip until rescue decision made

**Recommendation:** Standard hooks for all, but allow per-module override in `.pre-commit-config.yaml`.

---

#### 3. Telemetry Schema Unification
**Context:**
- **ARGUS Event Bus:** SQLite with `(id, timestamp, source, event_type, metadata JSON)`
- **PROTEUS Build Log:** JSONL with flat fields
- **Other Modules:** Not sending telemetry

**Question:** Should ARGUS event bus accept JSONL, SQLite, or both? Do we need a unified schema?

**Options:**

| Approach | Pros | Cons |
|----------|------|------|
| **SQLite Only** | Queryable, structured, transactions | File locking on Windows, schema migrations |
| **JSONL Only** | Append-only, no locking, portable | Hard to query, no schema enforcement |
| **Hybrid (SQLite + JSONL)** | Write to JSONL, import to SQLite | Complexity, two sources of truth |

**Recommendation:**
1. **Short-term:** Keep SQLite, add JSONL export for backup
2. **Long-term:** Migrate to ClickHouse or DuckDB (if telemetry volume grows)

**Schema Standard:**
```json
{
  "timestamp": "ISO8601",
  "source": "module_name",
  "event_type": "dotted.namespace.event",
  "level": "DEBUG | INFO | WARN | ERROR",
  "metadata": {
    "key": "value",
    "nested": {...}
  }
}
```

---

#### 4. WARDEN Integration Point
**Context:** WARDEN scanner exists (6,908 bytes) but not integrated into workflow.

**Question:** Where should WARDEN run? Pre-commit hook, GitHub Action, standalone CLI, or all three?

**Options:**

| Integration Point | Pros | Cons | Priority |
|------------------|------|------|----------|
| **Pre-commit Hook** | Catches secrets before commit | Slows down commits, local only | HIGH |
| **GitHub Action** | Enforces on PR, runs on CI server | Requires GitHub remotes | MEDIUM |
| **Standalone CLI** | Manual audits, existing codebases | Requires discipline to run | LOW |

**Recommendation:**
1. **Phase 1:** Pre-commit hook (blocks secrets immediately)
2. **Phase 2:** GitHub Action (once remotes configured)
3. **Phase 3:** Standalone CLI (for one-time audits of existing code)

**Estimated Effort:** 1 day (hook integration), 1 day (GitHub Action)

---

#### 5. Process Observer Implementation Approach
**Context:** ARGUS Process Observer is documented but not implemented. Must monitor Claude Code agent executions.

**Question:** How should Process Observer monitor Claude Code agents?

**Options:**

| Approach | Pros | Cons | Feasibility |
|----------|------|------|-------------|
| **MCP Plugin** | Official integration, stable API | Requires MCP server dev | Medium |
| **Filesystem Observer** | Watch `.claude/` directory for logs | No API dependency | High |
| **Claude Code API** | Direct telemetry from agent runtime | API may not exist or be stable | Low |
| **Hybrid (Filesystem + MCP)** | Robust, multi-source telemetry | Complexity, duplication | Medium |

**Recommendation:**
1. **Prototype:** Filesystem observer (watch `.claude/projects/*/memory/MEMORY.md` for task completion)
2. **Production:** MCP plugin (once stable API confirmed)

**Estimated Effort:** 3-5 days (filesystem), 1-2 weeks (MCP plugin)

---

#### 6. GitHub Strategy
**Context:** Only HART OS has GitHub remote. GAIA modules are local-only.

**Question:** What's the GitHub strategy for GAIA ecosystem?

**Sub-questions:**
1. **Repo Structure:** Monorepo (1 repo, 9 modules) vs. separate repos (9 repos)?
2. **Visibility:** Public open source vs. private?
3. **Branch Strategy:** Trunk-based (main only) vs. GitFlow (main + develop + feature)?

**Considerations:**

| Repo Structure | Pros | Cons | Recommendation |
|---------------|------|------|----------------|
| **Monorepo** | Single PR review, easier cross-module changes | Large repo, slow clones | If modules tightly coupled |
| **Separate Repos** | Independent versioning, smaller repos | Cross-module changes require multiple PRs | If modules loosely coupled |

**Visibility:**
- **Public:** Maximizes community, enables contributions
- **Private:** Protects IP, avoids support burden

**Recommendation:**
1. **Short-term:** Separate repos, private (iterate faster)
2. **Long-term:** Evaluate open source once stable (v1.0+)

---

#### 7. Python Version Policy
**Context:**
- **Standard:** 3.10+
- **Exception:** HART OS requires 3.9 (backward compatibility)

**Question:** Should all modules standardize on 3.10+, or allow 3.9?

**Implications:**
- **3.10+ Only:** Access to pattern matching, better type hints, ParamSpec
- **Allow 3.9:** Maintain HART OS compatibility, limit language features

**Recommendation:**
- **GAIA Modules:** Require 3.10+ (use modern features)
- **Products:** Allow 3.9 if needed (HART OS exception documented)

---

#### 8. Dependency Management Standardization
**Context:**
- **MYCEL:** Uses Poetry (pyproject.toml)
- **Others:** Use pip + requirements.txt

**Question:** Should we standardize on Poetry ecosystem-wide?

**Poetry Pros:**
- Lockfile (reproducible builds)
- Dev dependencies separation
- Automatic virtual environment management
- Better version resolution

**Poetry Cons:**
- Learning curve
- Migration effort for 8 modules
- Requires Poetry installation (not just pip)

**Recommendation:**
1. **Keep MYCEL on Poetry** (already configured)
2. **Migrate VULCAN, PROTEUS to Poetry** (active development, benefit from lockfiles)
3. **Leave products on pip** (simpler for end users)

---

### 10.3 For UX/UI

#### 1. Streamlit vs Web Framework Migration
**Context:** All GAIA dashboards use Streamlit (ARGUS, VULCAN, PROTEUS). All products use Streamlit (HART OS, VIA, DATA FORGE).

**Question:** Is Streamlit sufficient for production UX, or should we migrate to React/Vue/Next.js?

**Streamlit Pros:**
- Rapid prototyping
- Python-native (no separate frontend codebase)
- Built-in component library

**Streamlit Cons:**
- Limited customization
- State management issues (HART OS navigation loss)
- Not suitable for complex interactions

**Recommendation:**
- **Keep Streamlit for ARGUS/VULCAN** (internal tools, developer audience)
- **Evaluate migration for PROTEUS/HART OS** (if UX issues block users)
- **Decision criteria:** User feedback, A/B test (Streamlit vs. React prototype)

---

#### 2. ARGUS Dashboard Usability
**Context:** ARGUS dashboard is developer-focused (SQL queries, event logs, technical metrics).

**Question:** Do we need end-user dashboards per product (non-technical users)?

**Scenarios:**
1. **HART OS:** Therapist views patient assessment trends
2. **PROTEUS:** Job seeker views application pipeline, ATS scores
3. **VIA:** Investor views portfolio insights, decision logs

**Recommendation:**
- **Phase 1:** Keep ARGUS technical (for developers debugging)
- **Phase 2:** Build product-specific dashboards (e.g., PROTEUS job dashboard)
- **Phase 3:** Generic "Trust Dashboard" for end users (constitutional compliance)

---

#### 3. Trust Dashboard Design Requirements
**Context:** ARGUS documentation describes a "Trust Dashboard" (constitutional compliance visualization) but directory is empty.

**Question:** How do we visualize constitutional compliance metrics for non-technical users?

**Proposed Metrics:**
- **Transparency Score:** % of agent actions logged
- **Safety Score:** % of pre-commit hooks passed, secrets detected
- **Governance Score:** % of LOOM rules enforced
- **Learning Score:** % of errors promoted to prevention rules

**Design Challenges:**
- Avoid overwhelming users with technical details
- Make metrics actionable (not just "93% compliance")
- Balance "glass-box transparency" with information overload

**Recommendation:**
1. **User Research:** Interview 3-5 users (what compliance info do they need?)
2. **Wireframe:** Low-fidelity mockup (dashboard layout, metrics displayed)
3. **Prototype:** Build in Streamlit, A/B test with users

**Estimated Effort:** 2-3 weeks (research + design + implementation)

---

#### 4. VULCAN Questionnaire UX Friction
**Context:** VULCAN uses 7-step questionnaire to gather project context (useful but lengthy).

**Question:** Can we reduce friction without losing context capture quality?

**Options:**
1. **Progressive Disclosure:** Show 3 core questions, expand for advanced options
2. **Smart Defaults:** Pre-fill based on project type (Streamlit app = port 8501)
3. **Multi-column Layout:** Show all 7 questions on one screen (reduce page loads)
4. **Save & Resume:** Allow partial completion, return later

**Recommendation:**
- **Quick win:** Smart defaults + multi-column layout (1 day)
- **Future:** Progressive disclosure (requires UX testing)

---

#### 5. Error Message Standards for Graceful Degradation
**Context:** GAIA_BIBLE.md describes "graceful degradation" (products work even if LLM fails), but no UX design pattern documented.

**Question:** How do we present fallback states to users?

**Examples:**
- **HART OS:** If GPT-4 fails, fall back to GPT-3.5 (do we show warning?)
- **PROTEUS:** If resume generation fails, offer manual template (how to guide user?)
- **VIA:** If semantic search fails, fall back to keyword search (transparent or hidden?)

**Proposed Standard:**
```
[ERROR] Operation Failed
We encountered an issue with [service name].
You can:
  • Try again (may work if temporary)
  • Use fallback mode (reduced functionality)
  • Save progress and contact support

[Technical Details] (collapsible)
Error: ConnectionTimeout at llm_clients.py:142
Provider: OpenAI GPT-4
Retry Attempts: 3
```

**Recommendation:**
1. **Define error levels:** Critical (block user), Warning (fallback), Info (FYI)
2. **Standardize wording:** Avoid jargon, offer clear actions
3. **Implement in MYCEL:** Error handler returns structured error object, products render consistently

---

#### 6. HART OS Navigation State Loss Investigation
**Context:** GECO Audit documents "navigation state loss" in HART OS (users lose progress when switching pages).

**Question:** Is this a UX design issue (intentional flow) or a state management bug (unintended)?

**Investigation Steps:**
1. **Reproduce:** Identify exact user flow that triggers state loss
2. **Diagnose:** Streamlit session state audit (what's persisted, what's not?)
3. **Fix:** If bug, use `st.session_state` correctly; if design issue, add "Save Progress" button

**Estimated Effort:** 2-3 days (investigation + fix)

**Priority:** HIGH (blocks users, affects production product)

---

#### 7. Localization Strategy
**Context:** HART OS uses Spanish (`src/config/locales/es.json`). If ecosystem expands internationally, need i18n framework.

**Question:** Do we need i18n ecosystem-wide, or just per-product as needed?

**Considerations:**
- **GAIA Modules:** All documentation in English (developer audience)
- **Products:** May need multiple languages (HART OS Spanish, VIA English)
- **Framework:** gettext (Python standard), Flask-Babel, or custom JSON

**Recommendation:**
- **GAIA Modules:** English only (documentation, logs, errors)
- **Products:** Per-product i18n as needed (don't over-engineer)
- **If multiple products need i18n:** Extract to MYCEL shared library

---

#### 8. Mobile Support Requirements
**Context:** All GAIA products are desktop-only (Streamlit not mobile-optimized).

**Question:** Is mobile access a requirement for any product?

**Use Cases:**
- **HART OS:** Therapist reviews patient notes on phone
- **PROTEUS:** Job seeker checks application status on mobile
- **VIA:** Investor reviews insights on tablet

**Options:**
1. **No Mobile:** Keep desktop-only (simplest, fastest)
2. **Responsive Streamlit:** CSS tweaks for mobile (limited, Streamlit constraints)
3. **Progressive Web App:** Separate mobile UI (high effort)
4. **Native App:** iOS/Android (very high effort, separate codebase)

**Recommendation:**
1. **Validate demand:** User interviews (how many access on mobile?)
2. **If low demand:** Keep desktop-only
3. **If high demand:** Build PWA for 1 product (PROTEUS likely candidate)

---

### 10.4 Cross-Functional

#### 1. Constitutional Enforcement vs Developer Experience Balance
**Context:** GAIA promises "constitutional governance" (every agent bound to rules), but enforcement adds friction (pre-commit hooks, governance validators slow down iteration).

**Question:** Where do we draw the line between strict governance and developer experience?

**Spectrum:**
- **Strict:** Block all commits that fail hooks (enforce rules, slow development)
- **Permissive:** Warn on hook failures, allow override (fast development, risk rule violations)
- **Hybrid:** Enforce critical rules (secrets, test coverage), warn on style

**Recommendation:**
- **Critical Rules (BLOCK):** Secrets detection (WARDEN), test coverage < 80%, breaking changes
- **Style Rules (WARN):** Linting, formatting, docstring coverage
- **Escape Hatch:** `--no-verify` flag for emergencies (logged to ARGUS)

---

#### 2. MNEMIS Auto-Promote vs User Confirmation
**Context:** MNEMIS promotion protocol is currently manual (user runs CLI command). GAIA_BIBLE.md describes "learning loop" (errors auto-promote to prevention rules).

**Question:** Should MNEMIS auto-promote without user confirmation (faster learning) or require approval (Trust Principle: user control)?

**Trust Principle 4:** "User consent required for memory promotion."

**Options:**

| Approach | Pros | Cons | Alignment with Trust Principle |
|----------|------|------|-------------------------------|
| **Auto-promote** | Faster learning, no user burden | Risk of bad rules spreading | VIOLATES (no consent) |
| **User confirmation** | User control, review before promotion | Slower learning, user fatigue | COMPLIANT |
| **Smart auto-promote** | Auto for low-risk, confirm for high-risk | Complexity, edge cases | PARTIAL |

**Recommendation:**
- **Start with user confirmation** (honor trust principle)
- **Add "auto-approve" setting** (user opts in to auto-promotion per rule type)
- **Example:** Auto-approve "formatting errors," confirm "security vulnerabilities"

---

#### 3. Skill System Design
**Context:** 4 user-level skills exist (explain-only, phase-update, doc-sync, debug-explorer). GAIA_BIBLE.md describes "skill registry" for auto-discovery, but not implemented.

**Question:** How do we enable user contribution to skills without chaos?

**Proposed Architecture:**
```
Skills/
  user-level/          # Personal skills (per user)
    my-skill/
      SKILL.md
      handler.py
  gaia-level/          # Ecosystem skills (shared)
    registry.json      # Auto-discovered by Claude Code
    db-migration/
      SKILL.md
      handler.py
```

**Contribution Flow:**
1. User creates skill in `user-level/`
2. User tests skill locally
3. User submits PR to promote to `gaia-level/`
4. LOOM governance validates (no security issues, no duplication)
5. Merged, added to `registry.json`

**Recommendation:**
- **Phase 1:** Document contribution guidelines (CONTRIBUTING.md)
- **Phase 2:** Build `skill-validator.py` (automated checks)
- **Phase 3:** Integrate with LOOM governance (manual review for now)

---

#### 4. Documentation Restructuring
**Context:** GAIA_BIBLE.md is 41,000 tokens (comprehensive but overwhelming). 90% coverage, but scattered across single file.

**Question:** Do we need restructured docs for different audiences (user guide, API reference, architecture deep-dive)?

**Proposed Structure:**
```
docs/
  README.md                # Quick start (5 min read)
  USER_GUIDE.md            # End-user documentation (products)
  DEVELOPER_GUIDE.md       # Module development (GAIA contributors)
  ARCHITECTURE.md          # Deep-dive (decision-makers, auditors)
  API_REFERENCE.md         # Function signatures (generated from docstrings)
  TUTORIALS/
    01_using_vulcan.md
    02_integrating_argus.md
  GAIA_BIBLE.md            # Constitutional source (comprehensive, append-only)
```

**Recommendation:**
- **Phase 1:** Extract quick start (README.md) from GAIA_BIBLE.md
- **Phase 2:** Generate API reference from docstrings (automated)
- **Phase 3:** Write user-facing guides (tutorials)
- **Keep GAIA_BIBLE.md as canonical reference** (don't delete, link from other docs)

---

#### 5. Version Strategy
**Context:**
- GAIA: v0.4.3
- Products: Independent versions (HART OS 6.2.8, PROTEUS 0.2.1, VIA 6.4)

**Question:** Should we synchronize versions or keep separate?

**Synchronized (Semantic Versioning):**
- **Pros:** Clear compatibility (GAIA v1.0 → all modules v1.0)
- **Cons:** Products forced to bump versions even if no changes

**Independent:**
- **Pros:** Products version at own pace
- **Cons:** Compatibility matrix complex (PROTEUS 0.2.1 requires MYCEL ≥ 0.2.0)

**Recommendation:**
- **GAIA Modules:** Synchronized (9 modules move together)
- **Products:** Independent (HART OS, VIA, PROTEUS version independently)
- **Compatibility:** Document in registry.json (`"depends_on": ["mycel >= 0.2.0"]`)

---

#### 6. Team Structure & Ownership
**Context:** Currently solo developer (Federico). No defined team structure or ownership.

**Question:** Who owns GAIA platform vs. individual products?

**Proposed Structure:**

| Role | Responsibilities | Owner (Current) | Owner (Future) |
|------|------------------|-----------------|----------------|
| **Platform Team** | MYCEL, ARGUS, LOOM, MNEMIS, WARDEN | Federico | 2-3 engineers |
| **PROTEUS Team** | Product development, roadmap | Federico | 1-2 engineers |
| **HART OS Team** | Stabilization, localization | Federico | 1 engineer |
| **VIA/DATA FORGE** | Maintenance mode | Federico | 0.5 engineer |

**Recommendation:**
- **Document ownership in registry.json** (add `"owner"` field)
- **Define on-call rotation** (if/when team grows)
- **Assign PROD/ENG/UX reviewers per module** (for PRs)

---

### 10.5 ABIS Integration Questions

#### 1. ABIS-LOOM Integration Architecture
**Context:** ABIS is a visual system builder, LOOM is a workflow orchestration engine. Both deal with agent systems but at different levels.

**Question:** Should ABIS use LOOM's workflow engine directly, or have its own execution layer?

**Options:**

| Approach | Pros | Cons |
|----------|------|------|
| **ABIS → LOOM (Compile)** | Single execution engine, no duplication | ABIS limited by LOOM's capabilities |
| **ABIS → Independent Engine** | ABIS can optimize for visual workflows | Duplication, divergence risk |
| **Hybrid (ABIS + LOOM Layer)** | ABIS adds visual layer, LOOM handles execution | Complexity, tight coupling |

**Recommendation:**
- **Phase 1:** ABIS compiles to LOOM workflow spec (leverage existing engine)
- **Phase 2:** Evaluate if ABIS needs custom execution for visual-specific features

---

#### 2. System Graph Spec v1 Schema Design
**Context:** ABIS will generate "System Graph Spec v1" (JSON schema for user-designed systems). LOOM already has workflow spec.

**Question:** Should System Graph Spec v1 be a superset of LOOM's workflow spec or independent?

**Options:**

| Approach | Pros | Cons |
|----------|------|------|
| **Superset of LOOM Spec** | Full compatibility, easy compilation | Constrained by LOOM schema |
| **Independent Schema** | ABIS-specific optimizations | Requires mapping layer to LOOM |
| **Versioned Extension** | LOOM spec + ABIS extensions | Complexity in version management |

**Recommendation:**
- System Graph Spec v1 extends LOOM workflow spec with visual metadata (node positions, canvas layout, UI config)
- Core execution semantics identical to LOOM (agents, edges, contracts)
- ABIS compiler strips visual metadata before passing to LOOM

**Example Structure:**
```json
{
  "schema_version": "1.0",
  "loom_workflow": { /* standard LOOM spec */ },
  "abis_metadata": {
    "canvas": { "width": 1920, "height": 1080 },
    "nodes": [
      {"id": "agent_1", "x": 100, "y": 200, "color": "#4A90E2"}
    ],
    "edges": [
      {"from": "agent_1", "to": "agent_2", "label": "data_flow"}
    ]
  }
}
```

---

#### 3. ABIS Frontend Architecture
**Context:** ABIS requires visual node editor (React Flow is leading candidate). GAIA ecosystem is Python-first (Streamlit).

**Question:** Should ABIS frontend be React standalone or embedded in Streamlit via st.components?

**Options:**

| Approach | Pros | Cons | Recommendation |
|----------|------|------|----------------|
| **React Standalone** | Best performance, full React ecosystem | Separate deployment, Python-JS bridge needed | RECOMMENDED |
| **Streamlit st.components** | Python-native, single deployment | Performance limits, Streamlit constraints | Prototype only |
| **Electron App** | Desktop-native, offline-first | Large bundle size, platform-specific builds | Future consideration |

**Recommendation:**
1. **Prototype:** Streamlit + st.components (validate UX concept quickly)
2. **Production:** React standalone app + FastAPI backend (performance, scalability)
3. **Integration:** React app embeds in GAIA dashboard via iframe or standalone launch

**Tech Stack (Recommended):**
- **Frontend:** React + React Flow + TypeScript
- **Backend:** Python FastAPI (graph compiler, validation)
- **Communication:** REST API + WebSocket (real-time validation feedback)
- **Storage:** SQLite (system designs, version history)

---

### 10.6 DOS Architecture Questions

#### 1. Agent Count for Typical Decision
**Context:** DOS is a multi-agent decision system. Each agent analyzes one dimension (financial, regulatory, market, risk).

**Question:** How many specialized agents for a typical decision? Fixed (3-7) or dynamic?

**Options:**

| Approach | Pros | Cons |
|----------|------|------|
| **Fixed (5 agents)** | Predictable cost/latency, simple orchestration | May be insufficient for complex decisions |
| **Dynamic (3-10 range)** | Adapts to decision complexity | Complex orchestration, unpredictable cost |
| **User-Configured** | Flexibility, user control | Requires UX for agent selection |

**Recommendation:**
- **Default:** 5 agents (financial, regulatory, market, risk, competitive)
- **Advanced Mode:** User selects 3-10 agents from library (20+ available dimensions)
- **Auto-Detection:** DOS analyzes decision brief, suggests relevant agents

**Example Agent Library:**
- Financial: Revenue impact, cost analysis, ROI projections
- Regulatory: Compliance risk, legal exposure, audit requirements
- Market: Competitive landscape, market timing, customer demand
- Risk: Operational risk, reputational risk, execution risk
- Strategic: Long-term alignment, strategic fit, opportunity cost

---

#### 2. Agent Debate Protocol
**Context:** DOS agents debate findings before synthesizing recommendation. Need protocol for conflict resolution.

**Question:** Should debate protocol be round-robin, tournament, or free-form?

**Options:**

| Protocol | Description | Pros | Cons |
|----------|-------------|------|------|
| **Round-Robin** | Each agent presents, others respond in sequence | Fair, structured | Can be slow (N² interactions) |
| **Tournament** | Agents pair off, winners debate winners | Fast convergence | May miss nuanced conflicts |
| **Free-Form** | Agents debate until consensus or timeout | Natural, adaptive | Hard to guarantee termination |

**Recommendation:**
- **Phase 1:** Round-robin (1 round: each agent presents, others comment)
- **Phase 2:** Free-form debate with timeout (5 minutes max)
- **Termination:** Consensus (80%+ agreement) or timeout (synthesize with disagreements noted)

**Debate Structure:**
1. **Round 1 (Presentation):** Each agent presents analysis (2 min each)
2. **Round 2 (Challenge):** Agents challenge each other's assumptions (5 min total)
3. **Round 3 (Synthesis):** Meta-agent synthesizes findings, notes conflicts (3 min)

---

#### 3. DOS UI: ABIS Integration or Custom?
**Context:** DOS could use ABIS for visual workflow design, or have simpler decision-specific UI.

**Question:** Should DOS use ABIS for visual agent system design, or have its own simpler UI?

**Options:**

| Approach | Pros | Cons |
|----------|------|------|
| **Use ABIS** | Leverage ABIS visual editor, no duplication | ABIS complexity may be overkill for decisions |
| **Custom DOS UI** | Decision-specific UX, simpler | Duplication, divergence from ABIS patterns |
| **Hybrid** | ABIS for power users, simple UI for typical use | Complexity, two UIs to maintain |

**Recommendation:**
- **Default UI:** Simple decision form (title, context, criteria, budget/time constraints)
- **Advanced Mode:** "Design Decision System" button → launch ABIS (for custom agent configurations)
- **Most users:** Never need ABIS (default 5-agent setup works)
- **Power users:** Use ABIS to design custom decision frameworks (20+ agents, custom debate protocols)

---

### 10.7 jSeeker (formerly PROTEUS) Migration Questions

#### 1. Module Name: Keep `proteus` or Rename to `jseeker`?
**Context:** Python module is currently `proteus/`. Product renamed to jSeeker to avoid confusion.

**Question:** Should jSeeker keep the `proteus/` Python module name for backward compatibility, or rename to `jseeker/`?

**Options:**

| Approach | Pros | Cons |
|----------|------|------|
| **Keep `proteus/`** | No import changes, backward compatible | Confusing (product=jSeeker, module=proteus) |
| **Rename to `jseeker/`** | Consistent naming, clear | Breaking change, update all imports |
| **Alias (both work)** | Gradual migration path | Complexity, two names for same thing |

**Recommendation:**
- **Rename to `jseeker/`** (clean break, one-time migration)
- **Migration Script:** Automated find-replace for imports
- **Deprecation:** Add `proteus/__init__.py` with deprecation warning: `import jseeker as proteus`
- **Timeline:** Complete migration in one release (v0.3.0)

---

#### 2. MNEMIS Integration Timeline
**Context:** jSeeker has MNEMIS integration planned for Phase 3+ (resume pattern storage). GAIA Phase 3 also focuses on MNEMIS.

**Question:** Should jSeeker MNEMIS integration happen in jSeeker Phase 3 or GAIA Phase 5?

**Options:**

| Timing | Pros | Cons |
|--------|------|------|
| **jSeeker Phase 3** | jSeeker learns patterns early, validates MNEMIS | May depend on incomplete MNEMIS features |
| **GAIA Phase 5** | MNEMIS fully mature, stable API | jSeeker delayed learning capability |
| **Parallel** | Both teams coordinate, co-develop features | Requires tight coordination |

**Recommendation:**
- **Parallel (coordinated):** jSeeker Phase 3 aligns with GAIA Phase 3 (MNEMIS auto-promotion)
- jSeeker implements pattern storage, GAIA implements promotion workflow
- **Benefit:** jSeeker becomes first real-world test of MNEMIS learning loop

---

#### 3. jSeeker as GECO Enforcement Pilot
**Context:** jSeeker is most GAIA-integrated product (MYCEL + ARGUS + MNEMIS planned). Could be pilot for full GECO enforcement.

**Question:** Should jSeeker be the pilot project for full GECO enforcement (hooks, CI/CD, WARDEN)?

**GECO Enforcement Checklist:**
- [ ] Pre-commit hooks (ruff, mypy, pytest)
- [ ] GitHub Actions CI/CD
- [ ] WARDEN compliance scan (secrets, test coverage)
- [ ] CLAUDE.md compliance (agent roles, quality gates)
- [ ] ARGUS telemetry (all operations logged)
- [ ] MNEMIS integration (pattern learning)

**Options:**

| Approach | Pros | Cons |
|----------|------|------|
| **jSeeker as Pilot** | Validates GECO tooling, iterates quickly | jSeeker team bears integration cost |
| **VULCAN as Pilot** | VULCAN creates projects, should self-comply | VULCAN complexity may obscure issues |
| **New Small Project** | Clean slate, no legacy debt | Delays validation on real product |

**Recommendation:**
- **jSeeker as GECO pilot** (v0.3.0 milestone)
- Rationale: Active development, clean codebase, already has ARGUS telemetry
- **Success Criteria:** jSeeker v0.3.0 passes full WARDEN scan, 80%+ coverage, CI passing
- **Timeline:** 2-3 weeks (align with GAIA Phase 1 enforcement rollout)

---

### 10.7 GPT_ECHO Questions

#### 1. Chunking Strategy
**Context:** GPT_ECHO needs to parse and chunk ChatGPT conversation exports for semantic search and taxonomy.

**Question:** Should GPT_ECHO use MYCEL's semantic chunking for conversation parsing, or develop domain-specific chunking?

**Options:**
- **MYCEL Semantic Chunking:** Leverage existing infrastructure, consistent with ecosystem
- **Domain-Specific Chunking:** Optimize for conversation structure (turn-taking, context boundaries)

**Recommendation:** Start with MYCEL semantic chunking, add conversation-specific refinements if needed.

---

#### 2. LLM Provider Migration
**Context:** GPT_ECHO currently uses Gemini for taxonomy generation and summarization.

**Question:** Should GPT_ECHO migrate to MYCEL multi-provider routing?

**Options:**
- **Keep Gemini:** Simplest, no migration cost
- **Migrate to MYCEL:** Consistent with ecosystem, multi-provider fallback, cost optimization

**Recommendation:** Migrate to MYCEL as part of GPT_ECHO consolidation (ENG-003).

---

#### 3. Consolidation Priority
**Context:** GPT_ECHO has 19 version copies and needs GAIA integration.

**Question:** Priority: consolidate 19 version copies first (ENG-003) or add GAIA integration first?

**Recommendation:** Consolidate first (establish single source of truth), then integrate with GAIA.

---

### 10.8 RAVEN Questions

#### 1. Execution Model
**Context:** RAVEN performs research investigations that may take minutes to hours.

**Question:** Should RAVEN be synchronous (user asks, waits for answer) or asynchronous (submit task, get notification)?

**Options:**
- **Synchronous:** Simpler, immediate feedback
- **Asynchronous:** Better UX for long tasks, enables batch processing

**Recommendation:** Asynchronous with task queue (submit → track progress → notification on completion).

---

#### 2. External Source Access
**Context:** RAVEN needs to gather external information for research tasks.

**Question:** RAVEN external source access: web search, API calls, or document-only?

**Options:**
- **Document-Only:** Safest, no external network calls
- **Web Search:** Broader research capability, security risks
- **API Calls:** Targeted data retrieval (e.g., CVE databases), requires API key management

**Recommendation:** Phase 1: document-only (MYCEL RAG). Phase 2: web search with user approval. Phase 3: API calls with WARDEN-validated credentials.

---

#### 3. Auto-Promotion Policy
**Context:** RAVEN findings can be stored in MNEMIS and promoted from PROJECT to GAIA tier.

**Question:** Should RAVEN findings auto-trigger MNEMIS promotion or require human review?

**Recommendation:** Human review required (trust principle: explicit approval for cross-project learning).

---

### 10.9 100% Enforcement Questions

#### 1. Realistic Target
**Context:** GAIA aims for 100% enforcement, but solo developer context may limit achievability.

**Question:** Is 100% enforcement realistic for a solo developer, or should we target "95% automated + 5% manual"?

**Recommendation:** 95% target is more realistic. 100% as aspirational goal. Focus automation on high-risk areas (secrets, test coverage, CLAUDE.md compliance).

---

#### 2. WARDEN Runtime Enforcement Mode
**Context:** WARDEN Phase 2 includes runtime enforcement (not just pre-commit).

**Question:** Should WARDEN runtime enforcement be blocking (stop execution) or advisory (warn + log)?

**Options:**
- **Blocking:** Strict enforcement, prevents violations
- **Advisory:** Less disruptive, user can override

**Recommendation:** Blocking for security violations (secrets, unauthorized memory access). Advisory for quality issues (test coverage, code style).

---

#### 3. Archived Product Enforcement
**Context:** Some products are archived/complete (The Palace, Waymo Data).

**Question:** How do we handle enforcement for archived/complete products?

**Recommendation:** Archived products exempt from active enforcement but subject to periodic WARDEN audits. Security violations trigger alerts even for archived code.

---

## Section 11: Appendices

### Appendix A: GAIA Module Reference

| Module | Role | Version | Status | LOC | Tests | Coverage | Path |
|--------|------|---------|--------|-----|-------|----------|------|
| **MYCEL** | Shared Intelligence Library | 0.2.0 | Active | ~1,200 | 200+ tests (10 files) | 92-100% | `X:\Projects\_GAIA\_MYCEL` |
| **VULCAN** | Project Creator (The Forge) | 0.4.0-dev | Development | 1,847 | 137 tests (5 files) | 85% | `X:\Projects\_GAIA\_VULCAN` |
| **LOOM** | Visual Agent Editor (Workflow Engine) | 0.1.0 | Development | ~1,000 | 500+ tests (exists, not enforced) | Unknown | `X:\Projects\_GAIA\_LOOM` |
| **MNEMIS** | Cross-Project Memory (3-tier hierarchy) | 0.1.0 | Development | ~950 | 400+ tests (exists, not enforced) | Unknown | `X:\Projects\_GAIA\_MNEMIS` |
| **ARGUS** | Monitoring + Kanban (The Watchman) | 0.5.0-dev | Development | ~3,000 | Not enforced | Unknown | `X:\Projects\_GAIA\_ARGUS` |
| **WARDEN** | Security & Compliance Guardian | 0.1.0 | Development | 6,908 bytes | 0 (missing) | 0% | `X:\Projects\_GAIA\_WARDEN` |
| **ECHO** | Chat Archaeology & Taxonomy | 0.1.0 | Stale | ~8,000 (19 versions) | 0 (missing) | 0% | `X:\Projects\_GAIA\_ECHO` |
| **RAVEN** | Intelligence Messenger | 0.0.1 | Placeholder | 0 | 0 | 0% | `X:\Projects\_GAIA\_RAVEN` |
| **Mental Models** | Decision Support Framework (59 models) | 1.0.0 | Active | ~400 | Operational | N/A | `X:\Projects\_GAIA\mental_models` |

**Notes:**
- **LOC:** Lines of code (Python only, excludes comments/docstrings)
- **Tests:** Count includes unit, integration, and E2E tests
- **Coverage:** Measured via pytest-cov (where available)
- **Status:** Active (production-ready), Development (functional but incomplete), Stale (not maintained), Placeholder (not implemented)

---

### Appendix B: Product Registry

| Product | Version | Status | Framework | Port | Providers | Dependencies | Path |
|---------|---------|--------|-----------|------|-----------|--------------|------|
| **HART OS** | 6.2.8 | Production | Streamlit | None | OpenAI | None | `X:\Projects\hart_os_v6` |
| **VIA Intelligence** | 6.4 | Production | Streamlit | 8503 | Gemini, OpenAI, Anthropic | MYCEL | `X:\Projects\VIA` |
| **DATA FORGE** | 1.1 | Production | Streamlit | None | OpenAI | None | `X:\Projects\Python tools\data-forge-v1.1` |
| **jSeeker** (formerly PROTEUS) | 0.2.1 | Active | Streamlit | None | Anthropic | MYCEL | `X:\Projects\jSeeker` (currently `X:\Projects\_GAIA\_PROTEUS`) |
| **THE PALACE** | 1.0 | Complete | Static HTML | None | None | None | `X:\Projects\The Palace` |

**Notes:**
- **Status Definitions:**
  - **Production:** Live, user-facing, stable
  - **Active:** Under development, functional
  - **Complete:** Finished, no further development
- **Port:** Streamlit port (if applicable, `None` = not currently running)
- **Providers:** LLM providers used (OpenAI, Anthropic, Gemini)
- **Dependencies:** GAIA modules required (MYCEL = shared library)

**Key Insights:**
- **3/5 products in production** (HART OS, VIA, DATA FORGE)
- **2/5 use MYCEL** (VIA, PROTEUS) — demonstrates shared library value
- **All use Streamlit** except THE PALACE (static HTML case study)
- **Only HART OS has GitHub remote** (external project)

---

### Appendix C: GECO Audit Cross-Reference

**Source Document:** `X:\Projects\_GAIA\GECO_AUDIT.md` (GECO Audit v1.0.0)

**Analysis Summary:**
- **Total Gaps:** 27 questions across PROD/ENG/UX/Cross-Functional
- **Severity Breakdown:**
  - **5 CRITICAL:** No CI/CD, no pre-commit hooks, WARDEN not integrated, hardcoded secrets, no GitHub remotes
  - **11 HIGH:** Process Observer missing, Trust Dashboard empty, governance not enforced, telemetry inconsistent
  - **9 MEDIUM:** Missing CLAUDE.md files, no MCP integration, skill registry unimplemented
  - **2 LOW:** ECHO stale, URL knowledge base scattered

**Analysis Matrix (27 Questions Traced):**

| Question # | Category | Severity | Section Reference | Status |
|-----------|----------|----------|-------------------|--------|
| PROD-1 | ECHO Rescue/Retire | MEDIUM | Appendix B (file index) | Open |
| PROD-2 | Roadmap Priority | HIGH | Section 1.3 (vision) | Open |
| PROD-3 | Metrics Validation | MEDIUM | Section 3 (success criteria) | Open |
| PROD-4 | Market Positioning | MEDIUM | Section 1 (vision) | Open |
| PROD-5 | Monetization | LOW | Section 1.2 (problem) | Open |
| ENG-1 | CI/CD Rollout | CRITICAL | Appendix B (missing inventory) | Open |
| ENG-2 | Pre-commit Hooks | CRITICAL | Appendix B (missing inventory) | Open |
| ENG-3 | Telemetry Schema | HIGH | Section 9.4 (data architecture) | Open |
| ENG-4 | WARDEN Integration | CRITICAL | Appendix B (missing inventory) | Open |
| ENG-5 | Process Observer | HIGH | Appendix B (missing inventory) | Open |
| ENG-6 | GitHub Strategy | CRITICAL | Appendix B (missing inventory) | Open |
| ENG-7 | Python Version | MEDIUM | Section 9.2 (environment) | Open |
| ENG-8 | Dependency Mgmt | MEDIUM | Section 9.1 (tech stack) | Open |
| UX-1 | Streamlit Migration | MEDIUM | Section 9.1 (tech stack) | Open |
| UX-2 | ARGUS Usability | MEDIUM | Section 3.3 (ARGUS metrics) | Open |
| UX-3 | Trust Dashboard | HIGH | Appendix B (missing inventory) | Open |
| UX-4 | VULCAN UX Friction | MEDIUM | Section 3.1 (VULCAN metrics) | Open |
| UX-5 | Error Messages | MEDIUM | Section 6.1 (user personas) | Open |
| UX-6 | HART OS Nav State | HIGH | Section 7.1 (HART OS risks) | Open |
| UX-7 | Localization | LOW | Section 9.2 (environment) | Open |
| UX-8 | Mobile Support | MEDIUM | Section 9.2 (environment) | Open |
| CROSS-1 | Constitutional Balance | HIGH | Section 2 (trust contract) | Open |
| CROSS-2 | Auto-Promote | HIGH | Section 2.4 (governance) | Open |
| CROSS-3 | Skill System | MEDIUM | Appendix B (missing inventory) | Open |
| CROSS-4 | Docs Restructure | MEDIUM | Appendix C (gap analysis) | Open |
| CROSS-5 | Version Strategy | MEDIUM | Appendix A (version history) | Open |
| CROSS-6 | Team Structure | MEDIUM | Section 8 (timeline) | Open |

**Next Steps:**
1. PROD/ENG/UX review (target: 3 business days)
2. Open Questions resolution meeting (target: 1 week)
3. PRD v1.0 finalization with decisions locked

---

### Appendix D: Document Version History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| **0.1.0** | 2026-02-08 | Claude Opus 4.6 | Initial draft from GECO Audit, GAIA_BIBLE.md, registry.json. Sections 9-11 (Technical Specifications, Open Questions, Appendices). Pending ENG/PROD/UX review. |

**Document Lineage:**
- **Source 1:** `X:\Projects\_GAIA\GECO_AUDIT.md` (v1.0.0) — Gap analysis, open questions, file inventory
- **Source 2:** `X:\Projects\_GAIA\GAIA_BIBLE.md` (v0.4.3) — Constitutional framework, architecture
- **Source 3:** `X:\Projects\_GAIA\registry.json` (updated 2026-02-04) — Component metadata
- **Source 4:** `C:\Users\Fede\.claude\settings.json` — Claude Code configuration reference

**Reviewers (Pending):**
- **ENG Review:** Technical accuracy, feasibility of proposed solutions
- **PROD Review:** Roadmap alignment, success metrics validation
- **UX Review:** User impact assessment, design requirements clarity

**Post-Review Actions:**
1. Incorporate feedback from ENG/PROD/UX
2. Resolve Open Questions (Section 10) in decision log
3. Finalize PRD v1.0 (lock success criteria, technical specs)
4. Generate implementation plan (ENG), design specs (UX), roadmap (PROD)

---

### Appendix E: Glossary

**GECO (GAIA Ecosystem Constitutional Observatory)**
Audit methodology and tooling for ensuring GAIA modules comply with constitutional principles. Produces gap analysis reports identifying missing implementations, inconsistencies, and technical debt.

**GAIA (General Artificial Intelligence Architecture)**
Master ecosystem layer that sits above all AI projects, providing shared intelligence (MYCEL), governance (LOOM), memory (MNEMIS), monitoring (ARGUS), and creation tools (VULCAN). Not an acronym—named after the Greek primordial deity representing Earth.

**Three Pillars**
GAIA's architectural separation of concerns:
1. **VULCAN** creates (project scaffolding, file generation)
2. **LOOM** modifies (workflow editing, agent IDE)
3. **ARGUS** monitors (telemetry, cost tracking, observability)

**Glass-Box Transparency**
Design principle ensuring all agent actions are observable, explainable, and auditable. Contrast with "black-box" AI (opaque decision-making). Implemented via ARGUS event bus, 4-level explainability, and Trust Dashboard.

**Constitutional AI**
Governance model where AI agents are bound by codified principles (written in CLAUDE.md files). Rules are programmatically validated (via LOOM), violations logged (via ARGUS), and learnings captured (via MNEMIS).

**Growth Rungs**
Pedagogical framework mapping explanation complexity to user expertise:
1. **SIMPLE:** Non-technical (end users, executives)
2. **TECHNICAL:** Intermediate (developers, analysts)
3. **DEEP:** Advanced (engineers, architects)
4. **CAUSAL:** Expert (researchers, PhDs)

Implemented in ARGUS explainability system (see `explainer.py`).

**Mental Models**
59 decision-support frameworks (e.g., First Principles Thinking, Pareto Principle, Jobs-to-be-Done) with context-aware selection. Stored in `mental_models/registry.json`, invoked via `selector.py` based on task type, user expertise, and problem complexity.

**Memory Tiers**
Three-level hierarchy for knowledge storage:
1. **PROJECT:** Module-specific (e.g., VULCAN config)
2. **GAIA:** Ecosystem-wide (shared patterns, errors)
3. **PUBLIC:** External reference library (open source, read-only)

Authority graph enforces access control (PROJECT → read own, GAIA → read all, PUBLIC → read-only).

**Authority Graph**
Hierarchical permission model for memory access. Higher tiers can read lower tiers, but promotion requires explicit protocol. Example: PROJECT-level error can be promoted to GAIA-level prevention rule after validation.

**Promotion Protocol**
MNEMIS process for elevating memories across tiers:
1. **Initiate:** Detect recurring pattern (e.g., error seen 5+ times)
2. **Validate:** Check criteria (severity, frequency, uniqueness)
3. **Promote:** Move from PROJECT to GAIA tier
4. **Notify:** Log promotion event to ARGUS
5. **Distribute:** Make available to all modules

Currently manual (CLI command), planned automation in learning loop.

**Event Bus**
ARGUS centralized telemetry system. All modules send structured events (timestamp, source, event_type, metadata) to SQLite database. Enables cross-module observability, cost tracking, and pattern detection.

**Subconscious Layer**
ARGUS background analysis system:
- **Pattern Detector:** Identifies recurring events (e.g., "GPT-4 timeouts spike Fridays")
- **Hypothesis Generator:** Proposes causal explanations (e.g., "OpenAI rate limits on weekends")
- **Post-Mortem Analyzer:** (Not implemented) Analyzes task failures, suggests fixes

Runs asynchronously, surfaces insights to dashboard.

**Process Observer**
(Not implemented) ARGUS component that monitors Claude Code agent executions. Planned implementation: Filesystem observer watching `.claude/` directory for task logs, or MCP plugin for direct telemetry.

**Trust Dashboard**
(Not implemented) ARGUS UI component visualizing constitutional compliance:
- **Transparency Score:** % of agent actions logged
- **Safety Score:** % of secrets detected, hooks passed
- **Governance Score:** % of LOOM rules enforced
- **Learning Score:** % of errors promoted to prevention rules

Target audience: Non-technical users (executives, auditors).

**MCP (Model Context Protocol)**
Anthropic's protocol for Claude Code plugins. Enables tools (filesystem, GitHub, web fetch) and integrations. GAIA uses 10 external MCP plugins, planned: GAIA modules as MCP servers (exposing MYCEL, ARGUS as tools).

**HITL (Human-in-the-Loop)**
Design pattern requiring user confirmation before critical actions. Example: VULCAN questionnaire (7 steps), MNEMIS promotion approval. Balances automation speed with user control (Trust Principle 4).

**ATS (Applicant Tracking System)**
Software used by recruiters to filter resumes. PROTEUS optimizes resumes for ATS parsing (keyword matching, formatting). Success metric: ATS score ≥ 85 (acceptance threshold).

**RAG (Retrieval-Augmented Generation)**
LLM technique combining semantic search (retrieve relevant context) with text generation. Implemented in MYCEL (`RAGPipeline` class). Used by VIA (investment research), DATA FORGE (taxonomy), PROTEUS (job description matching).

**JSONL (JSON Lines)**
File format where each line is a valid JSON object. Used for telemetry logs (append-only, no file locking). Example: `proteus_build.jsonl` (one resume build event per line).

---

## Document Metadata

```yaml
document:
  title: GAIA Ecosystem PRD - Sections 9-11
  subtitle: Technical Specifications, Open Questions, Appendices
  version: 0.1.0
  status: Draft — Pending ENG/PROD/UX Review
  date: 2026-02-08

author:
  name: Claude Opus 4.6
  role: GAIA Ecosystem Auditor
  commissioned_by: Federico (Product Owner)

sources:
  - file: X:\Projects\_GAIA\GAIA_BIBLE.md
    version: 0.4.3
    purpose: Constitutional framework, architecture
  - file: X:\Projects\_GAIA\GECO_AUDIT.md
    version: 1.0.0
    purpose: Gap analysis, open questions
  - file: X:\Projects\_GAIA\registry.json
    updated: 2026-02-04
    purpose: Component metadata
  - file: C:\Users\Fede\.claude\settings.json
    purpose: Claude Code configuration reference

scope:
  - section: 9
    title: Technical Specifications
    subsections: [9.1 Technology Stack, 9.2 Environment Requirements, 9.3 API & Integration Points, 9.4 Data Architecture, 9.5 Security Considerations]
    word_count: ~1,800
  - section: 10
    title: Open Questions for Team Review
    subsections: [10.1 For Product (PROD), 10.2 For Engineering (ENG), 10.3 For UX/UI, 10.4 Cross-Functional]
    word_count: ~1,500
  - section: 11
    title: Appendices
    subsections: [A: Module Reference, B: Product Registry, C: GECO Audit Cross-Reference, D: Version History, E: Glossary]
    word_count: ~1,200

total_word_count: ~4,500

next_steps:
  - step: 1
    action: ENG/PROD/UX review
    owner: Federico + team
    deadline: 2026-02-11 (3 business days)
    deliverable: Feedback on open questions, technical accuracy
  - step: 2
    action: Open Questions resolution meeting
    owner: Federico (facilitator)
    deadline: 2026-02-15 (1 week)
    deliverable: Decision log with rationale
  - step: 3
    action: PRD v1.0 finalization
    owner: Claude Opus 4.6 (with Federico approval)
    deadline: 2026-02-18 (2 weeks)
    deliverable: Locked PRD with success criteria, technical specs, roadmap
  - step: 4
    action: Implementation planning
    owner: ENG team
    deadline: 2026-02-25 (3 weeks)
    deliverable: Sprint breakdown, task assignments, estimates
  - step: 5
    action: Design specifications
    owner: UX team
    deadline: 2026-02-25 (3 weeks)
    deliverable: Wireframes, user flows, component specs
  - step: 6
    action: Roadmap with sprint planning
    owner: PROD team
    deadline: 2026-02-25 (3 weeks)
    deliverable: Q2 OKRs, feature prioritization, milestones

related_documents:
  - X:\Projects\_GAIA\GAIA_BIBLE.md (constitutional source)
  - X:\Projects\_GAIA\GECO_AUDIT.md (audit findings)
  - X:\Projects\_GAIA\registry.json (component registry)
  - X:\Projects\hart_os_v6\docs\PRD\HART_OS_v6.1_PRD.md (product PRD example)
  - X:\Projects\_GAIA\_PROTEUS\docs\PRD.md (product PRD example)
  - C:\Users\Fede\.claude\projects\C--Users-Fede\memory\MEMORY.md (session memory)

absolute_file_path: X:\Projects\_GAIA\PRD_SECTIONS_D.md
```

---

**End of PRD Sections 9-11**

---

## Document Footer

**Generated:** February 08, 2026
**Total Sections:** 11
**Audience:** ENG, PROD, UX teams
**Next Steps:**
1. ENG/PROD/UX review (target: 3 business days)
2. Open Questions resolution meeting (target: 1 week)
3. PRD v1.0 finalization with decisions locked
4. Sprint planning and task assignments

**Related Documents:**
- `X:\Projects\_GAIA\GAIA_BIBLE.md` (Constitutional source)
- `X:\Projects\_GAIA\GECO_AUDIT.md` (Ecosystem audit)
- `X:\Projects\_GAIA\registry.json` (Component registry)

---
*End of GAIA Ecosystem PRD v1.0.0-draft*
